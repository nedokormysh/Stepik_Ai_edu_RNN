{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nedokormysh/Stepik_Ai_edu_RNN/blob/week_5_char_RNN/AiEdu_CharRNN_Hometask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W8R8WgZceEk"
      },
      "source": [
        "# Домашнее задание\n",
        "\n",
        "В этом домашнем задании вы обучите рекуррентную сеть для генерации текстов в стиле Шекспира."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqUOE2flceEl"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wHfCDyzceEl"
      },
      "source": [
        "## Загрузим данные\n",
        "\n",
        "Загрузим текстовый файл с пьесами Шекспира."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/aiedu-courses/rnn_bootcamp/main/shakespeare.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXAxa--jPUSd",
        "outputId": "5293d81b-c8e7-4b17-b826-244aa53a6e01"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-14 12:52:20--  https://raw.githubusercontent.com/aiedu-courses/rnn_bootcamp/main/shakespeare.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘shakespeare.txt’\n",
            "\n",
            "shakespeare.txt     100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-01-14 12:52:21 (20.0 MB/s) - ‘shakespeare.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Эта задача аналогична задаче, разобранной на вебинаре, поэтому код мы вам не предоставляем, а предлагаем или написать с нуля, или воспользоваться кодом с вебинара.*"
      ],
      "metadata": {
        "id": "YkQhIGj-nzYx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b34kfqIOceEl"
      },
      "source": [
        "# open text file and read in data as `text`\n",
        "with open(\"shakespeare.txt\", \"r\") as f:\n",
        "    text = f.read()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[:100]"
      ],
      "metadata": {
        "id": "07b3AMdeCP_4",
        "outputId": "cc06a8a4-6741-4908-c3d8-51263ade5711",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Токенизация"
      ],
      "metadata": {
        "id": "-YCmrxDoCUMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = tuple(set(text))\n",
        "\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "metadata": {
        "id": "YhH1dl5QCUri"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "int2char.items()"
      ],
      "metadata": {
        "id": "eFLSwK_iC9fk",
        "outputId": "b503b6c5-3bab-4b13-8c27-a9488103674e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([(0, 'F'), (1, 'H'), (2, 'L'), (3, 'O'), (4, '&'), (5, 'K'), (6, 'E'), (7, 'l'), (8, '!'), (9, 'b'), (10, 't'), (11, 'z'), (12, 'R'), (13, 'r'), (14, '\\n'), (15, 'c'), (16, 'V'), (17, ':'), (18, 'i'), (19, 'f'), (20, 'h'), (21, \"'\"), (22, 'J'), (23, 'Y'), (24, 'j'), (25, 'g'), (26, 'P'), (27, '-'), (28, 'T'), (29, 'a'), (30, 'd'), (31, '.'), (32, '?'), (33, ' '), (34, 'm'), (35, 'o'), (36, 'k'), (37, 'y'), (38, 'S'), (39, 'M'), (40, 'x'), (41, 's'), (42, 'u'), (43, 'N'), (44, 'Q'), (45, '$'), (46, 'C'), (47, ','), (48, 'p'), (49, 'v'), (50, '3'), (51, 'q'), (52, 'Z'), (53, 'n'), (54, ';'), (55, 'B'), (56, 'X'), (57, 'G'), (58, 'U'), (59, 'w'), (60, 'I'), (61, 'D'), (62, 'e'), (63, 'W'), (64, 'A')])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# int2char"
      ],
      "metadata": {
        "id": "jVU4l_VzC5y4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ii, ch in int2char.items():\n",
        "  print(ii, ch)"
      ],
      "metadata": {
        "id": "eCUT-8qyCdeS",
        "outputId": "42220d63-9a99-42c0-8b5a-e456fe50a144",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 F\n",
            "1 H\n",
            "2 L\n",
            "3 O\n",
            "4 &\n",
            "5 K\n",
            "6 E\n",
            "7 l\n",
            "8 !\n",
            "9 b\n",
            "10 t\n",
            "11 z\n",
            "12 R\n",
            "13 r\n",
            "14 \n",
            "\n",
            "15 c\n",
            "16 V\n",
            "17 :\n",
            "18 i\n",
            "19 f\n",
            "20 h\n",
            "21 '\n",
            "22 J\n",
            "23 Y\n",
            "24 j\n",
            "25 g\n",
            "26 P\n",
            "27 -\n",
            "28 T\n",
            "29 a\n",
            "30 d\n",
            "31 .\n",
            "32 ?\n",
            "33  \n",
            "34 m\n",
            "35 o\n",
            "36 k\n",
            "37 y\n",
            "38 S\n",
            "39 M\n",
            "40 x\n",
            "41 s\n",
            "42 u\n",
            "43 N\n",
            "44 Q\n",
            "45 $\n",
            "46 C\n",
            "47 ,\n",
            "48 p\n",
            "49 v\n",
            "50 3\n",
            "51 q\n",
            "52 Z\n",
            "53 n\n",
            "54 ;\n",
            "55 B\n",
            "56 X\n",
            "57 G\n",
            "58 U\n",
            "59 w\n",
            "60 I\n",
            "61 D\n",
            "62 e\n",
            "63 W\n",
            "64 A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Предобработка данных"
      ],
      "metadata": {
        "id": "9ZPWc-f3DRd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "\n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "\n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.0\n",
        "    # print(arr)\n",
        "    # print(arr.flatten())\n",
        "    # print(np.arange(one_hot.shape[0]))\n",
        "    # print(one_hot[1])\n",
        "\n",
        "\n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "\n",
        "    return one_hot"
      ],
      "metadata": {
        "id": "GThlDmOcDBct"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Создаем мини-батчи (mini-batchs)\n"
      ],
      "metadata": {
        "id": "iY0YM075GDHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(int_words, batch_size, seq_length):\n",
        "    # 1. Truncate text, so there are only full batches\n",
        "    window_size = seq_length + 1\n",
        "    batch_size_total = batch_size * window_size\n",
        "    n_batches = len(int_words) // batch_size_total\n",
        "    int_words = int_words[: n_batches * batch_size_total]\n",
        "\n",
        "    # 2. Reshape into batch_size rows\n",
        "    int_words = int_words.reshape((batch_size, -1))\n",
        "\n",
        "    # 3. Iterate through the text matrix\n",
        "    for position in range(0, int_words.shape[1], window_size):\n",
        "        x = int_words[:, position : position + window_size - 1]\n",
        "        y = int_words[:, position + 1 : position + window_size]\n",
        "        yield x, y"
      ],
      "metadata": {
        "id": "jSB7z4JJH1Lh"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Зададим архитектуру"
      ],
      "metadata": {
        "id": "Chhh-npnKeeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if train_on_gpu:\n",
        "    print(\"Training on GPU!\")\n",
        "else:\n",
        "    print(\"No GPU available, training on CPU; consider making n_epochs very small.\")"
      ],
      "metadata": {
        "id": "zle4bLc-Kjxg",
        "outputId": "f3249829-094a-4f48-cd5d-d0aeba6bfb95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on GPU!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "\n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "\n",
        "        ## TODO: define the LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True\n",
        "        )\n",
        "\n",
        "        ## TODO: define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        ## TODO: define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"Forward pass through the network.\n",
        "        These inputs are x, and the hidden/cell state `hidden`.\"\"\"\n",
        "\n",
        "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "\n",
        "        ## TODO: pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "\n",
        "        # Stack up LSTM outputs using view\n",
        "        # you may need to use contiguous to reshape the output\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "\n",
        "        ## TODO: put x through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"Initializes hidden state\"\"\"\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        if train_on_gpu:\n",
        "            hidden = (\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "            )\n",
        "        else:\n",
        "            hidden = (\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "            )\n",
        "\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "VegodbYcKioc"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обучим модель"
      ],
      "metadata": {
        "id": "G5J-x8pLNdyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    net,\n",
        "    data,\n",
        "    epochs=10,\n",
        "    batch_size=10,\n",
        "    seq_length=50,\n",
        "    lr=0.001,\n",
        "    clip=5,\n",
        "    val_frac=0.1,\n",
        "    print_every=10,\n",
        "):\n",
        "    net.train()\n",
        "\n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data) * (1 - val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "\n",
        "    if train_on_gpu:\n",
        "        net.cuda()\n",
        "\n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "\n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "\n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "            if train_on_gpu:\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "\n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "\n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "\n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                    inputs, targets = x, y\n",
        "                    if train_on_gpu:\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(\n",
        "                        output, targets.view(batch_size * seq_length).long()\n",
        "                    )\n",
        "\n",
        "                    val_losses.append(val_loss.item())\n",
        "\n",
        "                net.train()  # reset to train mode after iterationg through validation data\n",
        "\n",
        "                print(\n",
        "                    \"Epoch: {}/{}...\".format(e + 1, epochs),\n",
        "                    \"Step: {}...\".format(counter),\n",
        "                    \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                    \"Val Loss: {:.4f}\".format(np.mean(val_losses)),\n",
        "                )"
      ],
      "metadata": {
        "id": "p_C9hWU7NhSV"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Определим модель"
      ],
      "metadata": {
        "id": "IKTO5j4POVyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define and print the net\n",
        "n_hidden = 512\n",
        "n_layers = 2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "metadata": {
        "id": "hwFZbhGnOY1k",
        "outputId": "cf653bb1-e808-4378-ddb3-385ac5a14784",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(65, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=65, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Установим гиперпараметры"
      ],
      "metadata": {
        "id": "i_Hf-orZOgRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 20  # start smaller if you are just testing initial behavior\n",
        "\n",
        "# train the model\n",
        "train(\n",
        "    net,\n",
        "    encoded,\n",
        "    epochs=n_epochs,\n",
        "    batch_size=batch_size,\n",
        "    seq_length=seq_length,\n",
        "    lr=0.001,\n",
        "    print_every=10,\n",
        ")"
      ],
      "metadata": {
        "id": "aP0usZjmOg8f",
        "outputId": "ef4d50ae-4ae5-4d1e-975b-d47a103c2415",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/20... Step: 10... Loss: 9.4739... Val Loss: 10.0270\n",
            "Epoch: 1/20... Step: 20... Loss: 9.4473... Val Loss: 9.7983\n",
            "Epoch: 1/20... Step: 30... Loss: 9.1645... Val Loss: 9.5844\n",
            "Epoch: 1/20... Step: 40... Loss: 9.5382... Val Loss: 9.3889\n",
            "Epoch: 1/20... Step: 50... Loss: 9.2709... Val Loss: 9.2152\n",
            "Epoch: 1/20... Step: 60... Loss: 8.6014... Val Loss: 9.0633\n",
            "Epoch: 1/20... Step: 70... Loss: 8.8335... Val Loss: 8.9294\n",
            "Epoch: 2/20... Step: 80... Loss: 8.2937... Val Loss: 8.8097\n",
            "Epoch: 2/20... Step: 90... Loss: 8.4343... Val Loss: 8.7011\n",
            "Epoch: 2/20... Step: 100... Loss: 8.4853... Val Loss: 8.5998\n",
            "Epoch: 2/20... Step: 110... Loss: 8.5428... Val Loss: 8.5032\n",
            "Epoch: 2/20... Step: 120... Loss: 8.3469... Val Loss: 8.4118\n",
            "Epoch: 2/20... Step: 130... Loss: 7.8923... Val Loss: 8.3250\n",
            "Epoch: 2/20... Step: 140... Loss: 8.1823... Val Loss: 8.2428\n",
            "Epoch: 2/20... Step: 150... Loss: 8.0176... Val Loss: 8.1649\n",
            "Epoch: 3/20... Step: 160... Loss: 8.1017... Val Loss: 8.0926\n",
            "Epoch: 3/20... Step: 170... Loss: 7.6505... Val Loss: 8.0246\n",
            "Epoch: 3/20... Step: 180... Loss: 7.8197... Val Loss: 7.9585\n",
            "Epoch: 3/20... Step: 190... Loss: 7.7971... Val Loss: 7.8944\n",
            "Epoch: 3/20... Step: 200... Loss: 7.5501... Val Loss: 7.8318\n",
            "Epoch: 3/20... Step: 210... Loss: 7.6576... Val Loss: 7.7707\n",
            "Epoch: 3/20... Step: 220... Loss: 7.3562... Val Loss: 7.7100\n",
            "Epoch: 3/20... Step: 230... Loss: 7.3857... Val Loss: 7.6500\n",
            "Epoch: 4/20... Step: 240... Loss: 7.1282... Val Loss: 7.5917\n",
            "Epoch: 4/20... Step: 250... Loss: 7.6755... Val Loss: 7.5346\n",
            "Epoch: 4/20... Step: 260... Loss: 7.0047... Val Loss: 7.4769\n",
            "Epoch: 4/20... Step: 270... Loss: 7.4429... Val Loss: 7.4207\n",
            "Epoch: 4/20... Step: 280... Loss: 7.1982... Val Loss: 7.3652\n",
            "Epoch: 4/20... Step: 290... Loss: 7.1201... Val Loss: 7.3107\n",
            "Epoch: 4/20... Step: 300... Loss: 7.2474... Val Loss: 7.2564\n",
            "Epoch: 5/20... Step: 310... Loss: 6.8713... Val Loss: 7.2029\n",
            "Epoch: 5/20... Step: 320... Loss: 6.9685... Val Loss: 7.1506\n",
            "Epoch: 5/20... Step: 330... Loss: 7.0992... Val Loss: 7.0978\n",
            "Epoch: 5/20... Step: 340... Loss: 6.6166... Val Loss: 7.0444\n",
            "Epoch: 5/20... Step: 350... Loss: 6.9270... Val Loss: 6.9917\n",
            "Epoch: 5/20... Step: 360... Loss: 6.5580... Val Loss: 6.9391\n",
            "Epoch: 5/20... Step: 370... Loss: 6.8929... Val Loss: 6.8866\n",
            "Epoch: 5/20... Step: 380... Loss: 6.3965... Val Loss: 6.8334\n",
            "Epoch: 6/20... Step: 390... Loss: 6.4915... Val Loss: 6.7811\n",
            "Epoch: 6/20... Step: 400... Loss: 6.6249... Val Loss: 6.7295\n",
            "Epoch: 6/20... Step: 410... Loss: 6.5077... Val Loss: 6.6768\n",
            "Epoch: 6/20... Step: 420... Loss: 6.4985... Val Loss: 6.6246\n",
            "Epoch: 6/20... Step: 430... Loss: 6.4229... Val Loss: 6.5727\n",
            "Epoch: 6/20... Step: 440... Loss: 6.3160... Val Loss: 6.5213\n",
            "Epoch: 6/20... Step: 450... Loss: 6.2427... Val Loss: 6.4694\n",
            "Epoch: 6/20... Step: 460... Loss: 6.1979... Val Loss: 6.4175\n",
            "Epoch: 7/20... Step: 470... Loss: 6.0259... Val Loss: 6.3665\n",
            "Epoch: 7/20... Step: 480... Loss: 6.0092... Val Loss: 6.3161\n",
            "Epoch: 7/20... Step: 490... Loss: 6.2264... Val Loss: 6.2642\n",
            "Epoch: 7/20... Step: 500... Loss: 5.8772... Val Loss: 6.2132\n",
            "Epoch: 7/20... Step: 510... Loss: 5.8166... Val Loss: 6.1622\n",
            "Epoch: 7/20... Step: 520... Loss: 5.7906... Val Loss: 6.1116\n",
            "Epoch: 7/20... Step: 530... Loss: 6.1716... Val Loss: 6.0609\n",
            "Epoch: 8/20... Step: 540... Loss: 5.6302... Val Loss: 6.0107\n",
            "Epoch: 8/20... Step: 550... Loss: 5.4435... Val Loss: 5.9619\n",
            "Epoch: 8/20... Step: 560... Loss: 5.6476... Val Loss: 5.9139\n",
            "Epoch: 8/20... Step: 570... Loss: 5.6024... Val Loss: 5.8660\n",
            "Epoch: 8/20... Step: 580... Loss: 5.7842... Val Loss: 5.8204\n",
            "Epoch: 8/20... Step: 590... Loss: 5.5468... Val Loss: 5.7767\n",
            "Epoch: 8/20... Step: 600... Loss: 5.3254... Val Loss: 5.7356\n",
            "Epoch: 8/20... Step: 610... Loss: 5.5100... Val Loss: 5.6963\n",
            "Epoch: 9/20... Step: 620... Loss: 5.3843... Val Loss: 5.6604\n",
            "Epoch: 9/20... Step: 630... Loss: 5.3326... Val Loss: 5.6278\n",
            "Epoch: 9/20... Step: 640... Loss: 5.4227... Val Loss: 5.5966\n",
            "Epoch: 9/20... Step: 650... Loss: 5.5162... Val Loss: 5.5680\n",
            "Epoch: 9/20... Step: 660... Loss: 5.3110... Val Loss: 5.5417\n",
            "Epoch: 9/20... Step: 670... Loss: 5.2394... Val Loss: 5.5174\n",
            "Epoch: 9/20... Step: 680... Loss: 5.3914... Val Loss: 5.4943\n",
            "Epoch: 9/20... Step: 690... Loss: 5.2828... Val Loss: 5.4714\n",
            "Epoch: 10/20... Step: 700... Loss: 5.2211... Val Loss: 5.4498\n",
            "Epoch: 10/20... Step: 710... Loss: 5.3394... Val Loss: 5.4289\n",
            "Epoch: 10/20... Step: 720... Loss: 5.1758... Val Loss: 5.4068\n",
            "Epoch: 10/20... Step: 730... Loss: 5.1424... Val Loss: 5.3857\n",
            "Epoch: 10/20... Step: 740... Loss: 4.9947... Val Loss: 5.3650\n",
            "Epoch: 10/20... Step: 750... Loss: 5.2416... Val Loss: 5.3449\n",
            "Epoch: 10/20... Step: 760... Loss: 5.2387... Val Loss: 5.3247\n",
            "Epoch: 10/20... Step: 770... Loss: 5.3105... Val Loss: 5.3043\n",
            "Epoch: 11/20... Step: 780... Loss: 4.8490... Val Loss: 5.2849\n",
            "Epoch: 11/20... Step: 790... Loss: 4.8974... Val Loss: 5.2656\n",
            "Epoch: 11/20... Step: 800... Loss: 4.8741... Val Loss: 5.2455\n",
            "Epoch: 11/20... Step: 810... Loss: 5.2567... Val Loss: 5.2267\n",
            "Epoch: 11/20... Step: 820... Loss: 5.1029... Val Loss: 5.2087\n",
            "Epoch: 11/20... Step: 830... Loss: 4.6911... Val Loss: 5.1915\n",
            "Epoch: 11/20... Step: 840... Loss: 5.0535... Val Loss: 5.1743\n",
            "Epoch: 12/20... Step: 850... Loss: 4.6885... Val Loss: 5.1579\n",
            "Epoch: 12/20... Step: 860... Loss: 5.0450... Val Loss: 5.1427\n",
            "Epoch: 12/20... Step: 870... Loss: 4.8889... Val Loss: 5.1271\n",
            "Epoch: 12/20... Step: 880... Loss: 5.1151... Val Loss: 5.1116\n",
            "Epoch: 12/20... Step: 890... Loss: 4.9390... Val Loss: 5.0971\n",
            "Epoch: 12/20... Step: 900... Loss: 4.7186... Val Loss: 5.0835\n",
            "Epoch: 12/20... Step: 910... Loss: 4.9678... Val Loss: 5.0702\n",
            "Epoch: 12/20... Step: 920... Loss: 4.9070... Val Loss: 5.0565\n",
            "Epoch: 13/20... Step: 930... Loss: 4.9568... Val Loss: 5.0436\n",
            "Epoch: 13/20... Step: 940... Loss: 4.7360... Val Loss: 5.0313\n",
            "Epoch: 13/20... Step: 950... Loss: 4.8190... Val Loss: 5.0180\n",
            "Epoch: 13/20... Step: 960... Loss: 4.8960... Val Loss: 5.0057\n",
            "Epoch: 13/20... Step: 970... Loss: 4.7688... Val Loss: 4.9940\n",
            "Epoch: 13/20... Step: 980... Loss: 4.7563... Val Loss: 4.9830\n",
            "Epoch: 13/20... Step: 990... Loss: 4.6631... Val Loss: 4.9719\n",
            "Epoch: 13/20... Step: 1000... Loss: 4.7207... Val Loss: 4.9606\n",
            "Epoch: 14/20... Step: 1010... Loss: 4.5481... Val Loss: 4.9499\n",
            "Epoch: 14/20... Step: 1020... Loss: 4.9964... Val Loss: 4.9393\n",
            "Epoch: 14/20... Step: 1030... Loss: 4.4576... Val Loss: 4.9277\n",
            "Epoch: 14/20... Step: 1040... Loss: 4.8426... Val Loss: 4.9171\n",
            "Epoch: 14/20... Step: 1050... Loss: 4.7810... Val Loss: 4.9066\n",
            "Epoch: 14/20... Step: 1060... Loss: 4.5772... Val Loss: 4.8964\n",
            "Epoch: 14/20... Step: 1070... Loss: 4.8306... Val Loss: 4.8858\n",
            "Epoch: 15/20... Step: 1080... Loss: 4.5846... Val Loss: 4.8750\n",
            "Epoch: 15/20... Step: 1090... Loss: 4.7024... Val Loss: 4.8649\n",
            "Epoch: 15/20... Step: 1100... Loss: 4.7368... Val Loss: 4.8540\n",
            "Epoch: 15/20... Step: 1110... Loss: 4.5355... Val Loss: 4.8428\n",
            "Epoch: 15/20... Step: 1120... Loss: 4.6828... Val Loss: 4.8322\n",
            "Epoch: 15/20... Step: 1130... Loss: 4.5148... Val Loss: 4.8221\n",
            "Epoch: 15/20... Step: 1140... Loss: 4.7360... Val Loss: 4.8120\n",
            "Epoch: 15/20... Step: 1150... Loss: 4.3429... Val Loss: 4.8011\n",
            "Epoch: 16/20... Step: 1160... Loss: 4.5175... Val Loss: 4.7906\n",
            "Epoch: 16/20... Step: 1170... Loss: 4.6279... Val Loss: 4.7805\n",
            "Epoch: 16/20... Step: 1180... Loss: 4.5964... Val Loss: 4.7691\n",
            "Epoch: 16/20... Step: 1190... Loss: 4.6186... Val Loss: 4.7583\n",
            "Epoch: 16/20... Step: 1200... Loss: 4.5290... Val Loss: 4.7477\n",
            "Epoch: 16/20... Step: 1210... Loss: 4.4743... Val Loss: 4.7376\n",
            "Epoch: 16/20... Step: 1220... Loss: 4.4690... Val Loss: 4.7272\n",
            "Epoch: 16/20... Step: 1230... Loss: 4.5196... Val Loss: 4.7162\n",
            "Epoch: 17/20... Step: 1240... Loss: 4.4409... Val Loss: 4.7058\n",
            "Epoch: 17/20... Step: 1250... Loss: 4.4294... Val Loss: 4.6955\n",
            "Epoch: 17/20... Step: 1260... Loss: 4.5761... Val Loss: 4.6840\n",
            "Epoch: 17/20... Step: 1270... Loss: 4.4001... Val Loss: 4.6734\n",
            "Epoch: 17/20... Step: 1280... Loss: 4.3672... Val Loss: 4.6631\n",
            "Epoch: 17/20... Step: 1290... Loss: 4.3474... Val Loss: 4.6529\n",
            "Epoch: 17/20... Step: 1300... Loss: 4.6375... Val Loss: 4.6423\n",
            "Epoch: 18/20... Step: 1310... Loss: 4.2620... Val Loss: 4.6314\n",
            "Epoch: 18/20... Step: 1320... Loss: 4.2226... Val Loss: 4.6212\n",
            "Epoch: 18/20... Step: 1330... Loss: 4.3945... Val Loss: 4.6106\n",
            "Epoch: 18/20... Step: 1340... Loss: 4.3613... Val Loss: 4.5994\n",
            "Epoch: 18/20... Step: 1350... Loss: 4.4582... Val Loss: 4.5889\n",
            "Epoch: 18/20... Step: 1360... Loss: 4.3676... Val Loss: 4.5788\n",
            "Epoch: 18/20... Step: 1370... Loss: 4.1873... Val Loss: 4.5687\n",
            "Epoch: 18/20... Step: 1380... Loss: 4.4106... Val Loss: 4.5580\n",
            "Epoch: 19/20... Step: 1390... Loss: 4.3065... Val Loss: 4.5476\n",
            "Epoch: 19/20... Step: 1400... Loss: 4.2607... Val Loss: 4.5377\n",
            "Epoch: 19/20... Step: 1410... Loss: 4.3903... Val Loss: 4.5267\n",
            "Epoch: 19/20... Step: 1420... Loss: 4.4757... Val Loss: 4.5159\n",
            "Epoch: 19/20... Step: 1430... Loss: 4.3252... Val Loss: 4.5056\n",
            "Epoch: 19/20... Step: 1440... Loss: 4.3097... Val Loss: 4.4956\n",
            "Epoch: 19/20... Step: 1450... Loss: 4.4002... Val Loss: 4.4855\n",
            "Epoch: 19/20... Step: 1460... Loss: 4.2980... Val Loss: 4.4745\n",
            "Epoch: 20/20... Step: 1470... Loss: 4.2648... Val Loss: 4.4643\n",
            "Epoch: 20/20... Step: 1480... Loss: 4.3364... Val Loss: 4.4543\n",
            "Epoch: 20/20... Step: 1490... Loss: 4.2040... Val Loss: 4.4430\n",
            "Epoch: 20/20... Step: 1500... Loss: 4.2089... Val Loss: 4.4325\n",
            "Epoch: 20/20... Step: 1510... Loss: 4.0832... Val Loss: 4.4223\n",
            "Epoch: 20/20... Step: 1520... Loss: 4.3144... Val Loss: 4.4123\n",
            "Epoch: 20/20... Step: 1530... Loss: 4.2996... Val Loss: 4.4020\n",
            "Epoch: 20/20... Step: 1540... Loss: 4.3841... Val Loss: 4.3911\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 1\n",
        "\n",
        "Выведите общее число параметров (весов) сети, генерирующей тексты в стиле Шекспира. Сеть задавайте такую же, как и в ноутбуке на вебинаре, с теми же гиперпараметрами.\n",
        "\n",
        "Подсказка: число параметров на каждом слое сети `model` можно посмотреть так:\n",
        "\n",
        "for layer in net.parameters():\n",
        "\n",
        "    ...."
      ],
      "metadata": {
        "id": "ty9_sNx-TZSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum = 0\n",
        "for layer in net.parameters():\n",
        "  # print(layer.numel())\n",
        "  sum += layer.numel()\n",
        "\n",
        "sum"
      ],
      "metadata": {
        "id": "u7GlhiEuP4gT",
        "outputId": "fb29b63e-8345-4c70-8f1d-c8b873df7dea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3320385"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "def count_parameters(model):\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad:\n",
        "            continue\n",
        "        params = parameter.numel()\n",
        "        table.add_row([name, params])\n",
        "        total_params += params\n",
        "    print(table)\n",
        "    print(f\"Total Trainable Params: {total_params}\")\n",
        "    return total_params\n",
        "\n",
        "count_parameters(net)"
      ],
      "metadata": {
        "id": "x_A577qLRY_o",
        "outputId": "25ead4c6-a883-40a4-83a0-c151ad4ae34b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+------------+\n",
            "|      Modules      | Parameters |\n",
            "+-------------------+------------+\n",
            "| lstm.weight_ih_l0 |   133120   |\n",
            "| lstm.weight_hh_l0 |  1048576   |\n",
            "|  lstm.bias_ih_l0  |    2048    |\n",
            "|  lstm.bias_hh_l0  |    2048    |\n",
            "| lstm.weight_ih_l1 |  1048576   |\n",
            "| lstm.weight_hh_l1 |  1048576   |\n",
            "|  lstm.bias_ih_l1  |    2048    |\n",
            "|  lstm.bias_hh_l1  |    2048    |\n",
            "|     fc.weight     |   33280    |\n",
            "|      fc.bias      |     65     |\n",
            "+-------------------+------------+\n",
            "Total Trainable Params: 3320385\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3320385"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# only trainable parameters\n",
        "pytorch_total_params = np.array([p.numel() for p in net.parameters() if p.requires_grad]).sum()\n",
        "pytorch_total_params"
      ],
      "metadata": {
        "id": "ODcXomoGRNGi",
        "outputId": "812269e1-2873-43b1-a726-0762902e7317",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3320385"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 2\n",
        "\n",
        "Будет ли сеть обучаться, если задать learning rate равным 1?"
      ],
      "metadata": {
        "id": "B_ObCHw2ThT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 20  # start smaller if you are just testing initial behavior\n",
        "\n",
        "# train the model\n",
        "train(\n",
        "    net,\n",
        "    encoded,\n",
        "    epochs=n_epochs,\n",
        "    batch_size=batch_size,\n",
        "    seq_length=seq_length,\n",
        "    lr=1,\n",
        "    print_every=10,\n",
        ")"
      ],
      "metadata": {
        "id": "nJdnMUJzTodt",
        "outputId": "658f3663-87d1-497e-8834-8c80160583f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/20... Step: 10... Loss: 136.0693... Val Loss: 127.9466\n",
            "Epoch: 1/20... Step: 20... Loss: 146.7548... Val Loss: 139.7564\n",
            "Epoch: 1/20... Step: 30... Loss: 105.7887... Val Loss: 90.8751\n",
            "Epoch: 1/20... Step: 40... Loss: 89.1725... Val Loss: 81.9634\n",
            "Epoch: 1/20... Step: 50... Loss: 77.8061... Val Loss: 66.6790\n",
            "Epoch: 1/20... Step: 60... Loss: 59.7220... Val Loss: 57.7956\n",
            "Epoch: 1/20... Step: 70... Loss: 66.3815... Val Loss: 67.5402\n",
            "Epoch: 2/20... Step: 80... Loss: 70.9837... Val Loss: 53.4431\n",
            "Epoch: 2/20... Step: 90... Loss: 53.7087... Val Loss: 34.0933\n",
            "Epoch: 2/20... Step: 100... Loss: 31.6670... Val Loss: 19.4136\n",
            "Epoch: 2/20... Step: 110... Loss: 19.7305... Val Loss: 14.2371\n",
            "Epoch: 2/20... Step: 120... Loss: 22.0205... Val Loss: 22.6981\n",
            "Epoch: 2/20... Step: 130... Loss: 39.4527... Val Loss: 38.1751\n",
            "Epoch: 2/20... Step: 140... Loss: 55.6302... Val Loss: 53.0616\n",
            "Epoch: 2/20... Step: 150... Loss: 66.6802... Val Loss: 59.6406\n",
            "Epoch: 3/20... Step: 160... Loss: 49.9490... Val Loss: 43.3837\n",
            "Epoch: 3/20... Step: 170... Loss: 28.0375... Val Loss: 24.9790\n",
            "Epoch: 3/20... Step: 180... Loss: 21.1765... Val Loss: 17.1644\n",
            "Epoch: 3/20... Step: 190... Loss: 15.2737... Val Loss: 13.0989\n",
            "Epoch: 3/20... Step: 200... Loss: 16.1179... Val Loss: 15.1987\n",
            "Epoch: 3/20... Step: 210... Loss: 19.0756... Val Loss: 16.7996\n",
            "Epoch: 3/20... Step: 220... Loss: 20.4966... Val Loss: 19.7229\n",
            "Epoch: 3/20... Step: 230... Loss: 25.1318... Val Loss: 23.0981\n",
            "Epoch: 4/20... Step: 240... Loss: 26.3470... Val Loss: 31.7485\n",
            "Epoch: 4/20... Step: 250... Loss: 40.3217... Val Loss: 41.3403\n",
            "Epoch: 4/20... Step: 260... Loss: 48.3655... Val Loss: 51.3314\n",
            "Epoch: 4/20... Step: 270... Loss: 41.9268... Val Loss: 44.1079\n",
            "Epoch: 4/20... Step: 280... Loss: 44.2984... Val Loss: 48.1322\n",
            "Epoch: 4/20... Step: 290... Loss: 45.3599... Val Loss: 48.1031\n",
            "Epoch: 4/20... Step: 300... Loss: 36.0003... Val Loss: 34.8164\n",
            "Epoch: 5/20... Step: 310... Loss: 33.1543... Val Loss: 36.0186\n",
            "Epoch: 5/20... Step: 320... Loss: 34.5108... Val Loss: 32.0067\n",
            "Epoch: 5/20... Step: 330... Loss: 30.7329... Val Loss: 30.7383\n",
            "Epoch: 5/20... Step: 340... Loss: 35.3661... Val Loss: 33.3632\n",
            "Epoch: 5/20... Step: 350... Loss: 32.9358... Val Loss: 33.9464\n",
            "Epoch: 5/20... Step: 360... Loss: 37.1191... Val Loss: 40.9588\n",
            "Epoch: 5/20... Step: 370... Loss: 40.6324... Val Loss: 38.6556\n",
            "Epoch: 5/20... Step: 380... Loss: 48.8733... Val Loss: 43.8925\n",
            "Epoch: 6/20... Step: 390... Loss: 37.6489... Val Loss: 38.4555\n",
            "Epoch: 6/20... Step: 400... Loss: 43.6804... Val Loss: 40.2498\n",
            "Epoch: 6/20... Step: 410... Loss: 38.3786... Val Loss: 41.2682\n",
            "Epoch: 6/20... Step: 420... Loss: 41.1346... Val Loss: 40.1729\n",
            "Epoch: 6/20... Step: 430... Loss: 31.0049... Val Loss: 29.9346\n",
            "Epoch: 6/20... Step: 440... Loss: 25.0916... Val Loss: 24.3496\n",
            "Epoch: 6/20... Step: 450... Loss: 20.9811... Val Loss: 19.0638\n",
            "Epoch: 6/20... Step: 460... Loss: 21.1880... Val Loss: 20.7415\n",
            "Epoch: 7/20... Step: 470... Loss: 17.6535... Val Loss: 16.9317\n",
            "Epoch: 7/20... Step: 480... Loss: 16.0911... Val Loss: 18.6405\n",
            "Epoch: 7/20... Step: 490... Loss: 16.7931... Val Loss: 16.7776\n",
            "Epoch: 7/20... Step: 500... Loss: 18.4055... Val Loss: 17.8118\n",
            "Epoch: 7/20... Step: 510... Loss: 13.1620... Val Loss: 14.0651\n",
            "Epoch: 7/20... Step: 520... Loss: 14.9330... Val Loss: 14.1241\n",
            "Epoch: 7/20... Step: 530... Loss: 16.9646... Val Loss: 16.2447\n",
            "Epoch: 8/20... Step: 540... Loss: 20.9532... Val Loss: 19.7762\n",
            "Epoch: 8/20... Step: 550... Loss: 26.4467... Val Loss: 24.6745\n",
            "Epoch: 8/20... Step: 560... Loss: 25.0848... Val Loss: 25.4401\n",
            "Epoch: 8/20... Step: 570... Loss: 23.1504... Val Loss: 25.1755\n",
            "Epoch: 8/20... Step: 580... Loss: 26.5641... Val Loss: 25.4924\n",
            "Epoch: 8/20... Step: 590... Loss: 22.0881... Val Loss: 23.6085\n",
            "Epoch: 8/20... Step: 600... Loss: 20.2176... Val Loss: 19.9423\n",
            "Epoch: 8/20... Step: 610... Loss: 23.1706... Val Loss: 21.5250\n",
            "Epoch: 9/20... Step: 620... Loss: 23.4017... Val Loss: 22.8302\n",
            "Epoch: 9/20... Step: 630... Loss: 22.6292... Val Loss: 22.7995\n",
            "Epoch: 9/20... Step: 640... Loss: 19.5566... Val Loss: 17.0030\n",
            "Epoch: 9/20... Step: 650... Loss: 21.4240... Val Loss: 21.3565\n",
            "Epoch: 9/20... Step: 660... Loss: 17.6769... Val Loss: 18.4514\n",
            "Epoch: 9/20... Step: 670... Loss: 18.5167... Val Loss: 19.5990\n",
            "Epoch: 9/20... Step: 680... Loss: 17.3113... Val Loss: 18.8220\n",
            "Epoch: 9/20... Step: 690... Loss: 13.0673... Val Loss: 14.1563\n",
            "Epoch: 10/20... Step: 700... Loss: 16.2624... Val Loss: 17.0970\n",
            "Epoch: 10/20... Step: 710... Loss: 16.3044... Val Loss: 15.4224\n",
            "Epoch: 10/20... Step: 720... Loss: 15.3050... Val Loss: 15.8201\n",
            "Epoch: 10/20... Step: 730... Loss: 13.4118... Val Loss: 13.4782\n",
            "Epoch: 10/20... Step: 740... Loss: 15.5829... Val Loss: 13.6147\n",
            "Epoch: 10/20... Step: 750... Loss: 11.4727... Val Loss: 10.7966\n",
            "Epoch: 10/20... Step: 760... Loss: 12.9606... Val Loss: 12.4395\n",
            "Epoch: 10/20... Step: 770... Loss: 9.7969... Val Loss: 10.0989\n",
            "Epoch: 11/20... Step: 780... Loss: 8.2365... Val Loss: 7.7788\n",
            "Epoch: 11/20... Step: 790... Loss: 9.6750... Val Loss: 10.7187\n",
            "Epoch: 11/20... Step: 800... Loss: 9.1114... Val Loss: 9.6374\n",
            "Epoch: 11/20... Step: 810... Loss: 9.5594... Val Loss: 9.1842\n",
            "Epoch: 11/20... Step: 820... Loss: 9.5946... Val Loss: 8.5693\n",
            "Epoch: 11/20... Step: 830... Loss: 9.5248... Val Loss: 8.7801\n",
            "Epoch: 11/20... Step: 840... Loss: 12.4717... Val Loss: 11.2133\n",
            "Epoch: 12/20... Step: 850... Loss: 11.9796... Val Loss: 10.0956\n",
            "Epoch: 12/20... Step: 860... Loss: 9.9178... Val Loss: 9.2067\n",
            "Epoch: 12/20... Step: 870... Loss: 11.6325... Val Loss: 11.0408\n",
            "Epoch: 12/20... Step: 880... Loss: 10.6600... Val Loss: 9.4761\n",
            "Epoch: 12/20... Step: 890... Loss: 9.8842... Val Loss: 9.9935\n",
            "Epoch: 12/20... Step: 900... Loss: 10.8708... Val Loss: 10.6117\n",
            "Epoch: 12/20... Step: 910... Loss: 12.8123... Val Loss: 12.8962\n",
            "Epoch: 12/20... Step: 920... Loss: 12.4328... Val Loss: 12.2021\n",
            "Epoch: 13/20... Step: 930... Loss: 14.3562... Val Loss: 13.9066\n",
            "Epoch: 13/20... Step: 940... Loss: 13.4221... Val Loss: 11.7821\n",
            "Epoch: 13/20... Step: 950... Loss: 16.9451... Val Loss: 15.9723\n",
            "Epoch: 13/20... Step: 960... Loss: 17.4397... Val Loss: 17.2638\n",
            "Epoch: 13/20... Step: 970... Loss: 16.5552... Val Loss: 16.3685\n",
            "Epoch: 13/20... Step: 980... Loss: 14.3631... Val Loss: 14.6955\n",
            "Epoch: 13/20... Step: 990... Loss: 14.8885... Val Loss: 14.5190\n",
            "Epoch: 13/20... Step: 1000... Loss: 18.1347... Val Loss: 18.6117\n",
            "Epoch: 14/20... Step: 1010... Loss: 16.6472... Val Loss: 18.1092\n",
            "Epoch: 14/20... Step: 1020... Loss: 23.5931... Val Loss: 20.6445\n",
            "Epoch: 14/20... Step: 1030... Loss: 20.4511... Val Loss: 20.8666\n",
            "Epoch: 14/20... Step: 1040... Loss: 19.5464... Val Loss: 19.2406\n",
            "Epoch: 14/20... Step: 1050... Loss: 14.6116... Val Loss: 16.2896\n",
            "Epoch: 14/20... Step: 1060... Loss: 17.4501... Val Loss: 16.8611\n",
            "Epoch: 14/20... Step: 1070... Loss: 19.0958... Val Loss: 17.7585\n",
            "Epoch: 15/20... Step: 1080... Loss: 18.2151... Val Loss: 18.6262\n",
            "Epoch: 15/20... Step: 1090... Loss: 14.6902... Val Loss: 17.7185\n",
            "Epoch: 15/20... Step: 1100... Loss: 18.3403... Val Loss: 17.7536\n",
            "Epoch: 15/20... Step: 1110... Loss: 16.4968... Val Loss: 17.6447\n",
            "Epoch: 15/20... Step: 1120... Loss: 19.2197... Val Loss: 18.1450\n",
            "Epoch: 15/20... Step: 1130... Loss: 16.8233... Val Loss: 16.2846\n",
            "Epoch: 15/20... Step: 1140... Loss: 18.0708... Val Loss: 17.1846\n",
            "Epoch: 15/20... Step: 1150... Loss: 16.5794... Val Loss: 19.0432\n",
            "Epoch: 16/20... Step: 1160... Loss: 22.9112... Val Loss: 22.4556\n",
            "Epoch: 16/20... Step: 1170... Loss: 23.6562... Val Loss: 22.9366\n",
            "Epoch: 16/20... Step: 1180... Loss: 21.7078... Val Loss: 22.0219\n",
            "Epoch: 16/20... Step: 1190... Loss: 18.2159... Val Loss: 18.1862\n",
            "Epoch: 16/20... Step: 1200... Loss: 16.6075... Val Loss: 16.9953\n",
            "Epoch: 16/20... Step: 1210... Loss: 18.9265... Val Loss: 19.6723\n",
            "Epoch: 16/20... Step: 1220... Loss: 20.9781... Val Loss: 19.3790\n",
            "Epoch: 16/20... Step: 1230... Loss: 20.7376... Val Loss: 21.9297\n",
            "Epoch: 17/20... Step: 1240... Loss: 22.8548... Val Loss: 24.4965\n",
            "Epoch: 17/20... Step: 1250... Loss: 25.6796... Val Loss: 26.2102\n",
            "Epoch: 17/20... Step: 1260... Loss: 26.3266... Val Loss: 26.1236\n",
            "Epoch: 17/20... Step: 1270... Loss: 27.1952... Val Loss: 25.6393\n",
            "Epoch: 17/20... Step: 1280... Loss: 25.4678... Val Loss: 27.1071\n",
            "Epoch: 17/20... Step: 1290... Loss: 24.2278... Val Loss: 27.0295\n",
            "Epoch: 17/20... Step: 1300... Loss: 21.0559... Val Loss: 22.3333\n",
            "Epoch: 18/20... Step: 1310... Loss: 15.2700... Val Loss: 16.2924\n",
            "Epoch: 18/20... Step: 1320... Loss: 13.4361... Val Loss: 14.7322\n",
            "Epoch: 18/20... Step: 1330... Loss: 13.3899... Val Loss: 14.2379\n",
            "Epoch: 18/20... Step: 1340... Loss: 13.0999... Val Loss: 13.6969\n",
            "Epoch: 18/20... Step: 1350... Loss: 13.4184... Val Loss: 14.6273\n",
            "Epoch: 18/20... Step: 1360... Loss: 8.1283... Val Loss: 8.5790\n",
            "Epoch: 18/20... Step: 1370... Loss: 11.0294... Val Loss: 12.5004\n",
            "Epoch: 18/20... Step: 1380... Loss: 11.1493... Val Loss: 11.8689\n",
            "Epoch: 19/20... Step: 1390... Loss: 11.4648... Val Loss: 11.0927\n",
            "Epoch: 19/20... Step: 1400... Loss: 11.6334... Val Loss: 12.4476\n",
            "Epoch: 19/20... Step: 1410... Loss: 12.2351... Val Loss: 12.3532\n",
            "Epoch: 19/20... Step: 1420... Loss: 11.7537... Val Loss: 12.0134\n",
            "Epoch: 19/20... Step: 1430... Loss: 10.3231... Val Loss: 9.8466\n",
            "Epoch: 19/20... Step: 1440... Loss: 12.0559... Val Loss: 11.7459\n",
            "Epoch: 19/20... Step: 1450... Loss: 9.4525... Val Loss: 10.8626\n",
            "Epoch: 19/20... Step: 1460... Loss: 13.6201... Val Loss: 13.1207\n",
            "Epoch: 20/20... Step: 1470... Loss: 14.1511... Val Loss: 14.2775\n",
            "Epoch: 20/20... Step: 1480... Loss: 13.5283... Val Loss: 13.9939\n",
            "Epoch: 20/20... Step: 1490... Loss: 13.2800... Val Loss: 14.2235\n",
            "Epoch: 20/20... Step: 1500... Loss: 13.5063... Val Loss: 13.3059\n",
            "Epoch: 20/20... Step: 1510... Loss: 12.5898... Val Loss: 12.6705\n",
            "Epoch: 20/20... Step: 1520... Loss: 14.0956... Val Loss: 13.6714\n",
            "Epoch: 20/20... Step: 1530... Loss: 12.5304... Val Loss: 11.6966\n",
            "Epoch: 20/20... Step: 1540... Loss: 10.6360... Val Loss: 10.2722\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 3\n",
        "\n",
        "В комментарии напишите кусочек текста в стиле Шекспира, сгененированного вашей моделью. Выберите кусочек, который вам больше всего понравился!"
      ],
      "metadata": {
        "id": "98KdJ-fIT_Wv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Делаем предсказания"
      ],
      "metadata": {
        "id": "KtkvljT0UNeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "    \"\"\"Given a character, predict the next character.\n",
        "    Returns the predicted character and the hidden state.\n",
        "    \"\"\"\n",
        "\n",
        "    # tensor inputs\n",
        "    x = np.array([[net.char2int[char]]])\n",
        "    x = one_hot_encode(x, len(net.chars))\n",
        "    inputs = torch.from_numpy(x)\n",
        "\n",
        "    if train_on_gpu:\n",
        "        inputs = inputs.cuda()\n",
        "\n",
        "    # detach hidden state from history\n",
        "    h = tuple([each.data for each in h])\n",
        "    # get the output of the model\n",
        "    out, h = net(inputs, h)\n",
        "\n",
        "    # get the character probabilities\n",
        "    p = F.softmax(out, dim=1).data\n",
        "    if train_on_gpu:\n",
        "        p = p.cpu()  # move to cpu\n",
        "\n",
        "    # get top characters\n",
        "    if top_k is None:\n",
        "        top_ch = np.arange(len(net.chars))\n",
        "    else:\n",
        "        p, top_ch = p.topk(top_k)\n",
        "        top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "    # select the likely next character with some element of randomness\n",
        "    p = p.numpy().squeeze()\n",
        "    char = np.random.choice(top_ch, p=p / p.sum())\n",
        "\n",
        "    # return the encoded value of the predicted char and the hidden state\n",
        "    return net.int2char[char], h"
      ],
      "metadata": {
        "id": "W2Vw_BiPUHZo"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Priming"
      ],
      "metadata": {
        "id": "pAx-qhzxUTNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(net, size, prime=\"The\", top_k=None):\n",
        "\n",
        "    if train_on_gpu:\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "\n",
        "    net.eval()  # eval mode\n",
        "\n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "\n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return \"\".join(chars)"
      ],
      "metadata": {
        "id": "Sxr6xGDPUQMu"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample(net, 500, prime=\"The\", top_k=30))"
      ],
      "metadata": {
        "id": "czaqKp57VQX5",
        "outputId": "0a0ff5f0-6133-48bf-dfa8-ecba5f7eb087",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The \n",
            " sst at n ocg    ismhanneAp :rfanndo  e\n",
            "ne  sfg kimr o e aoe woo  yt\n",
            "icytrtwdnateft c  t c   wtte\n",
            "lldvlreEioadhli   g enl stres is c te t ldhyfvdeIon Inhr sh \n",
            "s ot r:\n",
            "eItyepnr , ewtoaetwiido\n",
            "i n ssl sti cddoloei ohmmeoooh ca,ntdthsf, tcmr:lder\n",
            "hl  tdoytl:nhn tsfa m oe,  m rs ysataoefoe Ihrsai \n",
            "  gsltolhIp,' e p hp tpmseb h cs,'ltrc mg  ih  alnsn losgto ts. slkgofb:p iba dflon wofdihAttt nooenrv\n",
            "and\n",
            "ionyi iAaa ired:htiiove\n",
            "Irtregw  ohmfweneotttyehmenigeIoa g nofsyhmoty\n",
            "ndiwdfd eet e f.rr\n",
            "lehtk,d\n"
          ]
        }
      ]
    }
  ]
}