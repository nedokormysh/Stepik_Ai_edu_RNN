{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nedokormysh/Stepik_Ai_edu_RNN/blob/week_5_char_RNN/AiEdu_CharRNN_Hometask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W8R8WgZceEk"
      },
      "source": [
        "# Домашнее задание\n",
        "\n",
        "В этом домашнем задании вы обучите рекуррентную сеть для генерации текстов в стиле Шекспира."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqUOE2flceEl"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wHfCDyzceEl"
      },
      "source": [
        "## Загрузим данные\n",
        "\n",
        "Загрузим текстовый файл с пьесами Шекспира."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/aiedu-courses/rnn_bootcamp/main/shakespeare.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXAxa--jPUSd",
        "outputId": "95615daa-3ae8-4194-f335-e46f91ab6b13"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-15 10:16:38--  https://raw.githubusercontent.com/aiedu-courses/rnn_bootcamp/main/shakespeare.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘shakespeare.txt’\n",
            "\n",
            "shakespeare.txt     100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-01-15 10:16:39 (20.0 MB/s) - ‘shakespeare.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Эта задача аналогична задаче, разобранной на вебинаре, поэтому код мы вам не предоставляем, а предлагаем или написать с нуля, или воспользоваться кодом с вебинара.*"
      ],
      "metadata": {
        "id": "YkQhIGj-nzYx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b34kfqIOceEl"
      },
      "source": [
        "# open text file and read in data as `text`\n",
        "with open(\"shakespeare.txt\", \"r\") as f:\n",
        "    text = f.read()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[:100]"
      ],
      "metadata": {
        "id": "07b3AMdeCP_4",
        "outputId": "9e7ca18f-6822-4084-9b6e-d78fede165b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Токенизация"
      ],
      "metadata": {
        "id": "-YCmrxDoCUMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = tuple(set(text))\n",
        "\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "metadata": {
        "id": "YhH1dl5QCUri"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "int2char.items()"
      ],
      "metadata": {
        "id": "eFLSwK_iC9fk",
        "outputId": "ef2de293-b629-4959-8cbf-8a3cf3f6e1fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([(0, 'v'), (1, 'P'), (2, 'g'), (3, 'b'), (4, 'Y'), (5, 'q'), (6, 'X'), (7, '?'), (8, 'j'), (9, \"'\"), (10, 'c'), (11, 'F'), (12, 's'), (13, 'S'), (14, 'E'), (15, 'y'), (16, '!'), (17, 'r'), (18, 'O'), (19, 'e'), (20, 'W'), (21, '$'), (22, '.'), (23, 'w'), (24, 'R'), (25, 'B'), (26, 'h'), (27, 'x'), (28, 'U'), (29, 'T'), (30, 'u'), (31, 'a'), (32, 'V'), (33, 'G'), (34, 'N'), (35, '&'), (36, 'K'), (37, 'M'), (38, 'l'), (39, ','), (40, 'Z'), (41, 'p'), (42, '-'), (43, 't'), (44, 'd'), (45, 'z'), (46, 'f'), (47, 'A'), (48, 'n'), (49, 'k'), (50, 'C'), (51, 'D'), (52, 'J'), (53, '\\n'), (54, 'L'), (55, ' '), (56, ':'), (57, ';'), (58, 'o'), (59, 'Q'), (60, 'i'), (61, 'I'), (62, 'm'), (63, '3'), (64, 'H')])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# int2char"
      ],
      "metadata": {
        "id": "jVU4l_VzC5y4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ii, ch in int2char.items():\n",
        "  print(ii, ch)"
      ],
      "metadata": {
        "id": "eCUT-8qyCdeS",
        "outputId": "54d46d62-2c0e-4128-ae44-d68a9ad75252",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 v\n",
            "1 P\n",
            "2 g\n",
            "3 b\n",
            "4 Y\n",
            "5 q\n",
            "6 X\n",
            "7 ?\n",
            "8 j\n",
            "9 '\n",
            "10 c\n",
            "11 F\n",
            "12 s\n",
            "13 S\n",
            "14 E\n",
            "15 y\n",
            "16 !\n",
            "17 r\n",
            "18 O\n",
            "19 e\n",
            "20 W\n",
            "21 $\n",
            "22 .\n",
            "23 w\n",
            "24 R\n",
            "25 B\n",
            "26 h\n",
            "27 x\n",
            "28 U\n",
            "29 T\n",
            "30 u\n",
            "31 a\n",
            "32 V\n",
            "33 G\n",
            "34 N\n",
            "35 &\n",
            "36 K\n",
            "37 M\n",
            "38 l\n",
            "39 ,\n",
            "40 Z\n",
            "41 p\n",
            "42 -\n",
            "43 t\n",
            "44 d\n",
            "45 z\n",
            "46 f\n",
            "47 A\n",
            "48 n\n",
            "49 k\n",
            "50 C\n",
            "51 D\n",
            "52 J\n",
            "53 \n",
            "\n",
            "54 L\n",
            "55  \n",
            "56 :\n",
            "57 ;\n",
            "58 o\n",
            "59 Q\n",
            "60 i\n",
            "61 I\n",
            "62 m\n",
            "63 3\n",
            "64 H\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Предобработка данных"
      ],
      "metadata": {
        "id": "9ZPWc-f3DRd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "\n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "\n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.0\n",
        "    # print(arr)\n",
        "    # print(arr.flatten())\n",
        "    # print(np.arange(one_hot.shape[0]))\n",
        "    # print(one_hot[1])\n",
        "\n",
        "\n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "\n",
        "    return one_hot"
      ],
      "metadata": {
        "id": "GThlDmOcDBct"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Создаем мини-батчи (mini-batchs)\n"
      ],
      "metadata": {
        "id": "iY0YM075GDHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(int_words, batch_size, seq_length):\n",
        "    # 1. Truncate text, so there are only full batches\n",
        "    window_size = seq_length + 1\n",
        "    batch_size_total = batch_size * window_size\n",
        "    n_batches = len(int_words) // batch_size_total\n",
        "    int_words = int_words[: n_batches * batch_size_total]\n",
        "\n",
        "    # 2. Reshape into batch_size rows\n",
        "    int_words = int_words.reshape((batch_size, -1))\n",
        "\n",
        "    # 3. Iterate through the text matrix\n",
        "    for position in range(0, int_words.shape[1], window_size):\n",
        "        x = int_words[:, position : position + window_size - 1]\n",
        "        y = int_words[:, position + 1 : position + window_size]\n",
        "        yield x, y"
      ],
      "metadata": {
        "id": "jSB7z4JJH1Lh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Зададим архитектуру"
      ],
      "metadata": {
        "id": "Chhh-npnKeeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if train_on_gpu:\n",
        "    print(\"Training on GPU!\")\n",
        "else:\n",
        "    print(\"No GPU available, training on CPU; consider making n_epochs very small.\")"
      ],
      "metadata": {
        "id": "zle4bLc-Kjxg",
        "outputId": "a96218c3-da3b-4272-b396-1655f7e94327",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on GPU!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "\n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "\n",
        "        ## TODO: define the LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True\n",
        "        )\n",
        "\n",
        "        ## TODO: define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        ## TODO: define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"Forward pass through the network.\n",
        "        These inputs are x, and the hidden/cell state `hidden`.\"\"\"\n",
        "\n",
        "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "\n",
        "        ## TODO: pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "\n",
        "        # Stack up LSTM outputs using view\n",
        "        # you may need to use contiguous to reshape the output\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "\n",
        "        ## TODO: put x through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"Initializes hidden state\"\"\"\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        if train_on_gpu:\n",
        "            hidden = (\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "            )\n",
        "        else:\n",
        "            hidden = (\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "            )\n",
        "\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "VegodbYcKioc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обучим модель"
      ],
      "metadata": {
        "id": "G5J-x8pLNdyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    net,\n",
        "    data,\n",
        "    epochs=10,\n",
        "    batch_size=10,\n",
        "    seq_length=50,\n",
        "    lr=0.001,\n",
        "    clip=5,\n",
        "    val_frac=0.1,\n",
        "    print_every=10,\n",
        "):\n",
        "    net.train()\n",
        "\n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data) * (1 - val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "\n",
        "    if train_on_gpu:\n",
        "        net.cuda()\n",
        "\n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "\n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "\n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "            if train_on_gpu:\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "\n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "\n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "\n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                    inputs, targets = x, y\n",
        "                    if train_on_gpu:\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(\n",
        "                        output, targets.view(batch_size * seq_length).long()\n",
        "                    )\n",
        "\n",
        "                    val_losses.append(val_loss.item())\n",
        "\n",
        "                net.train()  # reset to train mode after iterationg through validation data\n",
        "\n",
        "                print(\n",
        "                    \"Epoch: {}/{}...\".format(e + 1, epochs),\n",
        "                    \"Step: {}...\".format(counter),\n",
        "                    \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                    \"Val Loss: {:.4f}\".format(np.mean(val_losses)),\n",
        "                )"
      ],
      "metadata": {
        "id": "p_C9hWU7NhSV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Определим модель"
      ],
      "metadata": {
        "id": "IKTO5j4POVyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define and print the net\n",
        "n_hidden = 512\n",
        "n_layers = 2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "metadata": {
        "id": "hwFZbhGnOY1k",
        "outputId": "23bfc4c9-3fd3-41dd-8b05-68eedbf417b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(65, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=65, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Установим гиперпараметры"
      ],
      "metadata": {
        "id": "i_Hf-orZOgRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 20  # start smaller if you are just testing initial behavior\n",
        "\n",
        "# train the model\n",
        "train(\n",
        "    net,\n",
        "    encoded,\n",
        "    epochs=n_epochs,\n",
        "    batch_size=batch_size,\n",
        "    seq_length=seq_length,\n",
        "    lr=0.001,\n",
        "    print_every=10,\n",
        ")"
      ],
      "metadata": {
        "id": "aP0usZjmOg8f",
        "outputId": "74bb1a7a-4285-4a77-9e6b-5e4d7132b0db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/20... Step: 10... Loss: 3.3613... Val Loss: 3.3818\n",
            "Epoch: 1/20... Step: 20... Loss: 3.3313... Val Loss: 3.3508\n",
            "Epoch: 1/20... Step: 30... Loss: 3.3208... Val Loss: 3.3470\n",
            "Epoch: 1/20... Step: 40... Loss: 3.3502... Val Loss: 3.3420\n",
            "Epoch: 1/20... Step: 50... Loss: 3.3359... Val Loss: 3.3388\n",
            "Epoch: 1/20... Step: 60... Loss: 3.2920... Val Loss: 3.3235\n",
            "Epoch: 1/20... Step: 70... Loss: 3.3182... Val Loss: 3.3301\n",
            "Epoch: 2/20... Step: 80... Loss: 3.2840... Val Loss: 3.3174\n",
            "Epoch: 2/20... Step: 90... Loss: 3.2755... Val Loss: 3.2900\n",
            "Epoch: 2/20... Step: 100... Loss: 3.2321... Val Loss: 3.2253\n",
            "Epoch: 2/20... Step: 110... Loss: 3.1436... Val Loss: 3.1180\n",
            "Epoch: 2/20... Step: 120... Loss: 3.0570... Val Loss: 3.0178\n",
            "Epoch: 2/20... Step: 130... Loss: 2.9434... Val Loss: 3.0847\n",
            "Epoch: 2/20... Step: 140... Loss: 2.9272... Val Loss: 2.8712\n",
            "Epoch: 2/20... Step: 150... Loss: 2.8016... Val Loss: 2.7509\n",
            "Epoch: 3/20... Step: 160... Loss: 2.7138... Val Loss: 2.6619\n",
            "Epoch: 3/20... Step: 170... Loss: 2.6306... Val Loss: 2.5633\n",
            "Epoch: 3/20... Step: 180... Loss: 2.5742... Val Loss: 2.4954\n",
            "Epoch: 3/20... Step: 190... Loss: 2.5250... Val Loss: 2.4514\n",
            "Epoch: 3/20... Step: 200... Loss: 2.4558... Val Loss: 2.4138\n",
            "Epoch: 3/20... Step: 210... Loss: 2.4698... Val Loss: 2.3853\n",
            "Epoch: 3/20... Step: 220... Loss: 2.3966... Val Loss: 2.3534\n",
            "Epoch: 3/20... Step: 230... Loss: 2.3803... Val Loss: 2.3292\n",
            "Epoch: 4/20... Step: 240... Loss: 2.3484... Val Loss: 2.3011\n",
            "Epoch: 4/20... Step: 250... Loss: 2.3270... Val Loss: 2.2771\n",
            "Epoch: 4/20... Step: 260... Loss: 2.3275... Val Loss: 2.2548\n",
            "Epoch: 4/20... Step: 270... Loss: 2.3094... Val Loss: 2.2373\n",
            "Epoch: 4/20... Step: 280... Loss: 2.2640... Val Loss: 2.2253\n",
            "Epoch: 4/20... Step: 290... Loss: 2.2486... Val Loss: 2.2034\n",
            "Epoch: 4/20... Step: 300... Loss: 2.2275... Val Loss: 2.1790\n",
            "Epoch: 5/20... Step: 310... Loss: 2.2116... Val Loss: 2.1631\n",
            "Epoch: 5/20... Step: 320... Loss: 2.1879... Val Loss: 2.1434\n",
            "Epoch: 5/20... Step: 330... Loss: 2.1761... Val Loss: 2.1309\n",
            "Epoch: 5/20... Step: 340... Loss: 2.1664... Val Loss: 2.1173\n",
            "Epoch: 5/20... Step: 350... Loss: 2.1559... Val Loss: 2.1037\n",
            "Epoch: 5/20... Step: 360... Loss: 2.1392... Val Loss: 2.0983\n",
            "Epoch: 5/20... Step: 370... Loss: 2.1154... Val Loss: 2.0838\n",
            "Epoch: 5/20... Step: 380... Loss: 2.1059... Val Loss: 2.0669\n",
            "Epoch: 6/20... Step: 390... Loss: 2.0775... Val Loss: 2.0596\n",
            "Epoch: 6/20... Step: 400... Loss: 2.0872... Val Loss: 2.0514\n",
            "Epoch: 6/20... Step: 410... Loss: 2.0535... Val Loss: 2.0377\n",
            "Epoch: 6/20... Step: 420... Loss: 2.0607... Val Loss: 2.0251\n",
            "Epoch: 6/20... Step: 430... Loss: 2.0234... Val Loss: 2.0162\n",
            "Epoch: 6/20... Step: 440... Loss: 2.0406... Val Loss: 2.0136\n",
            "Epoch: 6/20... Step: 450... Loss: 2.0134... Val Loss: 2.0024\n",
            "Epoch: 6/20... Step: 460... Loss: 2.0146... Val Loss: 1.9943\n",
            "Epoch: 7/20... Step: 470... Loss: 2.0225... Val Loss: 1.9844\n",
            "Epoch: 7/20... Step: 480... Loss: 2.0170... Val Loss: 1.9795\n",
            "Epoch: 7/20... Step: 490... Loss: 1.9627... Val Loss: 1.9671\n",
            "Epoch: 7/20... Step: 500... Loss: 1.9740... Val Loss: 1.9599\n",
            "Epoch: 7/20... Step: 510... Loss: 1.9612... Val Loss: 1.9556\n",
            "Epoch: 7/20... Step: 520... Loss: 1.9738... Val Loss: 1.9473\n",
            "Epoch: 7/20... Step: 530... Loss: 1.9154... Val Loss: 1.9369\n",
            "Epoch: 8/20... Step: 540... Loss: 1.9618... Val Loss: 1.9294\n",
            "Epoch: 8/20... Step: 550... Loss: 1.9282... Val Loss: 1.9237\n",
            "Epoch: 8/20... Step: 560... Loss: 1.8989... Val Loss: 1.9184\n",
            "Epoch: 8/20... Step: 570... Loss: 1.9041... Val Loss: 1.9093\n",
            "Epoch: 8/20... Step: 580... Loss: 1.9075... Val Loss: 1.9005\n",
            "Epoch: 8/20... Step: 590... Loss: 1.9154... Val Loss: 1.8981\n",
            "Epoch: 8/20... Step: 600... Loss: 1.8797... Val Loss: 1.8936\n",
            "Epoch: 8/20... Step: 610... Loss: 1.8780... Val Loss: 1.8878\n",
            "Epoch: 9/20... Step: 620... Loss: 1.9031... Val Loss: 1.8881\n",
            "Epoch: 9/20... Step: 630... Loss: 1.8813... Val Loss: 1.8757\n",
            "Epoch: 9/20... Step: 640... Loss: 1.8378... Val Loss: 1.8664\n",
            "Epoch: 9/20... Step: 650... Loss: 1.8439... Val Loss: 1.8665\n",
            "Epoch: 9/20... Step: 660... Loss: 1.8328... Val Loss: 1.8590\n",
            "Epoch: 9/20... Step: 670... Loss: 1.8428... Val Loss: 1.8527\n",
            "Epoch: 9/20... Step: 680... Loss: 1.8419... Val Loss: 1.8495\n",
            "Epoch: 9/20... Step: 690... Loss: 1.8272... Val Loss: 1.8465\n",
            "Epoch: 10/20... Step: 700... Loss: 1.8199... Val Loss: 1.8386\n",
            "Epoch: 10/20... Step: 710... Loss: 1.8332... Val Loss: 1.8345\n",
            "Epoch: 10/20... Step: 720... Loss: 1.8168... Val Loss: 1.8294\n",
            "Epoch: 10/20... Step: 730... Loss: 1.8309... Val Loss: 1.8223\n",
            "Epoch: 10/20... Step: 740... Loss: 1.7976... Val Loss: 1.8190\n",
            "Epoch: 10/20... Step: 750... Loss: 1.7854... Val Loss: 1.8161\n",
            "Epoch: 10/20... Step: 760... Loss: 1.7793... Val Loss: 1.8088\n",
            "Epoch: 10/20... Step: 770... Loss: 1.7713... Val Loss: 1.8030\n",
            "Epoch: 11/20... Step: 780... Loss: 1.8064... Val Loss: 1.8023\n",
            "Epoch: 11/20... Step: 790... Loss: 1.7878... Val Loss: 1.7965\n",
            "Epoch: 11/20... Step: 800... Loss: 1.7753... Val Loss: 1.7925\n",
            "Epoch: 11/20... Step: 810... Loss: 1.7516... Val Loss: 1.7897\n",
            "Epoch: 11/20... Step: 820... Loss: 1.7324... Val Loss: 1.7847\n",
            "Epoch: 11/20... Step: 830... Loss: 1.7586... Val Loss: 1.7873\n",
            "Epoch: 11/20... Step: 840... Loss: 1.7497... Val Loss: 1.7775\n",
            "Epoch: 12/20... Step: 850... Loss: 1.7776... Val Loss: 1.7768\n",
            "Epoch: 12/20... Step: 860... Loss: 1.7483... Val Loss: 1.7688\n",
            "Epoch: 12/20... Step: 870... Loss: 1.7289... Val Loss: 1.7640\n",
            "Epoch: 12/20... Step: 880... Loss: 1.6987... Val Loss: 1.7598\n",
            "Epoch: 12/20... Step: 890... Loss: 1.7279... Val Loss: 1.7578\n",
            "Epoch: 12/20... Step: 900... Loss: 1.7245... Val Loss: 1.7577\n",
            "Epoch: 12/20... Step: 910... Loss: 1.6949... Val Loss: 1.7558\n",
            "Epoch: 12/20... Step: 920... Loss: 1.7170... Val Loss: 1.7495\n",
            "Epoch: 13/20... Step: 930... Loss: 1.6984... Val Loss: 1.7476\n",
            "Epoch: 13/20... Step: 940... Loss: 1.7128... Val Loss: 1.7461\n",
            "Epoch: 13/20... Step: 950... Loss: 1.6998... Val Loss: 1.7384\n",
            "Epoch: 13/20... Step: 960... Loss: 1.6747... Val Loss: 1.7370\n",
            "Epoch: 13/20... Step: 970... Loss: 1.6722... Val Loss: 1.7326\n",
            "Epoch: 13/20... Step: 980... Loss: 1.6871... Val Loss: 1.7321\n",
            "Epoch: 13/20... Step: 990... Loss: 1.6948... Val Loss: 1.7247\n",
            "Epoch: 13/20... Step: 1000... Loss: 1.6975... Val Loss: 1.7242\n",
            "Epoch: 14/20... Step: 1010... Loss: 1.6814... Val Loss: 1.7195\n",
            "Epoch: 14/20... Step: 1020... Loss: 1.6855... Val Loss: 1.7147\n",
            "Epoch: 14/20... Step: 1030... Loss: 1.7008... Val Loss: 1.7131\n",
            "Epoch: 14/20... Step: 1040... Loss: 1.6759... Val Loss: 1.7102\n",
            "Epoch: 14/20... Step: 1050... Loss: 1.6628... Val Loss: 1.7048\n",
            "Epoch: 14/20... Step: 1060... Loss: 1.6823... Val Loss: 1.7163\n",
            "Epoch: 14/20... Step: 1070... Loss: 1.6681... Val Loss: 1.7069\n",
            "Epoch: 15/20... Step: 1080... Loss: 1.6576... Val Loss: 1.7010\n",
            "Epoch: 15/20... Step: 1090... Loss: 1.6461... Val Loss: 1.6972\n",
            "Epoch: 15/20... Step: 1100... Loss: 1.6217... Val Loss: 1.6946\n",
            "Epoch: 15/20... Step: 1110... Loss: 1.6562... Val Loss: 1.6883\n",
            "Epoch: 15/20... Step: 1120... Loss: 1.6427... Val Loss: 1.6910\n",
            "Epoch: 15/20... Step: 1130... Loss: 1.6399... Val Loss: 1.6849\n",
            "Epoch: 15/20... Step: 1140... Loss: 1.6270... Val Loss: 1.6871\n",
            "Epoch: 15/20... Step: 1150... Loss: 1.6429... Val Loss: 1.6805\n",
            "Epoch: 16/20... Step: 1160... Loss: 1.6235... Val Loss: 1.6817\n",
            "Epoch: 16/20... Step: 1170... Loss: 1.6236... Val Loss: 1.6773\n",
            "Epoch: 16/20... Step: 1180... Loss: 1.6241... Val Loss: 1.6721\n",
            "Epoch: 16/20... Step: 1190... Loss: 1.6193... Val Loss: 1.6724\n",
            "Epoch: 16/20... Step: 1200... Loss: 1.5891... Val Loss: 1.6713\n",
            "Epoch: 16/20... Step: 1210... Loss: 1.6104... Val Loss: 1.6688\n",
            "Epoch: 16/20... Step: 1220... Loss: 1.6154... Val Loss: 1.6717\n",
            "Epoch: 16/20... Step: 1230... Loss: 1.6260... Val Loss: 1.6647\n",
            "Epoch: 17/20... Step: 1240... Loss: 1.6330... Val Loss: 1.6626\n",
            "Epoch: 17/20... Step: 1250... Loss: 1.6329... Val Loss: 1.6616\n",
            "Epoch: 17/20... Step: 1260... Loss: 1.5894... Val Loss: 1.6527\n",
            "Epoch: 17/20... Step: 1270... Loss: 1.6050... Val Loss: 1.6541\n",
            "Epoch: 17/20... Step: 1280... Loss: 1.6002... Val Loss: 1.6558\n",
            "Epoch: 17/20... Step: 1290... Loss: 1.6026... Val Loss: 1.6554\n",
            "Epoch: 17/20... Step: 1300... Loss: 1.5528... Val Loss: 1.6506\n",
            "Epoch: 18/20... Step: 1310... Loss: 1.6211... Val Loss: 1.6522\n",
            "Epoch: 18/20... Step: 1320... Loss: 1.6231... Val Loss: 1.6489\n",
            "Epoch: 18/20... Step: 1330... Loss: 1.5734... Val Loss: 1.6395\n",
            "Epoch: 18/20... Step: 1340... Loss: 1.5761... Val Loss: 1.6390\n",
            "Epoch: 18/20... Step: 1350... Loss: 1.5746... Val Loss: 1.6378\n",
            "Epoch: 18/20... Step: 1360... Loss: 1.6004... Val Loss: 1.6369\n",
            "Epoch: 18/20... Step: 1370... Loss: 1.5618... Val Loss: 1.6404\n",
            "Epoch: 18/20... Step: 1380... Loss: 1.5858... Val Loss: 1.6362\n",
            "Epoch: 19/20... Step: 1390... Loss: 1.5945... Val Loss: 1.6334\n",
            "Epoch: 19/20... Step: 1400... Loss: 1.5758... Val Loss: 1.6323\n",
            "Epoch: 19/20... Step: 1410... Loss: 1.5497... Val Loss: 1.6284\n",
            "Epoch: 19/20... Step: 1420... Loss: 1.5699... Val Loss: 1.6285\n",
            "Epoch: 19/20... Step: 1430... Loss: 1.5578... Val Loss: 1.6286\n",
            "Epoch: 19/20... Step: 1440... Loss: 1.5704... Val Loss: 1.6259\n",
            "Epoch: 19/20... Step: 1450... Loss: 1.5636... Val Loss: 1.6267\n",
            "Epoch: 19/20... Step: 1460... Loss: 1.5635... Val Loss: 1.6222\n",
            "Epoch: 20/20... Step: 1470... Loss: 1.5607... Val Loss: 1.6253\n",
            "Epoch: 20/20... Step: 1480... Loss: 1.5662... Val Loss: 1.6175\n",
            "Epoch: 20/20... Step: 1490... Loss: 1.5563... Val Loss: 1.6144\n",
            "Epoch: 20/20... Step: 1500... Loss: 1.5624... Val Loss: 1.6192\n",
            "Epoch: 20/20... Step: 1510... Loss: 1.5414... Val Loss: 1.6192\n",
            "Epoch: 20/20... Step: 1520... Loss: 1.5436... Val Loss: 1.6155\n",
            "Epoch: 20/20... Step: 1530... Loss: 1.5330... Val Loss: 1.6153\n",
            "Epoch: 20/20... Step: 1540... Loss: 1.5387... Val Loss: 1.6111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 1\n",
        "\n",
        "Выведите общее число параметров (весов) сети, генерирующей тексты в стиле Шекспира. Сеть задавайте такую же, как и в ноутбуке на вебинаре, с теми же гиперпараметрами.\n",
        "\n",
        "Подсказка: число параметров на каждом слое сети `model` можно посмотреть так:\n",
        "\n",
        "for layer in net.parameters():\n",
        "\n",
        "    ...."
      ],
      "metadata": {
        "id": "ty9_sNx-TZSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum = 0\n",
        "for layer in net.parameters():\n",
        "  # print(layer.numel())\n",
        "  sum += layer.numel()\n",
        "\n",
        "sum"
      ],
      "metadata": {
        "id": "u7GlhiEuP4gT",
        "outputId": "0493772d-1529-4034-b1f1-5a98529c1a29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3320385"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "def count_parameters(model):\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad:\n",
        "            continue\n",
        "        params = parameter.numel()\n",
        "        table.add_row([name, params])\n",
        "        total_params += params\n",
        "    print(table)\n",
        "    print(f\"Total Trainable Params: {total_params}\")\n",
        "    return total_params\n",
        "\n",
        "count_parameters(net)"
      ],
      "metadata": {
        "id": "x_A577qLRY_o",
        "outputId": "dcef5541-a2b9-4802-c66f-ba5ba7af71bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+------------+\n",
            "|      Modules      | Parameters |\n",
            "+-------------------+------------+\n",
            "| lstm.weight_ih_l0 |   133120   |\n",
            "| lstm.weight_hh_l0 |  1048576   |\n",
            "|  lstm.bias_ih_l0  |    2048    |\n",
            "|  lstm.bias_hh_l0  |    2048    |\n",
            "| lstm.weight_ih_l1 |  1048576   |\n",
            "| lstm.weight_hh_l1 |  1048576   |\n",
            "|  lstm.bias_ih_l1  |    2048    |\n",
            "|  lstm.bias_hh_l1  |    2048    |\n",
            "|     fc.weight     |   33280    |\n",
            "|      fc.bias      |     65     |\n",
            "+-------------------+------------+\n",
            "Total Trainable Params: 3320385\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3320385"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# only trainable parameters\n",
        "pytorch_total_params = np.array([p.numel() for p in net.parameters() if p.requires_grad]).sum()\n",
        "pytorch_total_params"
      ],
      "metadata": {
        "id": "ODcXomoGRNGi",
        "outputId": "28ce4b44-062b-412f-883d-4240a3945c33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3320385"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 2\n",
        "\n",
        "Будет ли сеть обучаться, если задать learning rate равным 1?"
      ],
      "metadata": {
        "id": "B_ObCHw2ThT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size = 128\n",
        "# seq_length = 100\n",
        "# n_epochs = 20  # start smaller if you are just testing initial behavior\n",
        "\n",
        "# # train the model\n",
        "# train(\n",
        "#     net,\n",
        "#     encoded,\n",
        "#     epochs=n_epochs,\n",
        "#     batch_size=batch_size,\n",
        "#     seq_length=seq_length,\n",
        "#     lr=1,\n",
        "#     print_every=10,\n",
        "# )"
      ],
      "metadata": {
        "id": "nJdnMUJzTodt",
        "outputId": "658f3663-87d1-497e-8834-8c80160583f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/20... Step: 10... Loss: 136.0693... Val Loss: 127.9466\n",
            "Epoch: 1/20... Step: 20... Loss: 146.7548... Val Loss: 139.7564\n",
            "Epoch: 1/20... Step: 30... Loss: 105.7887... Val Loss: 90.8751\n",
            "Epoch: 1/20... Step: 40... Loss: 89.1725... Val Loss: 81.9634\n",
            "Epoch: 1/20... Step: 50... Loss: 77.8061... Val Loss: 66.6790\n",
            "Epoch: 1/20... Step: 60... Loss: 59.7220... Val Loss: 57.7956\n",
            "Epoch: 1/20... Step: 70... Loss: 66.3815... Val Loss: 67.5402\n",
            "Epoch: 2/20... Step: 80... Loss: 70.9837... Val Loss: 53.4431\n",
            "Epoch: 2/20... Step: 90... Loss: 53.7087... Val Loss: 34.0933\n",
            "Epoch: 2/20... Step: 100... Loss: 31.6670... Val Loss: 19.4136\n",
            "Epoch: 2/20... Step: 110... Loss: 19.7305... Val Loss: 14.2371\n",
            "Epoch: 2/20... Step: 120... Loss: 22.0205... Val Loss: 22.6981\n",
            "Epoch: 2/20... Step: 130... Loss: 39.4527... Val Loss: 38.1751\n",
            "Epoch: 2/20... Step: 140... Loss: 55.6302... Val Loss: 53.0616\n",
            "Epoch: 2/20... Step: 150... Loss: 66.6802... Val Loss: 59.6406\n",
            "Epoch: 3/20... Step: 160... Loss: 49.9490... Val Loss: 43.3837\n",
            "Epoch: 3/20... Step: 170... Loss: 28.0375... Val Loss: 24.9790\n",
            "Epoch: 3/20... Step: 180... Loss: 21.1765... Val Loss: 17.1644\n",
            "Epoch: 3/20... Step: 190... Loss: 15.2737... Val Loss: 13.0989\n",
            "Epoch: 3/20... Step: 200... Loss: 16.1179... Val Loss: 15.1987\n",
            "Epoch: 3/20... Step: 210... Loss: 19.0756... Val Loss: 16.7996\n",
            "Epoch: 3/20... Step: 220... Loss: 20.4966... Val Loss: 19.7229\n",
            "Epoch: 3/20... Step: 230... Loss: 25.1318... Val Loss: 23.0981\n",
            "Epoch: 4/20... Step: 240... Loss: 26.3470... Val Loss: 31.7485\n",
            "Epoch: 4/20... Step: 250... Loss: 40.3217... Val Loss: 41.3403\n",
            "Epoch: 4/20... Step: 260... Loss: 48.3655... Val Loss: 51.3314\n",
            "Epoch: 4/20... Step: 270... Loss: 41.9268... Val Loss: 44.1079\n",
            "Epoch: 4/20... Step: 280... Loss: 44.2984... Val Loss: 48.1322\n",
            "Epoch: 4/20... Step: 290... Loss: 45.3599... Val Loss: 48.1031\n",
            "Epoch: 4/20... Step: 300... Loss: 36.0003... Val Loss: 34.8164\n",
            "Epoch: 5/20... Step: 310... Loss: 33.1543... Val Loss: 36.0186\n",
            "Epoch: 5/20... Step: 320... Loss: 34.5108... Val Loss: 32.0067\n",
            "Epoch: 5/20... Step: 330... Loss: 30.7329... Val Loss: 30.7383\n",
            "Epoch: 5/20... Step: 340... Loss: 35.3661... Val Loss: 33.3632\n",
            "Epoch: 5/20... Step: 350... Loss: 32.9358... Val Loss: 33.9464\n",
            "Epoch: 5/20... Step: 360... Loss: 37.1191... Val Loss: 40.9588\n",
            "Epoch: 5/20... Step: 370... Loss: 40.6324... Val Loss: 38.6556\n",
            "Epoch: 5/20... Step: 380... Loss: 48.8733... Val Loss: 43.8925\n",
            "Epoch: 6/20... Step: 390... Loss: 37.6489... Val Loss: 38.4555\n",
            "Epoch: 6/20... Step: 400... Loss: 43.6804... Val Loss: 40.2498\n",
            "Epoch: 6/20... Step: 410... Loss: 38.3786... Val Loss: 41.2682\n",
            "Epoch: 6/20... Step: 420... Loss: 41.1346... Val Loss: 40.1729\n",
            "Epoch: 6/20... Step: 430... Loss: 31.0049... Val Loss: 29.9346\n",
            "Epoch: 6/20... Step: 440... Loss: 25.0916... Val Loss: 24.3496\n",
            "Epoch: 6/20... Step: 450... Loss: 20.9811... Val Loss: 19.0638\n",
            "Epoch: 6/20... Step: 460... Loss: 21.1880... Val Loss: 20.7415\n",
            "Epoch: 7/20... Step: 470... Loss: 17.6535... Val Loss: 16.9317\n",
            "Epoch: 7/20... Step: 480... Loss: 16.0911... Val Loss: 18.6405\n",
            "Epoch: 7/20... Step: 490... Loss: 16.7931... Val Loss: 16.7776\n",
            "Epoch: 7/20... Step: 500... Loss: 18.4055... Val Loss: 17.8118\n",
            "Epoch: 7/20... Step: 510... Loss: 13.1620... Val Loss: 14.0651\n",
            "Epoch: 7/20... Step: 520... Loss: 14.9330... Val Loss: 14.1241\n",
            "Epoch: 7/20... Step: 530... Loss: 16.9646... Val Loss: 16.2447\n",
            "Epoch: 8/20... Step: 540... Loss: 20.9532... Val Loss: 19.7762\n",
            "Epoch: 8/20... Step: 550... Loss: 26.4467... Val Loss: 24.6745\n",
            "Epoch: 8/20... Step: 560... Loss: 25.0848... Val Loss: 25.4401\n",
            "Epoch: 8/20... Step: 570... Loss: 23.1504... Val Loss: 25.1755\n",
            "Epoch: 8/20... Step: 580... Loss: 26.5641... Val Loss: 25.4924\n",
            "Epoch: 8/20... Step: 590... Loss: 22.0881... Val Loss: 23.6085\n",
            "Epoch: 8/20... Step: 600... Loss: 20.2176... Val Loss: 19.9423\n",
            "Epoch: 8/20... Step: 610... Loss: 23.1706... Val Loss: 21.5250\n",
            "Epoch: 9/20... Step: 620... Loss: 23.4017... Val Loss: 22.8302\n",
            "Epoch: 9/20... Step: 630... Loss: 22.6292... Val Loss: 22.7995\n",
            "Epoch: 9/20... Step: 640... Loss: 19.5566... Val Loss: 17.0030\n",
            "Epoch: 9/20... Step: 650... Loss: 21.4240... Val Loss: 21.3565\n",
            "Epoch: 9/20... Step: 660... Loss: 17.6769... Val Loss: 18.4514\n",
            "Epoch: 9/20... Step: 670... Loss: 18.5167... Val Loss: 19.5990\n",
            "Epoch: 9/20... Step: 680... Loss: 17.3113... Val Loss: 18.8220\n",
            "Epoch: 9/20... Step: 690... Loss: 13.0673... Val Loss: 14.1563\n",
            "Epoch: 10/20... Step: 700... Loss: 16.2624... Val Loss: 17.0970\n",
            "Epoch: 10/20... Step: 710... Loss: 16.3044... Val Loss: 15.4224\n",
            "Epoch: 10/20... Step: 720... Loss: 15.3050... Val Loss: 15.8201\n",
            "Epoch: 10/20... Step: 730... Loss: 13.4118... Val Loss: 13.4782\n",
            "Epoch: 10/20... Step: 740... Loss: 15.5829... Val Loss: 13.6147\n",
            "Epoch: 10/20... Step: 750... Loss: 11.4727... Val Loss: 10.7966\n",
            "Epoch: 10/20... Step: 760... Loss: 12.9606... Val Loss: 12.4395\n",
            "Epoch: 10/20... Step: 770... Loss: 9.7969... Val Loss: 10.0989\n",
            "Epoch: 11/20... Step: 780... Loss: 8.2365... Val Loss: 7.7788\n",
            "Epoch: 11/20... Step: 790... Loss: 9.6750... Val Loss: 10.7187\n",
            "Epoch: 11/20... Step: 800... Loss: 9.1114... Val Loss: 9.6374\n",
            "Epoch: 11/20... Step: 810... Loss: 9.5594... Val Loss: 9.1842\n",
            "Epoch: 11/20... Step: 820... Loss: 9.5946... Val Loss: 8.5693\n",
            "Epoch: 11/20... Step: 830... Loss: 9.5248... Val Loss: 8.7801\n",
            "Epoch: 11/20... Step: 840... Loss: 12.4717... Val Loss: 11.2133\n",
            "Epoch: 12/20... Step: 850... Loss: 11.9796... Val Loss: 10.0956\n",
            "Epoch: 12/20... Step: 860... Loss: 9.9178... Val Loss: 9.2067\n",
            "Epoch: 12/20... Step: 870... Loss: 11.6325... Val Loss: 11.0408\n",
            "Epoch: 12/20... Step: 880... Loss: 10.6600... Val Loss: 9.4761\n",
            "Epoch: 12/20... Step: 890... Loss: 9.8842... Val Loss: 9.9935\n",
            "Epoch: 12/20... Step: 900... Loss: 10.8708... Val Loss: 10.6117\n",
            "Epoch: 12/20... Step: 910... Loss: 12.8123... Val Loss: 12.8962\n",
            "Epoch: 12/20... Step: 920... Loss: 12.4328... Val Loss: 12.2021\n",
            "Epoch: 13/20... Step: 930... Loss: 14.3562... Val Loss: 13.9066\n",
            "Epoch: 13/20... Step: 940... Loss: 13.4221... Val Loss: 11.7821\n",
            "Epoch: 13/20... Step: 950... Loss: 16.9451... Val Loss: 15.9723\n",
            "Epoch: 13/20... Step: 960... Loss: 17.4397... Val Loss: 17.2638\n",
            "Epoch: 13/20... Step: 970... Loss: 16.5552... Val Loss: 16.3685\n",
            "Epoch: 13/20... Step: 980... Loss: 14.3631... Val Loss: 14.6955\n",
            "Epoch: 13/20... Step: 990... Loss: 14.8885... Val Loss: 14.5190\n",
            "Epoch: 13/20... Step: 1000... Loss: 18.1347... Val Loss: 18.6117\n",
            "Epoch: 14/20... Step: 1010... Loss: 16.6472... Val Loss: 18.1092\n",
            "Epoch: 14/20... Step: 1020... Loss: 23.5931... Val Loss: 20.6445\n",
            "Epoch: 14/20... Step: 1030... Loss: 20.4511... Val Loss: 20.8666\n",
            "Epoch: 14/20... Step: 1040... Loss: 19.5464... Val Loss: 19.2406\n",
            "Epoch: 14/20... Step: 1050... Loss: 14.6116... Val Loss: 16.2896\n",
            "Epoch: 14/20... Step: 1060... Loss: 17.4501... Val Loss: 16.8611\n",
            "Epoch: 14/20... Step: 1070... Loss: 19.0958... Val Loss: 17.7585\n",
            "Epoch: 15/20... Step: 1080... Loss: 18.2151... Val Loss: 18.6262\n",
            "Epoch: 15/20... Step: 1090... Loss: 14.6902... Val Loss: 17.7185\n",
            "Epoch: 15/20... Step: 1100... Loss: 18.3403... Val Loss: 17.7536\n",
            "Epoch: 15/20... Step: 1110... Loss: 16.4968... Val Loss: 17.6447\n",
            "Epoch: 15/20... Step: 1120... Loss: 19.2197... Val Loss: 18.1450\n",
            "Epoch: 15/20... Step: 1130... Loss: 16.8233... Val Loss: 16.2846\n",
            "Epoch: 15/20... Step: 1140... Loss: 18.0708... Val Loss: 17.1846\n",
            "Epoch: 15/20... Step: 1150... Loss: 16.5794... Val Loss: 19.0432\n",
            "Epoch: 16/20... Step: 1160... Loss: 22.9112... Val Loss: 22.4556\n",
            "Epoch: 16/20... Step: 1170... Loss: 23.6562... Val Loss: 22.9366\n",
            "Epoch: 16/20... Step: 1180... Loss: 21.7078... Val Loss: 22.0219\n",
            "Epoch: 16/20... Step: 1190... Loss: 18.2159... Val Loss: 18.1862\n",
            "Epoch: 16/20... Step: 1200... Loss: 16.6075... Val Loss: 16.9953\n",
            "Epoch: 16/20... Step: 1210... Loss: 18.9265... Val Loss: 19.6723\n",
            "Epoch: 16/20... Step: 1220... Loss: 20.9781... Val Loss: 19.3790\n",
            "Epoch: 16/20... Step: 1230... Loss: 20.7376... Val Loss: 21.9297\n",
            "Epoch: 17/20... Step: 1240... Loss: 22.8548... Val Loss: 24.4965\n",
            "Epoch: 17/20... Step: 1250... Loss: 25.6796... Val Loss: 26.2102\n",
            "Epoch: 17/20... Step: 1260... Loss: 26.3266... Val Loss: 26.1236\n",
            "Epoch: 17/20... Step: 1270... Loss: 27.1952... Val Loss: 25.6393\n",
            "Epoch: 17/20... Step: 1280... Loss: 25.4678... Val Loss: 27.1071\n",
            "Epoch: 17/20... Step: 1290... Loss: 24.2278... Val Loss: 27.0295\n",
            "Epoch: 17/20... Step: 1300... Loss: 21.0559... Val Loss: 22.3333\n",
            "Epoch: 18/20... Step: 1310... Loss: 15.2700... Val Loss: 16.2924\n",
            "Epoch: 18/20... Step: 1320... Loss: 13.4361... Val Loss: 14.7322\n",
            "Epoch: 18/20... Step: 1330... Loss: 13.3899... Val Loss: 14.2379\n",
            "Epoch: 18/20... Step: 1340... Loss: 13.0999... Val Loss: 13.6969\n",
            "Epoch: 18/20... Step: 1350... Loss: 13.4184... Val Loss: 14.6273\n",
            "Epoch: 18/20... Step: 1360... Loss: 8.1283... Val Loss: 8.5790\n",
            "Epoch: 18/20... Step: 1370... Loss: 11.0294... Val Loss: 12.5004\n",
            "Epoch: 18/20... Step: 1380... Loss: 11.1493... Val Loss: 11.8689\n",
            "Epoch: 19/20... Step: 1390... Loss: 11.4648... Val Loss: 11.0927\n",
            "Epoch: 19/20... Step: 1400... Loss: 11.6334... Val Loss: 12.4476\n",
            "Epoch: 19/20... Step: 1410... Loss: 12.2351... Val Loss: 12.3532\n",
            "Epoch: 19/20... Step: 1420... Loss: 11.7537... Val Loss: 12.0134\n",
            "Epoch: 19/20... Step: 1430... Loss: 10.3231... Val Loss: 9.8466\n",
            "Epoch: 19/20... Step: 1440... Loss: 12.0559... Val Loss: 11.7459\n",
            "Epoch: 19/20... Step: 1450... Loss: 9.4525... Val Loss: 10.8626\n",
            "Epoch: 19/20... Step: 1460... Loss: 13.6201... Val Loss: 13.1207\n",
            "Epoch: 20/20... Step: 1470... Loss: 14.1511... Val Loss: 14.2775\n",
            "Epoch: 20/20... Step: 1480... Loss: 13.5283... Val Loss: 13.9939\n",
            "Epoch: 20/20... Step: 1490... Loss: 13.2800... Val Loss: 14.2235\n",
            "Epoch: 20/20... Step: 1500... Loss: 13.5063... Val Loss: 13.3059\n",
            "Epoch: 20/20... Step: 1510... Loss: 12.5898... Val Loss: 12.6705\n",
            "Epoch: 20/20... Step: 1520... Loss: 14.0956... Val Loss: 13.6714\n",
            "Epoch: 20/20... Step: 1530... Loss: 12.5304... Val Loss: 11.6966\n",
            "Epoch: 20/20... Step: 1540... Loss: 10.6360... Val Loss: 10.2722\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 3\n",
        "\n",
        "В комментарии напишите кусочек текста в стиле Шекспира, сгененированного вашей моделью. Выберите кусочек, который вам больше всего понравился!"
      ],
      "metadata": {
        "id": "98KdJ-fIT_Wv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Делаем предсказания"
      ],
      "metadata": {
        "id": "KtkvljT0UNeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "    \"\"\"Given a character, predict the next character.\n",
        "    Returns the predicted character and the hidden state.\n",
        "    \"\"\"\n",
        "\n",
        "    # tensor inputs\n",
        "    x = np.array([[net.char2int[char]]])\n",
        "    x = one_hot_encode(x, len(net.chars))\n",
        "    inputs = torch.from_numpy(x)\n",
        "\n",
        "    if train_on_gpu:\n",
        "        inputs = inputs.cuda()\n",
        "\n",
        "    # detach hidden state from history\n",
        "    h = tuple([each.data for each in h])\n",
        "    # get the output of the model\n",
        "    out, h = net(inputs, h)\n",
        "\n",
        "    # get the character probabilities\n",
        "    p = F.softmax(out, dim=1).data\n",
        "    if train_on_gpu:\n",
        "        p = p.cpu()  # move to cpu\n",
        "\n",
        "    # get top characters\n",
        "    if top_k is None:\n",
        "        top_ch = np.arange(len(net.chars))\n",
        "    else:\n",
        "        p, top_ch = p.topk(top_k)\n",
        "        top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "    # select the likely next character with some element of randomness\n",
        "    p = p.numpy().squeeze()\n",
        "    char = np.random.choice(top_ch, p=p / p.sum())\n",
        "\n",
        "    # return the encoded value of the predicted char and the hidden state\n",
        "    return net.int2char[char], h"
      ],
      "metadata": {
        "id": "W2Vw_BiPUHZo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Priming"
      ],
      "metadata": {
        "id": "pAx-qhzxUTNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(net, size, prime=\"The\", top_k=None):\n",
        "\n",
        "    if train_on_gpu:\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "\n",
        "    net.eval()  # eval mode\n",
        "\n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "\n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return \"\".join(chars)"
      ],
      "metadata": {
        "id": "Sxr6xGDPUQMu"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample(net, 500, prime=\"The King Lear\", top_k=5))"
      ],
      "metadata": {
        "id": "czaqKp57VQX5",
        "outputId": "ed5c9aa6-b0c2-4e32-8381-7b0c5580e2c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The King Learing,\n",
            "The well that is me to the sent to speak;\n",
            "A though thou senseling as I dead to mischarged,\n",
            "Take thised the weary that with him, and some\n",
            "This to be thou to take his livery: to make you,\n",
            "Treat those offectors is the welcome, well,\n",
            "Who shall been the present too that stay.\n",
            "\n",
            "KING RICHARD III:\n",
            "Thas, the may life, we can thy lons.\n",
            "\n",
            "CORIOLANUS:\n",
            "He may not send to stor to him, that whils\n",
            "Ore wearthy whict to make yet still be now,\n",
            "That some wind with that was with a thate was as it,\n",
            "As is my shore \n"
          ]
        }
      ]
    }
  ]
}