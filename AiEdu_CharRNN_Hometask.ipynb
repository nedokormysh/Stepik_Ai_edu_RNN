{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nedokormysh/Stepik_Ai_edu_RNN/blob/week_5_char_RNN/AiEdu_CharRNN_Hometask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W8R8WgZceEk"
      },
      "source": [
        "# Домашнее задание\n",
        "\n",
        "В этом домашнем задании вы обучите рекуррентную сеть для генерации текстов в стиле Шекспира."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqUOE2flceEl"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wHfCDyzceEl"
      },
      "source": [
        "## Загрузим данные\n",
        "\n",
        "Загрузим текстовый файл с пьесами Шекспира."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/aiedu-courses/rnn_bootcamp/main/shakespeare.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXAxa--jPUSd",
        "outputId": "24bd3104-5ec8-4813-d3e7-0296ad894ec6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-15 10:50:42--  https://raw.githubusercontent.com/aiedu-courses/rnn_bootcamp/main/shakespeare.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘shakespeare.txt’\n",
            "\n",
            "\rshakespeare.txt       0%[                    ]       0  --.-KB/s               \rshakespeare.txt     100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-01-15 10:50:42 (57.3 MB/s) - ‘shakespeare.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Эта задача аналогична задаче, разобранной на вебинаре, поэтому код мы вам не предоставляем, а предлагаем или написать с нуля, или воспользоваться кодом с вебинара.*"
      ],
      "metadata": {
        "id": "YkQhIGj-nzYx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b34kfqIOceEl"
      },
      "source": [
        "# open text file and read in data as `text`\n",
        "with open(\"shakespeare.txt\", \"r\") as f:\n",
        "    text = f.read()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[:100]"
      ],
      "metadata": {
        "id": "07b3AMdeCP_4",
        "outputId": "9138fe1c-031b-43fc-83f8-2e979fea9c8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Токенизация"
      ],
      "metadata": {
        "id": "-YCmrxDoCUMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = tuple(set(text))\n",
        "\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "metadata": {
        "id": "YhH1dl5QCUri"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "int2char.items()"
      ],
      "metadata": {
        "id": "eFLSwK_iC9fk",
        "outputId": "c757558d-9f93-43fb-f8ca-7e1e605d5b02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([(0, 'V'), (1, '3'), (2, 'H'), (3, 'S'), (4, '?'), (5, 'J'), (6, 's'), (7, 'r'), (8, '.'), (9, 'Z'), (10, 'z'), (11, 'G'), (12, '$'), (13, 'B'), (14, 'Y'), (15, 'A'), (16, 'I'), (17, 'm'), (18, 'b'), (19, '\\n'), (20, 'R'), (21, '&'), (22, 'y'), (23, 'P'), (24, '!'), (25, 'D'), (26, 'q'), (27, ':'), (28, 'd'), (29, 'p'), (30, \"'\"), (31, 'O'), (32, 'a'), (33, 'X'), (34, 'n'), (35, ' '), (36, 'o'), (37, 'c'), (38, 'M'), (39, 'Q'), (40, 'L'), (41, 'v'), (42, 'l'), (43, 'W'), (44, 'j'), (45, 'i'), (46, 'h'), (47, 'C'), (48, 'w'), (49, 'T'), (50, 'x'), (51, 'E'), (52, ';'), (53, 'f'), (54, ','), (55, 'g'), (56, '-'), (57, 'K'), (58, 'e'), (59, 'U'), (60, 'k'), (61, 'F'), (62, 't'), (63, 'N'), (64, 'u')])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# int2char"
      ],
      "metadata": {
        "id": "jVU4l_VzC5y4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ii, ch in int2char.items():\n",
        "  print(ii, ch)"
      ],
      "metadata": {
        "id": "eCUT-8qyCdeS",
        "outputId": "d38d1e55-79e7-4e34-e95a-3854bc76be2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 V\n",
            "1 3\n",
            "2 H\n",
            "3 S\n",
            "4 ?\n",
            "5 J\n",
            "6 s\n",
            "7 r\n",
            "8 .\n",
            "9 Z\n",
            "10 z\n",
            "11 G\n",
            "12 $\n",
            "13 B\n",
            "14 Y\n",
            "15 A\n",
            "16 I\n",
            "17 m\n",
            "18 b\n",
            "19 \n",
            "\n",
            "20 R\n",
            "21 &\n",
            "22 y\n",
            "23 P\n",
            "24 !\n",
            "25 D\n",
            "26 q\n",
            "27 :\n",
            "28 d\n",
            "29 p\n",
            "30 '\n",
            "31 O\n",
            "32 a\n",
            "33 X\n",
            "34 n\n",
            "35  \n",
            "36 o\n",
            "37 c\n",
            "38 M\n",
            "39 Q\n",
            "40 L\n",
            "41 v\n",
            "42 l\n",
            "43 W\n",
            "44 j\n",
            "45 i\n",
            "46 h\n",
            "47 C\n",
            "48 w\n",
            "49 T\n",
            "50 x\n",
            "51 E\n",
            "52 ;\n",
            "53 f\n",
            "54 ,\n",
            "55 g\n",
            "56 -\n",
            "57 K\n",
            "58 e\n",
            "59 U\n",
            "60 k\n",
            "61 F\n",
            "62 t\n",
            "63 N\n",
            "64 u\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Предобработка данных"
      ],
      "metadata": {
        "id": "9ZPWc-f3DRd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "\n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "\n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.0\n",
        "    # print(arr)\n",
        "    # print(arr.flatten())\n",
        "    # print(np.arange(one_hot.shape[0]))\n",
        "    # print(one_hot[1])\n",
        "\n",
        "\n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "\n",
        "    return one_hot"
      ],
      "metadata": {
        "id": "GThlDmOcDBct"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Создаем мини-батчи (mini-batchs)\n"
      ],
      "metadata": {
        "id": "iY0YM075GDHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(int_words, batch_size, seq_length):\n",
        "    # 1. Truncate text, so there are only full batches\n",
        "    window_size = seq_length + 1\n",
        "    batch_size_total = batch_size * window_size\n",
        "    n_batches = len(int_words) // batch_size_total\n",
        "    int_words = int_words[: n_batches * batch_size_total]\n",
        "\n",
        "    # 2. Reshape into batch_size rows\n",
        "    int_words = int_words.reshape((batch_size, -1))\n",
        "\n",
        "    # 3. Iterate through the text matrix\n",
        "    for position in range(0, int_words.shape[1], window_size):\n",
        "        x = int_words[:, position : position + window_size - 1]\n",
        "        y = int_words[:, position + 1 : position + window_size]\n",
        "        yield x, y"
      ],
      "metadata": {
        "id": "jSB7z4JJH1Lh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Зададим архитектуру"
      ],
      "metadata": {
        "id": "Chhh-npnKeeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if train_on_gpu:\n",
        "    print(\"Training on GPU!\")\n",
        "else:\n",
        "    print(\"No GPU available, training on CPU; consider making n_epochs very small.\")"
      ],
      "metadata": {
        "id": "zle4bLc-Kjxg",
        "outputId": "31ee02c6-8fad-44ae-995a-619723e318ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on GPU!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "\n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "\n",
        "        ## TODO: define the LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True\n",
        "        )\n",
        "\n",
        "        ## TODO: define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        ## TODO: define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"Forward pass through the network.\n",
        "        These inputs are x, and the hidden/cell state `hidden`.\"\"\"\n",
        "\n",
        "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "\n",
        "        ## TODO: pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "\n",
        "        # Stack up LSTM outputs using view\n",
        "        # you may need to use contiguous to reshape the output\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "\n",
        "        ## TODO: put x through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"Initializes hidden state\"\"\"\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        if train_on_gpu:\n",
        "            hidden = (\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "            )\n",
        "        else:\n",
        "            hidden = (\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "            )\n",
        "\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "VegodbYcKioc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обучим модель"
      ],
      "metadata": {
        "id": "G5J-x8pLNdyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    net,\n",
        "    data,\n",
        "    epochs=10,\n",
        "    batch_size=10,\n",
        "    seq_length=50,\n",
        "    lr=0.001,\n",
        "    clip=5,\n",
        "    val_frac=0.1,\n",
        "    print_every=10,\n",
        "):\n",
        "    net.train()\n",
        "\n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data) * (1 - val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "\n",
        "    if train_on_gpu:\n",
        "        net.cuda()\n",
        "\n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "\n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "\n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "            if train_on_gpu:\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "\n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "\n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "\n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                    inputs, targets = x, y\n",
        "                    if train_on_gpu:\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(\n",
        "                        output, targets.view(batch_size * seq_length).long()\n",
        "                    )\n",
        "\n",
        "                    val_losses.append(val_loss.item())\n",
        "\n",
        "                net.train()  # reset to train mode after iterationg through validation data\n",
        "\n",
        "                print(\n",
        "                    \"Epoch: {}/{}...\".format(e + 1, epochs),\n",
        "                    \"Step: {}...\".format(counter),\n",
        "                    \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                    \"Val Loss: {:.4f}\".format(np.mean(val_losses)),\n",
        "                )"
      ],
      "metadata": {
        "id": "p_C9hWU7NhSV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Определим модель"
      ],
      "metadata": {
        "id": "IKTO5j4POVyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define and print the net\n",
        "n_hidden = 512\n",
        "n_layers = 2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "metadata": {
        "id": "hwFZbhGnOY1k",
        "outputId": "7fdbd7ab-a3bb-4ff8-fd1b-65d28678503b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(65, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=65, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Установим гиперпараметры"
      ],
      "metadata": {
        "id": "i_Hf-orZOgRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 20  # start smaller if you are just testing initial behavior\n",
        "\n",
        "# train the model\n",
        "train(\n",
        "    net,\n",
        "    encoded,\n",
        "    epochs=n_epochs,\n",
        "    batch_size=batch_size,\n",
        "    seq_length=seq_length,\n",
        "    lr=0.001,\n",
        "    print_every=10,\n",
        ")"
      ],
      "metadata": {
        "id": "aP0usZjmOg8f",
        "outputId": "0d716c9a-761d-4b69-adf7-9c11c6c7ba8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/20... Step: 10... Loss: 3.3681... Val Loss: 3.4055\n",
            "Epoch: 1/20... Step: 20... Loss: 3.3303... Val Loss: 3.3551\n",
            "Epoch: 1/20... Step: 30... Loss: 3.3127... Val Loss: 3.3461\n",
            "Epoch: 1/20... Step: 40... Loss: 3.3440... Val Loss: 3.3412\n",
            "Epoch: 1/20... Step: 50... Loss: 3.3337... Val Loss: 3.3358\n",
            "Epoch: 1/20... Step: 60... Loss: 3.2899... Val Loss: 3.3335\n",
            "Epoch: 1/20... Step: 70... Loss: 3.3137... Val Loss: 3.3239\n",
            "Epoch: 2/20... Step: 80... Loss: 3.2742... Val Loss: 3.3098\n",
            "Epoch: 2/20... Step: 90... Loss: 3.2585... Val Loss: 3.2709\n",
            "Epoch: 2/20... Step: 100... Loss: 3.2303... Val Loss: 3.2131\n",
            "Epoch: 2/20... Step: 110... Loss: 3.1656... Val Loss: 3.1471\n",
            "Epoch: 2/20... Step: 120... Loss: 3.1099... Val Loss: 3.0748\n",
            "Epoch: 2/20... Step: 130... Loss: 3.0044... Val Loss: 2.9673\n",
            "Epoch: 2/20... Step: 140... Loss: 2.9214... Val Loss: 2.8677\n",
            "Epoch: 2/20... Step: 150... Loss: 2.8452... Val Loss: 2.8029\n",
            "Epoch: 3/20... Step: 160... Loss: 2.7543... Val Loss: 2.7146\n",
            "Epoch: 3/20... Step: 170... Loss: 2.6774... Val Loss: 2.6137\n",
            "Epoch: 3/20... Step: 180... Loss: 2.6227... Val Loss: 2.5426\n",
            "Epoch: 3/20... Step: 190... Loss: 2.5382... Val Loss: 2.4838\n",
            "Epoch: 3/20... Step: 200... Loss: 2.4750... Val Loss: 2.4318\n",
            "Epoch: 3/20... Step: 210... Loss: 2.4812... Val Loss: 2.4052\n",
            "Epoch: 3/20... Step: 220... Loss: 2.4194... Val Loss: 2.3732\n",
            "Epoch: 3/20... Step: 230... Loss: 2.3995... Val Loss: 2.3605\n",
            "Epoch: 4/20... Step: 240... Loss: 2.3617... Val Loss: 2.3265\n",
            "Epoch: 4/20... Step: 250... Loss: 2.3510... Val Loss: 2.2985\n",
            "Epoch: 4/20... Step: 260... Loss: 2.3443... Val Loss: 2.2749\n",
            "Epoch: 4/20... Step: 270... Loss: 2.3203... Val Loss: 2.2555\n",
            "Epoch: 4/20... Step: 280... Loss: 2.2790... Val Loss: 2.2499\n",
            "Epoch: 4/20... Step: 290... Loss: 2.2724... Val Loss: 2.2281\n",
            "Epoch: 4/20... Step: 300... Loss: 2.2342... Val Loss: 2.2030\n",
            "Epoch: 5/20... Step: 310... Loss: 2.2312... Val Loss: 2.1887\n",
            "Epoch: 5/20... Step: 320... Loss: 2.2061... Val Loss: 2.1754\n",
            "Epoch: 5/20... Step: 330... Loss: 2.1959... Val Loss: 2.1552\n",
            "Epoch: 5/20... Step: 340... Loss: 2.1925... Val Loss: 2.1524\n",
            "Epoch: 5/20... Step: 350... Loss: 2.1711... Val Loss: 2.1341\n",
            "Epoch: 5/20... Step: 360... Loss: 2.1489... Val Loss: 2.1280\n",
            "Epoch: 5/20... Step: 370... Loss: 2.1260... Val Loss: 2.1117\n",
            "Epoch: 5/20... Step: 380... Loss: 2.1298... Val Loss: 2.0933\n",
            "Epoch: 6/20... Step: 390... Loss: 2.0884... Val Loss: 2.0798\n",
            "Epoch: 6/20... Step: 400... Loss: 2.1094... Val Loss: 2.0686\n",
            "Epoch: 6/20... Step: 410... Loss: 2.0571... Val Loss: 2.0612\n",
            "Epoch: 6/20... Step: 420... Loss: 2.0722... Val Loss: 2.0518\n",
            "Epoch: 6/20... Step: 430... Loss: 2.0334... Val Loss: 2.0418\n",
            "Epoch: 6/20... Step: 440... Loss: 2.0480... Val Loss: 2.0300\n",
            "Epoch: 6/20... Step: 450... Loss: 2.0190... Val Loss: 2.0258\n",
            "Epoch: 6/20... Step: 460... Loss: 2.0277... Val Loss: 2.0147\n",
            "Epoch: 7/20... Step: 470... Loss: 2.0313... Val Loss: 2.0052\n",
            "Epoch: 7/20... Step: 480... Loss: 2.0272... Val Loss: 1.9935\n",
            "Epoch: 7/20... Step: 490... Loss: 1.9735... Val Loss: 1.9837\n",
            "Epoch: 7/20... Step: 500... Loss: 1.9816... Val Loss: 1.9759\n",
            "Epoch: 7/20... Step: 510... Loss: 1.9770... Val Loss: 1.9726\n",
            "Epoch: 7/20... Step: 520... Loss: 1.9798... Val Loss: 1.9693\n",
            "Epoch: 7/20... Step: 530... Loss: 1.9164... Val Loss: 1.9522\n",
            "Epoch: 8/20... Step: 540... Loss: 1.9815... Val Loss: 1.9499\n",
            "Epoch: 8/20... Step: 550... Loss: 1.9471... Val Loss: 1.9425\n",
            "Epoch: 8/20... Step: 560... Loss: 1.9060... Val Loss: 1.9340\n",
            "Epoch: 8/20... Step: 570... Loss: 1.9033... Val Loss: 1.9293\n",
            "Epoch: 8/20... Step: 580... Loss: 1.9078... Val Loss: 1.9214\n",
            "Epoch: 8/20... Step: 590... Loss: 1.9167... Val Loss: 1.9115\n",
            "Epoch: 8/20... Step: 600... Loss: 1.8788... Val Loss: 1.9086\n",
            "Epoch: 8/20... Step: 610... Loss: 1.8849... Val Loss: 1.9003\n",
            "Epoch: 9/20... Step: 620... Loss: 1.9037... Val Loss: 1.8947\n",
            "Epoch: 9/20... Step: 630... Loss: 1.8915... Val Loss: 1.8912\n",
            "Epoch: 9/20... Step: 640... Loss: 1.8456... Val Loss: 1.8874\n",
            "Epoch: 9/20... Step: 650... Loss: 1.8655... Val Loss: 1.8783\n",
            "Epoch: 9/20... Step: 660... Loss: 1.8373... Val Loss: 1.8690\n",
            "Epoch: 9/20... Step: 670... Loss: 1.8509... Val Loss: 1.8655\n",
            "Epoch: 9/20... Step: 680... Loss: 1.8357... Val Loss: 1.8615\n",
            "Epoch: 9/20... Step: 690... Loss: 1.8288... Val Loss: 1.8561\n",
            "Epoch: 10/20... Step: 700... Loss: 1.8210... Val Loss: 1.8470\n",
            "Epoch: 10/20... Step: 710... Loss: 1.8284... Val Loss: 1.8444\n",
            "Epoch: 10/20... Step: 720... Loss: 1.8255... Val Loss: 1.8359\n",
            "Epoch: 10/20... Step: 730... Loss: 1.8481... Val Loss: 1.8392\n",
            "Epoch: 10/20... Step: 740... Loss: 1.8025... Val Loss: 1.8316\n",
            "Epoch: 10/20... Step: 750... Loss: 1.7851... Val Loss: 1.8271\n",
            "Epoch: 10/20... Step: 760... Loss: 1.7872... Val Loss: 1.8170\n",
            "Epoch: 10/20... Step: 770... Loss: 1.7865... Val Loss: 1.8153\n",
            "Epoch: 11/20... Step: 780... Loss: 1.8164... Val Loss: 1.8120\n",
            "Epoch: 11/20... Step: 790... Loss: 1.7971... Val Loss: 1.8088\n",
            "Epoch: 11/20... Step: 800... Loss: 1.7767... Val Loss: 1.7959\n",
            "Epoch: 11/20... Step: 810... Loss: 1.7538... Val Loss: 1.7926\n",
            "Epoch: 11/20... Step: 820... Loss: 1.7480... Val Loss: 1.7936\n",
            "Epoch: 11/20... Step: 830... Loss: 1.7577... Val Loss: 1.7950\n",
            "Epoch: 11/20... Step: 840... Loss: 1.7526... Val Loss: 1.7841\n",
            "Epoch: 12/20... Step: 850... Loss: 1.7820... Val Loss: 1.7806\n",
            "Epoch: 12/20... Step: 860... Loss: 1.7452... Val Loss: 1.7761\n",
            "Epoch: 12/20... Step: 870... Loss: 1.7423... Val Loss: 1.7741\n",
            "Epoch: 12/20... Step: 880... Loss: 1.6935... Val Loss: 1.7696\n",
            "Epoch: 12/20... Step: 890... Loss: 1.7223... Val Loss: 1.7639\n",
            "Epoch: 12/20... Step: 900... Loss: 1.7270... Val Loss: 1.7621\n",
            "Epoch: 12/20... Step: 910... Loss: 1.7057... Val Loss: 1.7625\n",
            "Epoch: 12/20... Step: 920... Loss: 1.7217... Val Loss: 1.7575\n",
            "Epoch: 13/20... Step: 930... Loss: 1.7038... Val Loss: 1.7564\n",
            "Epoch: 13/20... Step: 940... Loss: 1.7304... Val Loss: 1.7506\n",
            "Epoch: 13/20... Step: 950... Loss: 1.7053... Val Loss: 1.7399\n",
            "Epoch: 13/20... Step: 960... Loss: 1.6862... Val Loss: 1.7398\n",
            "Epoch: 13/20... Step: 970... Loss: 1.6799... Val Loss: 1.7388\n",
            "Epoch: 13/20... Step: 980... Loss: 1.6827... Val Loss: 1.7361\n",
            "Epoch: 13/20... Step: 990... Loss: 1.6972... Val Loss: 1.7316\n",
            "Epoch: 13/20... Step: 1000... Loss: 1.6973... Val Loss: 1.7330\n",
            "Epoch: 14/20... Step: 1010... Loss: 1.6808... Val Loss: 1.7287\n",
            "Epoch: 14/20... Step: 1020... Loss: 1.6864... Val Loss: 1.7211\n",
            "Epoch: 14/20... Step: 1030... Loss: 1.7131... Val Loss: 1.7175\n",
            "Epoch: 14/20... Step: 1040... Loss: 1.6916... Val Loss: 1.7124\n",
            "Epoch: 14/20... Step: 1050... Loss: 1.6764... Val Loss: 1.7112\n",
            "Epoch: 14/20... Step: 1060... Loss: 1.6948... Val Loss: 1.7124\n",
            "Epoch: 14/20... Step: 1070... Loss: 1.6651... Val Loss: 1.7067\n",
            "Epoch: 15/20... Step: 1080... Loss: 1.6562... Val Loss: 1.7034\n",
            "Epoch: 15/20... Step: 1090... Loss: 1.6580... Val Loss: 1.7027\n",
            "Epoch: 15/20... Step: 1100... Loss: 1.6319... Val Loss: 1.6970\n",
            "Epoch: 15/20... Step: 1110... Loss: 1.6545... Val Loss: 1.6912\n",
            "Epoch: 15/20... Step: 1120... Loss: 1.6524... Val Loss: 1.6865\n",
            "Epoch: 15/20... Step: 1130... Loss: 1.6510... Val Loss: 1.6902\n",
            "Epoch: 15/20... Step: 1140... Loss: 1.6333... Val Loss: 1.6863\n",
            "Epoch: 15/20... Step: 1150... Loss: 1.6546... Val Loss: 1.6823\n",
            "Epoch: 16/20... Step: 1160... Loss: 1.6313... Val Loss: 1.6848\n",
            "Epoch: 16/20... Step: 1170... Loss: 1.6385... Val Loss: 1.6811\n",
            "Epoch: 16/20... Step: 1180... Loss: 1.6261... Val Loss: 1.6746\n",
            "Epoch: 16/20... Step: 1190... Loss: 1.6222... Val Loss: 1.6750\n",
            "Epoch: 16/20... Step: 1200... Loss: 1.5938... Val Loss: 1.6720\n",
            "Epoch: 16/20... Step: 1210... Loss: 1.6150... Val Loss: 1.6691\n",
            "Epoch: 16/20... Step: 1220... Loss: 1.6173... Val Loss: 1.6670\n",
            "Epoch: 16/20... Step: 1230... Loss: 1.6333... Val Loss: 1.6658\n",
            "Epoch: 17/20... Step: 1240... Loss: 1.6333... Val Loss: 1.6627\n",
            "Epoch: 17/20... Step: 1250... Loss: 1.6541... Val Loss: 1.6596\n",
            "Epoch: 17/20... Step: 1260... Loss: 1.6052... Val Loss: 1.6540\n",
            "Epoch: 17/20... Step: 1270... Loss: 1.6089... Val Loss: 1.6542\n",
            "Epoch: 17/20... Step: 1280... Loss: 1.6156... Val Loss: 1.6540\n",
            "Epoch: 17/20... Step: 1290... Loss: 1.6147... Val Loss: 1.6552\n",
            "Epoch: 17/20... Step: 1300... Loss: 1.5642... Val Loss: 1.6493\n",
            "Epoch: 18/20... Step: 1310... Loss: 1.6361... Val Loss: 1.6515\n",
            "Epoch: 18/20... Step: 1320... Loss: 1.6220... Val Loss: 1.6502\n",
            "Epoch: 18/20... Step: 1330... Loss: 1.5765... Val Loss: 1.6434\n",
            "Epoch: 18/20... Step: 1340... Loss: 1.5862... Val Loss: 1.6415\n",
            "Epoch: 18/20... Step: 1350... Loss: 1.5742... Val Loss: 1.6390\n",
            "Epoch: 18/20... Step: 1360... Loss: 1.6055... Val Loss: 1.6424\n",
            "Epoch: 18/20... Step: 1370... Loss: 1.5633... Val Loss: 1.6391\n",
            "Epoch: 18/20... Step: 1380... Loss: 1.5925... Val Loss: 1.6367\n",
            "Epoch: 19/20... Step: 1390... Loss: 1.6169... Val Loss: 1.6351\n",
            "Epoch: 19/20... Step: 1400... Loss: 1.5900... Val Loss: 1.6352\n",
            "Epoch: 19/20... Step: 1410... Loss: 1.5546... Val Loss: 1.6288\n",
            "Epoch: 19/20... Step: 1420... Loss: 1.5755... Val Loss: 1.6262\n",
            "Epoch: 19/20... Step: 1430... Loss: 1.5559... Val Loss: 1.6247\n",
            "Epoch: 19/20... Step: 1440... Loss: 1.5827... Val Loss: 1.6292\n",
            "Epoch: 19/20... Step: 1450... Loss: 1.5616... Val Loss: 1.6250\n",
            "Epoch: 19/20... Step: 1460... Loss: 1.5710... Val Loss: 1.6204\n",
            "Epoch: 20/20... Step: 1470... Loss: 1.5677... Val Loss: 1.6203\n",
            "Epoch: 20/20... Step: 1480... Loss: 1.5592... Val Loss: 1.6161\n",
            "Epoch: 20/20... Step: 1490... Loss: 1.5569... Val Loss: 1.6154\n",
            "Epoch: 20/20... Step: 1500... Loss: 1.5686... Val Loss: 1.6151\n",
            "Epoch: 20/20... Step: 1510... Loss: 1.5508... Val Loss: 1.6110\n",
            "Epoch: 20/20... Step: 1520... Loss: 1.5437... Val Loss: 1.6130\n",
            "Epoch: 20/20... Step: 1530... Loss: 1.5458... Val Loss: 1.6121\n",
            "Epoch: 20/20... Step: 1540... Loss: 1.5511... Val Loss: 1.6088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 1\n",
        "\n",
        "Выведите общее число параметров (весов) сети, генерирующей тексты в стиле Шекспира. Сеть задавайте такую же, как и в ноутбуке на вебинаре, с теми же гиперпараметрами.\n",
        "\n",
        "Подсказка: число параметров на каждом слое сети `model` можно посмотреть так:\n",
        "\n",
        "for layer in net.parameters():\n",
        "\n",
        "    ...."
      ],
      "metadata": {
        "id": "ty9_sNx-TZSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum = 0\n",
        "for layer in net.parameters():\n",
        "  # print(layer.numel())\n",
        "  sum += layer.numel()\n",
        "\n",
        "sum"
      ],
      "metadata": {
        "id": "u7GlhiEuP4gT",
        "outputId": "10e40bb6-b329-4b72-8a4f-1aeb22f8bd4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3320385"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "def count_parameters(model):\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad:\n",
        "            continue\n",
        "        params = parameter.numel()\n",
        "        table.add_row([name, params])\n",
        "        total_params += params\n",
        "    print(table)\n",
        "    print(f\"Total Trainable Params: {total_params}\")\n",
        "    return total_params\n",
        "\n",
        "count_parameters(net)"
      ],
      "metadata": {
        "id": "x_A577qLRY_o",
        "outputId": "3b29b5d0-3c89-422f-8299-0ea1a43f31f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+------------+\n",
            "|      Modules      | Parameters |\n",
            "+-------------------+------------+\n",
            "| lstm.weight_ih_l0 |   133120   |\n",
            "| lstm.weight_hh_l0 |  1048576   |\n",
            "|  lstm.bias_ih_l0  |    2048    |\n",
            "|  lstm.bias_hh_l0  |    2048    |\n",
            "| lstm.weight_ih_l1 |  1048576   |\n",
            "| lstm.weight_hh_l1 |  1048576   |\n",
            "|  lstm.bias_ih_l1  |    2048    |\n",
            "|  lstm.bias_hh_l1  |    2048    |\n",
            "|     fc.weight     |   33280    |\n",
            "|      fc.bias      |     65     |\n",
            "+-------------------+------------+\n",
            "Total Trainable Params: 3320385\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3320385"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# only trainable parameters\n",
        "pytorch_total_params = np.array([p.numel() for p in net.parameters() if p.requires_grad]).sum()\n",
        "pytorch_total_params"
      ],
      "metadata": {
        "id": "ODcXomoGRNGi",
        "outputId": "1631318f-83d3-4e82-f5a3-0e5139ad46b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3320385"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 2\n",
        "\n",
        "Будет ли сеть обучаться, если задать learning rate равным 1?"
      ],
      "metadata": {
        "id": "B_ObCHw2ThT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define and print the net\n",
        "n_hidden = 512\n",
        "n_layers = 2\n",
        "\n",
        "net_1 = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "metadata": {
        "id": "17GWsX6wuusj",
        "outputId": "aa7da31e-cde9-46d3-89c3-01f2fb7825b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(65, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=65, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 20  # start smaller if you are just testing initial behavior\n",
        "\n",
        "# train the model\n",
        "train(\n",
        "    net_1 ,\n",
        "    encoded,\n",
        "    epochs=n_epochs,\n",
        "    batch_size=batch_size,\n",
        "    seq_length=seq_length,\n",
        "    lr=1,\n",
        "    print_every=10,\n",
        ")"
      ],
      "metadata": {
        "id": "nJdnMUJzTodt",
        "outputId": "b59943c5-f274-4e9c-9629-8db7098a0436",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/20... Step: 10... Loss: 833.3704... Val Loss: 903.6915\n",
            "Epoch: 1/20... Step: 20... Loss: 2166.0168... Val Loss: 2268.6631\n",
            "Epoch: 1/20... Step: 30... Loss: 2847.2141... Val Loss: 2877.4757\n",
            "Epoch: 1/20... Step: 40... Loss: 2427.1572... Val Loss: 2278.6945\n",
            "Epoch: 1/20... Step: 50... Loss: 1935.0138... Val Loss: 1795.6775\n",
            "Epoch: 1/20... Step: 60... Loss: 979.0643... Val Loss: 967.7872\n",
            "Epoch: 1/20... Step: 70... Loss: 673.7847... Val Loss: 682.7332\n",
            "Epoch: 2/20... Step: 80... Loss: 598.1299... Val Loss: 600.8639\n",
            "Epoch: 2/20... Step: 90... Loss: 497.3957... Val Loss: 411.3968\n",
            "Epoch: 2/20... Step: 100... Loss: 391.0258... Val Loss: 368.8602\n",
            "Epoch: 2/20... Step: 110... Loss: 320.0061... Val Loss: 357.0964\n",
            "Epoch: 2/20... Step: 120... Loss: 221.5636... Val Loss: 242.6220\n",
            "Epoch: 2/20... Step: 130... Loss: 281.3942... Val Loss: 255.5488\n",
            "Epoch: 2/20... Step: 140... Loss: 282.5867... Val Loss: 276.1578\n",
            "Epoch: 2/20... Step: 150... Loss: 362.7218... Val Loss: 369.9529\n",
            "Epoch: 3/20... Step: 160... Loss: 329.6917... Val Loss: 326.4288\n",
            "Epoch: 3/20... Step: 170... Loss: 365.2351... Val Loss: 402.6811\n",
            "Epoch: 3/20... Step: 180... Loss: 378.6315... Val Loss: 348.9323\n",
            "Epoch: 3/20... Step: 190... Loss: 312.9059... Val Loss: 330.4358\n",
            "Epoch: 3/20... Step: 200... Loss: 414.5541... Val Loss: 376.5315\n",
            "Epoch: 3/20... Step: 210... Loss: 411.7930... Val Loss: 398.8014\n",
            "Epoch: 3/20... Step: 220... Loss: 346.2970... Val Loss: 330.8777\n",
            "Epoch: 3/20... Step: 230... Loss: 310.2112... Val Loss: 314.4408\n",
            "Epoch: 4/20... Step: 240... Loss: 333.4692... Val Loss: 343.8696\n",
            "Epoch: 4/20... Step: 250... Loss: 396.7476... Val Loss: 412.0644\n",
            "Epoch: 4/20... Step: 260... Loss: 338.2238... Val Loss: 343.2376\n",
            "Epoch: 4/20... Step: 270... Loss: 311.4292... Val Loss: 358.9531\n",
            "Epoch: 4/20... Step: 280... Loss: 337.1835... Val Loss: 370.8929\n",
            "Epoch: 4/20... Step: 290... Loss: 298.4249... Val Loss: 321.9698\n",
            "Epoch: 4/20... Step: 300... Loss: 351.9533... Val Loss: 370.0369\n",
            "Epoch: 5/20... Step: 310... Loss: 324.1820... Val Loss: 367.6717\n",
            "Epoch: 5/20... Step: 320... Loss: 410.1830... Val Loss: 391.7676\n",
            "Epoch: 5/20... Step: 330... Loss: 374.7492... Val Loss: 359.6975\n",
            "Epoch: 5/20... Step: 340... Loss: 349.1694... Val Loss: 370.9753\n",
            "Epoch: 5/20... Step: 350... Loss: 412.4898... Val Loss: 445.4933\n",
            "Epoch: 5/20... Step: 360... Loss: 420.9933... Val Loss: 432.9693\n",
            "Epoch: 5/20... Step: 370... Loss: 367.2050... Val Loss: 322.2467\n",
            "Epoch: 5/20... Step: 380... Loss: 409.4809... Val Loss: 445.9057\n",
            "Epoch: 6/20... Step: 390... Loss: 434.2304... Val Loss: 443.3315\n",
            "Epoch: 6/20... Step: 400... Loss: 363.4586... Val Loss: 395.2843\n",
            "Epoch: 6/20... Step: 410... Loss: 393.7136... Val Loss: 371.9286\n",
            "Epoch: 6/20... Step: 420... Loss: 433.4641... Val Loss: 412.4950\n",
            "Epoch: 6/20... Step: 430... Loss: 315.9606... Val Loss: 324.4937\n",
            "Epoch: 6/20... Step: 440... Loss: 379.3719... Val Loss: 418.9774\n",
            "Epoch: 6/20... Step: 450... Loss: 430.8750... Val Loss: 393.4813\n",
            "Epoch: 6/20... Step: 460... Loss: 399.6815... Val Loss: 407.1012\n",
            "Epoch: 7/20... Step: 470... Loss: 380.6533... Val Loss: 400.7154\n",
            "Epoch: 7/20... Step: 480... Loss: 421.6993... Val Loss: 460.5504\n",
            "Epoch: 7/20... Step: 490... Loss: 467.5556... Val Loss: 423.6263\n",
            "Epoch: 7/20... Step: 500... Loss: 455.9715... Val Loss: 485.2264\n",
            "Epoch: 7/20... Step: 510... Loss: 490.5566... Val Loss: 480.7267\n",
            "Epoch: 7/20... Step: 520... Loss: 446.6222... Val Loss: 415.7021\n",
            "Epoch: 7/20... Step: 530... Loss: 477.5890... Val Loss: 443.3526\n",
            "Epoch: 8/20... Step: 540... Loss: 409.4615... Val Loss: 372.4838\n",
            "Epoch: 8/20... Step: 550... Loss: 426.3672... Val Loss: 436.8040\n",
            "Epoch: 8/20... Step: 560... Loss: 355.1801... Val Loss: 384.5330\n",
            "Epoch: 8/20... Step: 570... Loss: 442.4439... Val Loss: 468.4609\n",
            "Epoch: 8/20... Step: 580... Loss: 348.6879... Val Loss: 353.2399\n",
            "Epoch: 8/20... Step: 590... Loss: 408.4795... Val Loss: 434.0694\n",
            "Epoch: 8/20... Step: 600... Loss: 358.8598... Val Loss: 380.1802\n",
            "Epoch: 8/20... Step: 610... Loss: 380.4530... Val Loss: 393.5678\n",
            "Epoch: 9/20... Step: 620... Loss: 328.9859... Val Loss: 317.2152\n",
            "Epoch: 9/20... Step: 630... Loss: 355.4240... Val Loss: 328.4830\n",
            "Epoch: 9/20... Step: 640... Loss: 411.6687... Val Loss: 410.0963\n",
            "Epoch: 9/20... Step: 650... Loss: 398.8399... Val Loss: 416.1831\n",
            "Epoch: 9/20... Step: 660... Loss: 443.5128... Val Loss: 463.4883\n",
            "Epoch: 9/20... Step: 670... Loss: 376.0843... Val Loss: 371.0286\n",
            "Epoch: 9/20... Step: 680... Loss: 361.6809... Val Loss: 374.5048\n",
            "Epoch: 9/20... Step: 690... Loss: 360.3612... Val Loss: 377.4504\n",
            "Epoch: 10/20... Step: 700... Loss: 406.8748... Val Loss: 450.4024\n",
            "Epoch: 10/20... Step: 710... Loss: 442.6448... Val Loss: 431.1504\n",
            "Epoch: 10/20... Step: 720... Loss: 453.9198... Val Loss: 480.3719\n",
            "Epoch: 10/20... Step: 730... Loss: 472.8008... Val Loss: 468.4774\n",
            "Epoch: 10/20... Step: 740... Loss: 427.3226... Val Loss: 460.7402\n",
            "Epoch: 10/20... Step: 750... Loss: 478.5393... Val Loss: 472.0737\n",
            "Epoch: 10/20... Step: 760... Loss: 410.9518... Val Loss: 406.2780\n",
            "Epoch: 10/20... Step: 770... Loss: 437.8673... Val Loss: 457.6264\n",
            "Epoch: 11/20... Step: 780... Loss: 414.5770... Val Loss: 467.4875\n",
            "Epoch: 11/20... Step: 790... Loss: 391.3752... Val Loss: 402.0461\n",
            "Epoch: 11/20... Step: 800... Loss: 483.7303... Val Loss: 527.5808\n",
            "Epoch: 11/20... Step: 810... Loss: 457.3849... Val Loss: 495.3056\n",
            "Epoch: 11/20... Step: 820... Loss: 406.4930... Val Loss: 438.5827\n",
            "Epoch: 11/20... Step: 830... Loss: 403.9117... Val Loss: 422.0731\n",
            "Epoch: 11/20... Step: 840... Loss: 386.7997... Val Loss: 375.2829\n",
            "Epoch: 12/20... Step: 850... Loss: 390.6965... Val Loss: 455.2668\n",
            "Epoch: 12/20... Step: 860... Loss: 487.3700... Val Loss: 409.8001\n",
            "Epoch: 12/20... Step: 870... Loss: 414.0348... Val Loss: 441.7193\n",
            "Epoch: 12/20... Step: 880... Loss: 448.9607... Val Loss: 439.3173\n",
            "Epoch: 12/20... Step: 890... Loss: 437.2316... Val Loss: 489.0422\n",
            "Epoch: 12/20... Step: 900... Loss: 351.0556... Val Loss: 395.1321\n",
            "Epoch: 12/20... Step: 910... Loss: 343.3003... Val Loss: 395.2249\n",
            "Epoch: 12/20... Step: 920... Loss: 345.8137... Val Loss: 373.0608\n",
            "Epoch: 13/20... Step: 930... Loss: 421.8922... Val Loss: 449.7215\n",
            "Epoch: 13/20... Step: 940... Loss: 415.3613... Val Loss: 426.7073\n",
            "Epoch: 13/20... Step: 950... Loss: 423.1056... Val Loss: 453.1154\n",
            "Epoch: 13/20... Step: 960... Loss: 463.6123... Val Loss: 454.5553\n",
            "Epoch: 13/20... Step: 970... Loss: 453.5663... Val Loss: 408.9943\n",
            "Epoch: 13/20... Step: 980... Loss: 441.7401... Val Loss: 434.2253\n",
            "Epoch: 13/20... Step: 990... Loss: 476.3726... Val Loss: 480.7946\n",
            "Epoch: 13/20... Step: 1000... Loss: 385.6250... Val Loss: 360.9819\n",
            "Epoch: 14/20... Step: 1010... Loss: 516.6551... Val Loss: 526.6469\n",
            "Epoch: 14/20... Step: 1020... Loss: 419.7801... Val Loss: 443.4098\n",
            "Epoch: 14/20... Step: 1030... Loss: 433.4232... Val Loss: 454.9706\n",
            "Epoch: 14/20... Step: 1040... Loss: 428.4947... Val Loss: 434.7289\n",
            "Epoch: 14/20... Step: 1050... Loss: 450.0199... Val Loss: 462.5803\n",
            "Epoch: 14/20... Step: 1060... Loss: 376.8602... Val Loss: 395.3419\n",
            "Epoch: 14/20... Step: 1070... Loss: 451.7799... Val Loss: 450.4274\n",
            "Epoch: 15/20... Step: 1080... Loss: 446.9797... Val Loss: 458.9033\n",
            "Epoch: 15/20... Step: 1090... Loss: 454.2423... Val Loss: 506.7361\n",
            "Epoch: 15/20... Step: 1100... Loss: 430.3019... Val Loss: 438.8625\n",
            "Epoch: 15/20... Step: 1110... Loss: 393.7119... Val Loss: 446.5263\n",
            "Epoch: 15/20... Step: 1120... Loss: 378.2857... Val Loss: 429.7986\n",
            "Epoch: 15/20... Step: 1130... Loss: 345.3156... Val Loss: 404.8706\n",
            "Epoch: 15/20... Step: 1140... Loss: 468.5269... Val Loss: 465.6297\n",
            "Epoch: 15/20... Step: 1150... Loss: 404.3954... Val Loss: 401.4369\n",
            "Epoch: 16/20... Step: 1160... Loss: 384.1893... Val Loss: 403.6341\n",
            "Epoch: 16/20... Step: 1170... Loss: 397.5110... Val Loss: 365.6438\n",
            "Epoch: 16/20... Step: 1180... Loss: 457.7404... Val Loss: 474.3742\n",
            "Epoch: 16/20... Step: 1190... Loss: 488.5504... Val Loss: 406.0759\n",
            "Epoch: 16/20... Step: 1200... Loss: 432.7355... Val Loss: 443.9200\n",
            "Epoch: 16/20... Step: 1210... Loss: 421.0545... Val Loss: 404.4427\n",
            "Epoch: 16/20... Step: 1220... Loss: 364.7469... Val Loss: 379.0340\n",
            "Epoch: 16/20... Step: 1230... Loss: 400.0805... Val Loss: 460.3668\n",
            "Epoch: 17/20... Step: 1240... Loss: 445.0389... Val Loss: 487.7504\n",
            "Epoch: 17/20... Step: 1250... Loss: 455.1783... Val Loss: 473.6134\n",
            "Epoch: 17/20... Step: 1260... Loss: 436.5137... Val Loss: 451.9962\n",
            "Epoch: 17/20... Step: 1270... Loss: 448.0207... Val Loss: 474.8293\n",
            "Epoch: 17/20... Step: 1280... Loss: 428.4012... Val Loss: 446.4166\n",
            "Epoch: 17/20... Step: 1290... Loss: 355.0109... Val Loss: 364.0242\n",
            "Epoch: 17/20... Step: 1300... Loss: 338.8061... Val Loss: 308.2931\n",
            "Epoch: 18/20... Step: 1310... Loss: 371.9633... Val Loss: 370.9875\n",
            "Epoch: 18/20... Step: 1320... Loss: 353.7121... Val Loss: 359.6719\n",
            "Epoch: 18/20... Step: 1330... Loss: 366.8385... Val Loss: 377.1076\n",
            "Epoch: 18/20... Step: 1340... Loss: 399.5105... Val Loss: 363.3179\n",
            "Epoch: 18/20... Step: 1350... Loss: 404.5367... Val Loss: 420.8720\n",
            "Epoch: 18/20... Step: 1360... Loss: 479.4166... Val Loss: 453.7872\n",
            "Epoch: 18/20... Step: 1370... Loss: 394.8822... Val Loss: 405.8788\n",
            "Epoch: 18/20... Step: 1380... Loss: 379.6675... Val Loss: 340.6829\n",
            "Epoch: 19/20... Step: 1390... Loss: 366.7345... Val Loss: 380.4965\n",
            "Epoch: 19/20... Step: 1400... Loss: 388.9304... Val Loss: 418.2063\n",
            "Epoch: 19/20... Step: 1410... Loss: 343.0365... Val Loss: 343.8753\n",
            "Epoch: 19/20... Step: 1420... Loss: 365.5514... Val Loss: 384.1619\n",
            "Epoch: 19/20... Step: 1430... Loss: 399.6455... Val Loss: 409.4842\n",
            "Epoch: 19/20... Step: 1440... Loss: 410.1734... Val Loss: 447.4126\n",
            "Epoch: 19/20... Step: 1450... Loss: 470.0799... Val Loss: 420.3276\n",
            "Epoch: 19/20... Step: 1460... Loss: 426.5648... Val Loss: 473.7412\n",
            "Epoch: 20/20... Step: 1470... Loss: 360.7842... Val Loss: 388.7483\n",
            "Epoch: 20/20... Step: 1480... Loss: 426.3014... Val Loss: 452.9230\n",
            "Epoch: 20/20... Step: 1490... Loss: 428.9677... Val Loss: 453.8846\n",
            "Epoch: 20/20... Step: 1500... Loss: 501.9164... Val Loss: 515.0102\n",
            "Epoch: 20/20... Step: 1510... Loss: 490.1671... Val Loss: 509.2431\n",
            "Epoch: 20/20... Step: 1520... Loss: 481.4494... Val Loss: 487.8938\n",
            "Epoch: 20/20... Step: 1530... Loss: 463.8079... Val Loss: 481.7635\n",
            "Epoch: 20/20... Step: 1540... Loss: 428.8759... Val Loss: 455.4770\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 3\n",
        "\n",
        "В комментарии напишите кусочек текста в стиле Шекспира, сгененированного вашей моделью. Выберите кусочек, который вам больше всего понравился!"
      ],
      "metadata": {
        "id": "98KdJ-fIT_Wv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Делаем предсказания"
      ],
      "metadata": {
        "id": "KtkvljT0UNeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "    \"\"\"Given a character, predict the next character.\n",
        "    Returns the predicted character and the hidden state.\n",
        "    \"\"\"\n",
        "\n",
        "    # tensor inputs\n",
        "    x = np.array([[net.char2int[char]]])\n",
        "    x = one_hot_encode(x, len(net.chars))\n",
        "    inputs = torch.from_numpy(x)\n",
        "\n",
        "    if train_on_gpu:\n",
        "        inputs = inputs.cuda()\n",
        "\n",
        "    # detach hidden state from history\n",
        "    h = tuple([each.data for each in h])\n",
        "    # get the output of the model\n",
        "    out, h = net(inputs, h)\n",
        "\n",
        "    # get the character probabilities\n",
        "    p = F.softmax(out, dim=1).data\n",
        "    if train_on_gpu:\n",
        "        p = p.cpu()  # move to cpu\n",
        "\n",
        "    # get top characters\n",
        "    if top_k is None:\n",
        "        top_ch = np.arange(len(net.chars))\n",
        "    else:\n",
        "        p, top_ch = p.topk(top_k)\n",
        "        top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "    # select the likely next character with some element of randomness\n",
        "    p = p.numpy().squeeze()\n",
        "    char = np.random.choice(top_ch, p=p / p.sum())\n",
        "\n",
        "    # return the encoded value of the predicted char and the hidden state\n",
        "    return net.int2char[char], h"
      ],
      "metadata": {
        "id": "W2Vw_BiPUHZo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Priming"
      ],
      "metadata": {
        "id": "pAx-qhzxUTNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(net, size, prime=\"The\", top_k=None):\n",
        "\n",
        "    if train_on_gpu:\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "\n",
        "    net.eval()  # eval mode\n",
        "\n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "\n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return \"\".join(chars)"
      ],
      "metadata": {
        "id": "Sxr6xGDPUQMu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample(net, 500, prime=\"The King Lear\", top_k=5))"
      ],
      "metadata": {
        "id": "czaqKp57VQX5",
        "outputId": "ad07b2cf-f7a8-4948-9ae1-96a526ef6b63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The King Learin him; but well\n",
            "To be the hard war that the will or see,\n",
            "And have her she there in your gone, the corts\n",
            "And bosts, that with his should we have my sendel.\n",
            "\n",
            "KING RICHARD III:\n",
            "What, to her son, and shill's this than\n",
            "they, a many were art and the son?\n",
            "\n",
            "BUCKINGHAM:\n",
            "What! why had, then, which thy ship of honour stards\n",
            "The childis of the servisity of his life.\n",
            "\n",
            "CAPULET:\n",
            "And as the prince, when I shall shall his hand.\n",
            "Therefore, with me, the world with me, with his\n",
            "sear that thou art of thou sport. Tho\n"
          ]
        }
      ]
    }
  ]
}