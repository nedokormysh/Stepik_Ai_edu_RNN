{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nedokormysh/Stepik_Ai_edu_RNN/blob/week_6_Word_RNN_Classification/AiEdu_WordRNN_Hometask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scyE4k8gpwl0"
      },
      "source": [
        "# Домашнее задание: BiLSTM для задачи PoS Tagging\n",
        "\n",
        "В этом ноутбуке мы будем создавать модель машинного обучения, которая генерирует результат для каждого элемента входной последовательности с использованием PyTorch и TorchText. Конкретно, мы будем подавать текст на вход, а модель будет выводить метку - часть речи (PoS) для каждого токена во входном тексте. Этот подход также может применяться для распознавания именованных сущностей (NER), где результатом для каждого токена будет указание на тип сущности, если таковая имеется.\n",
        "\n",
        "В этом блокноте мы реализуем многослойную двунаправленную LSTM (BiLSTM) для предсказания меток частей речи с использованием набора данных Universal Dependencies English Web Treebank (UDPOS)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.6.0 -q"
      ],
      "metadata": {
        "id": "5Xf-0XQQW9G8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de6e3ec8-a7d9-47ed-f569-d8bf59c9996a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m543.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9osUMmompwl2"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import time\n",
        "import random"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5OZxOjBpwl3"
      },
      "source": [
        "Зафиксируем случайности для воспроизводимости результатов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egSkC0DDpwl4"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTuoKelVpwl4"
      },
      "source": [
        "В этом наборе данных есть два разных набора меток: метки универсальных зависимостей (UD) и метки Penn Treebank (PTB). Мы будем обучать модель только на метках UD, но загрузим метки PTB, чтобы показать, как их можно использовать вместо них.\n",
        "\n",
        "* UD_TAGS определяет, как следует обрабатывать метки UD. В нашем словаре TEXT, который мы создадим позже, будут неизвестные токены, то есть токены, которых нет в нашем словаре. Однако у нас не будет неизвестных меток, поскольку мы имеем дело с конечным набором возможных меток. Мы будем обозначать неизвестные токены как <unk>, и затем будем их убирать, установив unk_token = None.\n",
        "\n",
        "* PTB_TAGS выполняет то же самое, что и UD_TAGS, но обрабатывает метки PTB."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fidoBBJfpwl5"
      },
      "source": [
        "from torchtext.data import Field\n",
        "\n",
        "TEXT = Field(lower = True)\n",
        "UD_TAGS = Field(unk_token = None)\n",
        "PTB_TAGS = Field(unk_token = None)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iqya4Ki-pwl6"
      },
      "source": [
        "fields = ((\"text\", TEXT), (\"udtags\", UD_TAGS), (\"ptbtags\", PTB_TAGS))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AHzDh46pwl7"
      },
      "source": [
        "Загрузим датасет UDPOS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI_RLyzzpwl7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9363f3b7-0cda-4be8-a431-01b521532186"
      },
      "source": [
        "train_data, valid_data, test_data = datasets.UDPOS.splits(fields)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading en-ud-v2.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "en-ud-v2.zip: 100%|██████████| 688k/688k [00:00<00:00, 4.93MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "extracting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cHlSTlEpwl7"
      },
      "source": [
        "## Задание\n",
        "\n",
        "Посмотрите на количество объектов в датасетах `train_data, valid_data и test_data`. В ответ запишите число объектов в самом маленьком датасете."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of training examples: {len(train_data)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data)}\")\n",
        "print(f\"Number of testing examples: {len(test_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQZ9oDPYIPp_",
        "outputId": "62708162-ecfe-4f28-eba8-3a72eff2bacb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 12543\n",
            "Number of validation examples: 2002\n",
            "Number of testing examples: 2077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCznT-c3pwl8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc077029-7b04-4c07-aac4-0f117bd5d0a4"
      },
      "source": [
        "sum((1 for x in train_data))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12543"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum((1 for x in valid_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsFeJ5f6A60z",
        "outputId": "6ed3a2b8-abd6-4d39-cae4-84dabe14e16f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2002"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum((1 for x in test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiz_WO9GA_Gi",
        "outputId": "755575ea-1439-4078-b230-d00a76eaccaa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2077"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIY61Z5Cpwl9"
      },
      "source": [
        "Напечатаем пример из датасета"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpadMDOCpwl9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e6343e1-e50c-4a69-f420-1094a9a88c23"
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': ['al', '-', 'zaman', ':', 'american', 'forces', 'killed', 'shaikh', 'abdullah', 'al', '-', 'ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'qaim', ',', 'near', 'the', 'syrian', 'border', '.'], 'udtags': ['PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT'], 'ptbtags': ['NNP', 'HYPH', 'NNP', ':', 'JJ', 'NNS', 'VBD', 'NNP', 'NNP', 'NNP', 'HYPH', 'NNP', ',', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', ',', 'IN', 'DT', 'JJ', 'NN', '.']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S39xsVtxpwl9"
      },
      "source": [
        "Можем отдельно посмотреть на текст и на теги"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UULauMEBpwl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46719daf-a9fe-4c6f-f624-110bead19175"
      },
      "source": [
        "print(vars(train_data.examples[0])['text'])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['al', '-', 'zaman', ':', 'american', 'forces', 'killed', 'shaikh', 'abdullah', 'al', '-', 'ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'qaim', ',', 'near', 'the', 'syrian', 'border', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACWgCsPqpwl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11e9ab42-a059-4c80-8b2e-49f9d9556538"
      },
      "source": [
        "print(vars(train_data.examples[0])['udtags'])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko_hI-Fgpwl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee7e1051-1d9a-4279-e947-05da43108312"
      },
      "source": [
        "print(vars(train_data.examples[0])['ptbtags'])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NNP', 'HYPH', 'NNP', ':', 'JJ', 'NNS', 'VBD', 'NNP', 'NNP', 'NNP', 'HYPH', 'NNP', ',', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', ',', 'IN', 'DT', 'JJ', 'NN', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TYUko-Bpwl-"
      },
      "source": [
        "Что мы сделаем дальше:\n",
        "\n",
        "* Мы создадим словарь - отображение токенов в целые числа.\n",
        "\n",
        "* Мы хотим, чтобы в нашем наборе данных были некоторые неизвестные токены, чтобы воссоздать, как эта модель будет использоваться в реальной жизни, поэтому мы устанавливаем `min_freq = 2`, что означает, что в словарь будут добавлены только токены, появляющиеся хотя бы дважды в обучающем наборе, и остальные будут заменены токенами `<unk>`.\n",
        "\n",
        "* Мы также загружаем предобученные векторы GloVe длины 100 для инициализации эмбеддингов.\n",
        "\n",
        "* `unk_init` используется для инициализации эмбеддингов токенов, которых нет в словаре предварительно обученных вложений. По умолчанию эта инициализация устанавливает эти эмбеддинги в нули, однако лучше избежать их инициализации одним и тем же значением, поэтому мы инициализируем их из нормального распределения.\n",
        "\n",
        "* Предобученные векторы загружаем в наш словарь и будем инициализировать нашу модель этими значениями позже."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание\n",
        "\n",
        "По тренировочным данным постройте три словаря, используя `build_vocab`:\n",
        "\n",
        "* Cловарь по текстам `TEXT` с гиперпараметрами:\n",
        "  * min_freq = MIN_FREQ\n",
        "  * vectors = \"glove.6B.100d\"\n",
        "  * unk_init = torch.Tensor.normal_\n",
        "\n",
        "* Словарь по `UD_TAGS`\n",
        "\n",
        "* Словарь по `PTB_TAGS`\n",
        "\n",
        "Сколько уникальных токенов в словаре, построенном по текстам?"
      ],
      "metadata": {
        "id": "R4V40fC1dLEA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGKCnORWpwl_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92a5e34b-ce35-4456-93bf-f78c6f555c28"
      },
      "source": [
        "MIN_FREQ = 2\n",
        "\n",
        "TEXT.build_vocab(\n",
        "    train_data,\n",
        "    # max_size=vocab_size,\n",
        "    min_freq = MIN_FREQ,\n",
        "    vectors=\"glove.6B.100d\",\n",
        "    unk_init = torch.Tensor.normal_\n",
        ")\n",
        "\n",
        "UD_TAGS.build_vocab(train_data)\n",
        "PTB_TAGS.build_vocab(train_data)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:39, 5.40MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:22<00:00, 17394.68it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UD_TAGS.build_vocab(\n",
        "#     train_data,\n",
        "#     # max_size=vocab_size,\n",
        "#     min_freq = MIN_FREQ,\n",
        "#     vectors=\"glove.6B.100d\",\n",
        "#     unk_init = torch.Tensor.normal_\n",
        "# )"
      ],
      "metadata": {
        "id": "4jkyhxF1BUg9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PTB_TAGS.build_vocab(\n",
        "#     train_data,\n",
        "#     # max_size=vocab_size,\n",
        "#     min_freq = MIN_FREQ,\n",
        "#     vectors=\"glove.6B.100d\",\n",
        "#     unk_init = torch.Tensor.normal_\n",
        "# )"
      ],
      "metadata": {
        "id": "MS9TTteSBY7f"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT.vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QOIj_lsUsEx",
        "outputId": "0a01ee7d-a47a-4781-8997-9b462222e3e0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torchtext.vocab.Vocab at 0x7f8aa168d060>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(TEXT.vocab.itos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpMfsp6PE1ER",
        "outputId": "f8b0f2ba-0686-4d87-df13-0e4cf3e41b06"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8866"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
        "print(f\"Unique tokens in UD_TAG vocabulary: {len(UD_TAGS.vocab)}\")\n",
        "print(f\"Unique tokens in PTB_TAG vocabulary: {len(PTB_TAGS.vocab)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGsGjg-8IemM",
        "outputId": "41fbfe34-169c-4558-d813-34de6420dbf2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique tokens in TEXT vocabulary: 8866\n",
            "Unique tokens in UD_TAG vocabulary: 18\n",
            "Unique tokens in PTB_TAG vocabulary: 51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-jeBfsSpwmG"
      },
      "source": [
        "## Задание\n",
        "\n",
        "Какой самый популярный (часто встречающийся) токен в словаре, построенном по текстам?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFHRIvpMpwmH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8948eb4-81b9-49b3-d8ac-e0011d7c5593"
      },
      "source": [
        "TEXT.vocab.freqs.most_common(10)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 9076),\n",
              " ('.', 8640),\n",
              " (',', 7021),\n",
              " ('to', 5137),\n",
              " ('and', 5002),\n",
              " ('a', 3782),\n",
              " ('of', 3622),\n",
              " ('i', 3379),\n",
              " ('in', 3112),\n",
              " ('is', 2239)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pj_pOsqpwmJ"
      },
      "source": [
        "Посмотрим на функцию, вычисляющую процентное соотношение тегов в текстах."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ee-Hm4E-pwmJ"
      },
      "source": [
        "def tag_percentage(tag_counts):\n",
        "\n",
        "    total_count = sum([count for tag, count in tag_counts])\n",
        "\n",
        "    tag_counts_percentages = [(tag, count, count/total_count) for tag, count in tag_counts]\n",
        "\n",
        "    return tag_counts_percentages"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание\n",
        "\n",
        "Пользуясь функцией `tag_percentage`, выведите на экран процентное соотношение каждого UD-тэга.\n",
        "\n",
        "Какой тег встречается в текстах чаще всего (в процентах)?"
      ],
      "metadata": {
        "id": "bWF7fqNEd69_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2sFxAAHpwmJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "034f81c0-8deb-4313-ec5d-e10b507859a6"
      },
      "source": [
        "print(\"Tag\\t\\tCount\\t\\tPercentage\\n\")\n",
        "\n",
        "for tag, count, percent in tag_percentage(UD_TAGS.vocab.freqs.most_common()):\n",
        "    print(f\"{tag}\\t\\t{count}\\t\\t{percent*100:4.1f}%\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag\t\tCount\t\tPercentage\n",
            "\n",
            "NOUN\t\t34781\t\t17.0%\n",
            "PUNCT\t\t23679\t\t11.6%\n",
            "VERB\t\t23081\t\t11.3%\n",
            "PRON\t\t18577\t\t 9.1%\n",
            "ADP\t\t17638\t\t 8.6%\n",
            "DET\t\t16285\t\t 8.0%\n",
            "PROPN\t\t12946\t\t 6.3%\n",
            "ADJ\t\t12477\t\t 6.1%\n",
            "AUX\t\t12343\t\t 6.0%\n",
            "ADV\t\t10548\t\t 5.2%\n",
            "CCONJ\t\t6707\t\t 3.3%\n",
            "PART\t\t5567\t\t 2.7%\n",
            "NUM\t\t3999\t\t 2.0%\n",
            "SCONJ\t\t3843\t\t 1.9%\n",
            "X\t\t847\t\t 0.4%\n",
            "INTJ\t\t688\t\t 0.3%\n",
            "SYM\t\t599\t\t 0.3%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOu9Uv6ZpwmL"
      },
      "source": [
        "## Задание\n",
        "\n",
        "Используя `BucketIterator.split`, создайте объекты `train_iterator, valid_iterator, test_iterator` для итерирования по батчам."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78DjTNiJpwmL"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device\n",
        ")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/bentrevett/pytorch-pos-tagging/blob/master/1_bilstm.ipynb"
      ],
      "metadata": {
        "id": "628Y3U2rJwPf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-KCF_thpwmO"
      },
      "source": [
        "## Создаем архитектуру нейронной сети\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-pos-tagging/blob/master/assets/pos-bidirectional-lstm.png?raw=1)\n",
        "\n",
        "Задайте нейронную сеть по аналогии с сетью из вебинара:\n",
        "\n",
        "* Слой Embedding:\n",
        "  * помимо прочего задайте `padding_idx = pad_idx`\n",
        "\n",
        "* Затем слой LSTM с гиперпараметрами:\n",
        "  * `n_layers = 1`\n",
        "  * `bidirectional = True`\n",
        "  * задайте `dropout`\n",
        "\n",
        "* Затем DropOut слой\n",
        "\n",
        "* Линейный слой, принимающий на вход `hidden_dim * 2` нейронов (так как двунаправленная сеть) и на выходе `output_dim` нейронов\n",
        "\n",
        "В ответ на задание выберите, как выглядит первая строчка в архитектуре сети."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-Wa3Tp5pwmO"
      },
      "source": [
        "class BiLSTMPOSTagger(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 embedding_dim,\n",
        "                 hidden_dim,\n",
        "                 output_dim,\n",
        "                 n_layers,\n",
        "                 bidirectional,\n",
        "                 dropout,\n",
        "                 pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx = pad_idx)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim,\n",
        "                            hidden_dim,\n",
        "                            num_layers = n_layers,\n",
        "                            bidirectional = bidirectional,\n",
        "                            dropout = dropout if n_layers > 1 else 0)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        #pass text through embedding layer and then through dropout layer\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "\n",
        "        #pass embeddings into LSTM\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "\n",
        "        #apply dropout and then linear layer\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "\n",
        "        return predictions"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTjQq-Eqpwmi"
      },
      "source": [
        "## Обучение модели\n",
        "\n",
        "## Задание\n",
        "\n",
        "Запустите ячейку ниже. Если класс `BiLSTMPOSTagger` реализован корректно, ячейка отработает без ошибок. Получилось?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crDf8hTHpwmi"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 128\n",
        "OUTPUT_DIM = len(UD_TAGS.vocab)\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.25\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model = BiLSTMPOSTagger(INPUT_DIM,\n",
        "                        EMBEDDING_DIM,\n",
        "                        HIDDEN_DIM,\n",
        "                        OUTPUT_DIM,\n",
        "                        N_LAYERS,\n",
        "                        BIDIRECTIONAL,\n",
        "                        DROPOUT,\n",
        "                        PAD_IDX)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc0xJFAApwmi"
      },
      "source": [
        "Инициализируем веса сети числами из стандартного нормального распределения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEq91x0Epwmj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75e5edda-3f30-4c29-9bbb-bb4f2b15f76a"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean = 0, std = 0.1)\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiLSTMPOSTagger(\n",
              "  (embedding): Embedding(8866, 100, padding_idx=1)\n",
              "  (lstm): LSTM(100, 128, num_layers=2, dropout=0.25, bidirectional=True)\n",
              "  (fc): Linear(in_features=256, out_features=18, bias=True)\n",
              "  (dropout): Dropout(p=0.25, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQmNFG_wpwmj"
      },
      "source": [
        "## Задание\n",
        "\n",
        "Напишите функцию для вычисления количества весов сети.\n",
        "\n",
        "С помощью этой функции выведите на экран число весов нашей сети."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfPyt7TTpwmk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "447fcf2d-a3e8-44b3-aa87-afa65fa97f04"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 1,522,010 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTVuLKdFpwmk"
      },
      "source": [
        "## Задание\n",
        "\n",
        "Инициализируйте embedding-слой сети предобученными GloVe-векторами.\n",
        "\n",
        "В ответ напишите число координат в предобученных эмбеддингах."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-H-ucDCpwml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fe9553d-8c0e-49cd-95c9-ef674e59a9e1"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8866, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Инициализируем нулями pad-токены"
      ],
      "metadata": {
        "id": "vDD0mql7rR7W"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pbjDSgNpwmm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3d709fb-d79d-402e-fdb1-77c5169367f6"
      },
      "source": [
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "print(model.embedding.weight.data)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0081, -0.1063, -0.0783,  ...,  0.0307, -0.1222,  0.0106],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0287,  0.1974, -0.2067,  ...,  0.0309,  0.0399, -0.0211],\n",
            "        ...,\n",
            "        [ 0.0215, -0.0280,  0.0792,  ...,  0.0782,  0.0991, -0.0131],\n",
            "        [ 0.1314, -0.1191, -0.0668,  ...,  0.0351, -0.1579, -0.1078],\n",
            "        [ 0.0252,  0.0359, -0.1001,  ..., -0.1587,  0.0723, -0.0598]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO51ESn5pwmn"
      },
      "source": [
        "Зададим оптимизатор"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpCGPGTkpwmn"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8EQh0BGpwmn"
      },
      "source": [
        "Зададим loss.\n",
        "\n",
        "В случае токена `<pad>` (пустота) лосс мы не считаем, поэтому индексы таких токенов мы пропускаем (игнорируем)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOhdfyscpwmo"
      },
      "source": [
        "TAG_PAD_IDX = UD_TAGS.vocab.stoi[UD_TAGS.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmiF0b31pwmo"
      },
      "source": [
        "Переносим модель на GPU по возможности"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHLdYmqhpwmp"
      },
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIgmzjvEpwmq"
      },
      "source": [
        "Функция ниже вычисляет `accuracy` для каждого батча"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6FMQ9Pppwmq"
      },
      "source": [
        "def categorical_accuracy(preds, y, tag_pad_idx):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
        "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
        "    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
        "    return correct.sum() / y[non_pad_elements].shape[0]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzbRWYkIpwmq"
      },
      "source": [
        "## Задание\n",
        "\n",
        "Допишите цикл обучения модели.\n",
        "\n",
        "Для каждого батча на каждой итерации:\n",
        "- зануляем градиенты\n",
        "- применяем модель к батчу\n",
        "- делаем reshape прогнозов, так как loss нельзя вычислить для тензора размерности 3 (это уже написано)\n",
        "- вычисляем loss и accuracy\n",
        "- вычисляем градиенты и делаем шаг градиентного спуска"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QGjrEYppwmr"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, tag_pad_idx):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch in iterator:\n",
        "\n",
        "        text = batch.text\n",
        "        tags = batch.udtags\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = model(text)\n",
        "\n",
        "        predictions = predictions.view(-1, predictions.shape[-1]) # predictions - прогнозы модели\n",
        "        tags = tags.view(-1) # tags - правильные ответы (метки)\n",
        "\n",
        "        loss = criterion(predictions, tags)\n",
        "\n",
        "        acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkdJ4J-Gpwms"
      },
      "source": [
        "Функцию `evaluate` для простоты мы написали."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoxU16Sbpwms"
      },
      "source": [
        "def evaluate(model, iterator, criterion, tag_pad_idx):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch in iterator:\n",
        "\n",
        "            text = batch.text\n",
        "            tags = batch.udtags\n",
        "\n",
        "            predictions = model(text)\n",
        "\n",
        "            predictions = predictions.view(-1, predictions.shape[-1])\n",
        "            tags = tags.view(-1)\n",
        "\n",
        "            loss = criterion(predictions, tags)\n",
        "\n",
        "            acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUnSxXfmpwmt"
      },
      "source": [
        "Ниже функция, которая замеряет время обучения на каждой эпохе"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM2Mfyohpwmt"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlWPogO8pwmv"
      },
      "source": [
        "## Задание\n",
        "\n",
        "Обучим нашу модель. Допишите цикл по подсказкам в коде.\n",
        "\n",
        "Какая accuracy (в процентах) получается на валидации на последней эпохе? Ответ округлите до целого числа."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W08Crfvzpwmw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24fcf8ac-ff99-48da-a5d9-97770bddad12"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, TAG_PAD_IDX) # - примените функцию для обучения модели\n",
        "    valid_loss, valid_acc = evaluate(model, test_iterator, criterion, TAG_PAD_IDX)# примените функцию для применения и оценки качества модели\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)# замерьте время выполнения эпохи, используя написанную для этого функцию\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "\n",
        "\n",
        "    # для каждой эпохи выведите train loss, train accuracy, val loss, val accuracy, epoch time\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'train loss = {train_loss}, train accuracy = {train_acc*100}%,| val loss = {valid_loss}, val accuracy = {valid_acc*100}%')\n",
        "\n",
        "    # print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    # print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    # print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 4s\n",
            "train loss = 1.708321938709337, train accuracy = 46.18485237718845%,| val loss = 0.9638762999983395, val accuracy = 71.10648190273959%\n",
            "Epoch: 02 | Epoch Time: 0m 2s\n",
            "train loss = 0.5194270370566115, train accuracy = 84.02690060284674%,| val loss = 0.5766892433166504, val accuracy = 83.79984743454877%\n",
            "Epoch: 03 | Epoch Time: 0m 2s\n",
            "train loss = 0.30512974791380826, train accuracy = 90.70384508492995%,| val loss = 0.4826907340218039, val accuracy = 86.204835246591%\n",
            "Epoch: 04 | Epoch Time: 0m 2s\n",
            "train loss = 0.23240846562750486, train accuracy = 92.79214338380464%,| val loss = 0.4386892879710478, val accuracy = 86.91256607280057%\n",
            "Epoch: 05 | Epoch Time: 0m 2s\n",
            "train loss = 0.19992551390005617, train accuracy = 93.76049005255408%,| val loss = 0.41594840147916007, val accuracy = 88.28883381450878%\n",
            "Epoch: 06 | Epoch Time: 0m 2s\n",
            "train loss = 0.17648501891870888, train accuracy = 94.4484501468892%,| val loss = 0.40380264731014476, val accuracy = 88.69704849579755%\n",
            "Epoch: 07 | Epoch Time: 0m 2s\n",
            "train loss = 0.15747011407297484, train accuracy = 94.9655077287129%,| val loss = 0.39659711367943706, val accuracy = 88.94988052985248%\n",
            "Epoch: 08 | Epoch Time: 0m 2s\n",
            "train loss = 0.14452981735978807, train accuracy = 95.36503656786314%,| val loss = 0.3907155841588974, val accuracy = 89.1486918225008%\n",
            "Epoch: 09 | Epoch Time: 0m 2s\n",
            "train loss = 0.13224378456266558, train accuracy = 95.76962292194366%,| val loss = 0.3892162959365284, val accuracy = 89.03537427677828%\n",
            "Epoch: 10 | Epoch Time: 0m 2s\n",
            "train loss = 0.12214660157962721, train accuracy = 96.08769897295504%,| val loss = 0.38361212698852315, val accuracy = 89.28264554809121%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMKuvg9apwmw"
      },
      "source": [
        "Посмотрим на качество обученной модели на тесте"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWVYuxHqpwmx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bead9ba4-5d1e-48ed-ec94-8096997e8316"
      },
      "source": [
        "model.load_state_dict(torch.load('tut1-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion, TAG_PAD_IDX)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.384 |  Test Acc: 89.28%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoRcN30cpwmx"
      },
      "source": [
        "## Инференс\n",
        "\n",
        "Посмотрим, как модель работает на новых данных. Допишите функцию `tag_sentence` для применения обученной модели, по подсказкам ниже."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT.vocab.stoi[]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOjOpO8EA1t6",
        "outputId": "fb4773e8-143d-4bc6-e283-c868f1fca097"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7f8aa168d060>>,\n",
              "            {'<unk>': 0,\n",
              "             '<pad>': 1,\n",
              "             'the': 2,\n",
              "             '.': 3,\n",
              "             ',': 4,\n",
              "             'to': 5,\n",
              "             'and': 6,\n",
              "             'a': 7,\n",
              "             'of': 8,\n",
              "             'i': 9,\n",
              "             'in': 10,\n",
              "             'is': 11,\n",
              "             'you': 12,\n",
              "             'that': 13,\n",
              "             'it': 14,\n",
              "             'for': 15,\n",
              "             '-': 16,\n",
              "             'have': 17,\n",
              "             '\"': 18,\n",
              "             'on': 19,\n",
              "             'was': 20,\n",
              "             'with': 21,\n",
              "             'this': 22,\n",
              "             'be': 23,\n",
              "             'are': 24,\n",
              "             'they': 25,\n",
              "             'not': 26,\n",
              "             'as': 27,\n",
              "             'we': 28,\n",
              "             \"'s\": 29,\n",
              "             'my': 30,\n",
              "             ')': 31,\n",
              "             '(': 32,\n",
              "             'do': 33,\n",
              "             'will': 34,\n",
              "             'he': 35,\n",
              "             'at': 36,\n",
              "             '?': 37,\n",
              "             'but': 38,\n",
              "             'if': 39,\n",
              "             'or': 40,\n",
              "             'your': 41,\n",
              "             'from': 42,\n",
              "             \"n't\": 43,\n",
              "             'by': 44,\n",
              "             'can': 45,\n",
              "             'would': 46,\n",
              "             'me': 47,\n",
              "             ':': 48,\n",
              "             'there': 49,\n",
              "             'so': 50,\n",
              "             '!': 51,\n",
              "             'all': 52,\n",
              "             'has': 53,\n",
              "             'an': 54,\n",
              "             'had': 55,\n",
              "             'one': 56,\n",
              "             'out': 57,\n",
              "             'about': 58,\n",
              "             'what': 59,\n",
              "             'their': 60,\n",
              "             'like': 61,\n",
              "             'time': 62,\n",
              "             'were': 63,\n",
              "             'when': 64,\n",
              "             'up': 65,\n",
              "             'get': 66,\n",
              "             'just': 67,\n",
              "             'his': 68,\n",
              "             'our': 69,\n",
              "             'some': 70,\n",
              "             'who': 71,\n",
              "             'them': 72,\n",
              "             'know': 73,\n",
              "             'which': 74,\n",
              "             'been': 75,\n",
              "             'also': 76,\n",
              "             'am': 77,\n",
              "             'us': 78,\n",
              "             'very': 79,\n",
              "             'any': 80,\n",
              "             'more': 81,\n",
              "             'no': 82,\n",
              "             'good': 83,\n",
              "             'did': 84,\n",
              "             'could': 85,\n",
              "             'him': 86,\n",
              "             'go': 87,\n",
              "             'new': 88,\n",
              "             'other': 89,\n",
              "             'she': 90,\n",
              "             'only': 91,\n",
              "             'please': 92,\n",
              "             '$': 93,\n",
              "             'how': 94,\n",
              "             'should': 95,\n",
              "             'people': 96,\n",
              "             'may': 97,\n",
              "             \"'\": 98,\n",
              "             'then': 99,\n",
              "             'back': 100,\n",
              "             'now': 101,\n",
              "             'said': 102,\n",
              "             '...': 103,\n",
              "             'even': 104,\n",
              "             'after': 105,\n",
              "             'work': 106,\n",
              "             'bush': 107,\n",
              "             'want': 108,\n",
              "             'well': 109,\n",
              "             'her': 110,\n",
              "             'great': 111,\n",
              "             '/': 112,\n",
              "             'because': 113,\n",
              "             'way': 114,\n",
              "             'see': 115,\n",
              "             'best': 116,\n",
              "             'than': 117,\n",
              "             'place': 118,\n",
              "             'these': 119,\n",
              "             \"'m\": 120,\n",
              "             'into': 121,\n",
              "             'take': 122,\n",
              "             'going': 123,\n",
              "             'over': 124,\n",
              "             'service': 125,\n",
              "             'need': 126,\n",
              "             'thanks': 127,\n",
              "             'make': 128,\n",
              "             'many': 129,\n",
              "             'before': 130,\n",
              "             'year': 131,\n",
              "             'here': 132,\n",
              "             'number': 133,\n",
              "             'day': 134,\n",
              "             'think': 135,\n",
              "             'two': 136,\n",
              "             'food': 137,\n",
              "             'much': 138,\n",
              "             'let': 139,\n",
              "             'first': 140,\n",
              "             's': 141,\n",
              "             'its': 142,\n",
              "             'call': 143,\n",
              "             'does': 144,\n",
              "             '2': 145,\n",
              "             '--': 146,\n",
              "             'help': 147,\n",
              "             'years': 148,\n",
              "             'being': 149,\n",
              "             'never': 150,\n",
              "             'really': 151,\n",
              "             \"'ll\": 152,\n",
              "             'most': 153,\n",
              "             'pm': 154,\n",
              "             'use': 155,\n",
              "             'made': 156,\n",
              "             'still': 157,\n",
              "             'same': 158,\n",
              "             'world': 159,\n",
              "             'right': 160,\n",
              "             'got': 161,\n",
              "             'say': 162,\n",
              "             'another': 163,\n",
              "             'give': 164,\n",
              "             'last': 165,\n",
              "             'off': 166,\n",
              "             'since': 167,\n",
              "             '3': 168,\n",
              "             \"'ve\": 169,\n",
              "             'those': 170,\n",
              "             'find': 171,\n",
              "             'too': 172,\n",
              "             'where': 173,\n",
              "             'around': 174,\n",
              "             'put': 175,\n",
              "             'again': 176,\n",
              "             'iraq': 177,\n",
              "             'sure': 178,\n",
              "             '1': 179,\n",
              "             'down': 180,\n",
              "             'little': 181,\n",
              "             'long': 182,\n",
              "             'al': 183,\n",
              "             'better': 184,\n",
              "             'price': 185,\n",
              "             '&': 186,\n",
              "             'both': 187,\n",
              "             'told': 188,\n",
              "             'through': 189,\n",
              "             'called': 190,\n",
              "             'next': 191,\n",
              "             'few': 192,\n",
              "             'while': 193,\n",
              "             'days': 194,\n",
              "             'look': 195,\n",
              "             'enron': 196,\n",
              "             '4': 197,\n",
              "             'come': 198,\n",
              "             'home': 199,\n",
              "             'used': 200,\n",
              "             'always': 201,\n",
              "             'feel': 202,\n",
              "             \"'re\": 203,\n",
              "             ';': 204,\n",
              "             'business': 205,\n",
              "             'car': 206,\n",
              "             '5': 207,\n",
              "             'change': 208,\n",
              "             'name': 209,\n",
              "             'such': 210,\n",
              "             'every': 211,\n",
              "             'family': 212,\n",
              "             'own': 213,\n",
              "             'lot': 214,\n",
              "             'money': 215,\n",
              "             'took': 216,\n",
              "             'ca': 217,\n",
              "             'however': 218,\n",
              "             'keep': 219,\n",
              "             'week': 220,\n",
              "             'between': 221,\n",
              "             'ever': 222,\n",
              "             'done': 223,\n",
              "             'experience': 224,\n",
              "             'love': 225,\n",
              "             'something': 226,\n",
              "             'states': 227,\n",
              "             'try': 228,\n",
              "             'already': 229,\n",
              "             'company': 230,\n",
              "             'united': 231,\n",
              "             'anyone': 232,\n",
              "             'office': 233,\n",
              "             '10': 234,\n",
              "             'doing': 235,\n",
              "             'night': 236,\n",
              "             'things': 237,\n",
              "             'job': 238,\n",
              "             'looking': 239,\n",
              "             'end': 240,\n",
              "             'iran': 241,\n",
              "             'recommend': 242,\n",
              "             'staff': 243,\n",
              "             'thing': 244,\n",
              "             'under': 245,\n",
              "             'm': 246,\n",
              "             'went': 247,\n",
              "             'came': 248,\n",
              "             'each': 249,\n",
              "             'attached': 250,\n",
              "             'life': 251,\n",
              "             'might': 252,\n",
              "             'room': 253,\n",
              "             'china': 254,\n",
              "             'group': 255,\n",
              "             'old': 256,\n",
              "             'away': 257,\n",
              "             'anything': 258,\n",
              "             'found': 259,\n",
              "             'nice': 260,\n",
              "             'against': 261,\n",
              "             'getting': 262,\n",
              "             'order': 263,\n",
              "             'nt': 264,\n",
              "             'once': 265,\n",
              "             'power': 266,\n",
              "             'problem': 267,\n",
              "             'questions': 268,\n",
              "             'hope': 269,\n",
              "             'point': 270,\n",
              "             'water': 271,\n",
              "             'american': 272,\n",
              "             'during': 273,\n",
              "             'military': 274,\n",
              "             'thank': 275,\n",
              "             'though': 276,\n",
              "             'able': 277,\n",
              "             'amount': 278,\n",
              "             'bad': 279,\n",
              "             'meeting': 280,\n",
              "             'phone': 281,\n",
              "             'state': 282,\n",
              "             'nothing': 283,\n",
              "             'president': 284,\n",
              "             'times': 285,\n",
              "             '!!': 286,\n",
              "             'care': 287,\n",
              "             'deal': 288,\n",
              "             'email': 289,\n",
              "             'far': 290,\n",
              "             'having': 291,\n",
              "             'today': 292,\n",
              "             '..': 293,\n",
              "             'agreement': 294,\n",
              "             'person': 295,\n",
              "             'actually': 296,\n",
              "             'asked': 297,\n",
              "             'left': 298,\n",
              "             'yes': 299,\n",
              "             'big': 300,\n",
              "             'else': 301,\n",
              "             'excellent': 302,\n",
              "             'high': 303,\n",
              "             'information': 304,\n",
              "             'live': 305,\n",
              "             'without': 306,\n",
              "             'country': 307,\n",
              "             'sent': 308,\n",
              "             'three': 309,\n",
              "             'gas': 310,\n",
              "             'house': 311,\n",
              "             'per': 312,\n",
              "             'small': 313,\n",
              "             'someone': 314,\n",
              "             'trying': 315,\n",
              "             'until': 316,\n",
              "             'war': 317,\n",
              "             'why': 318,\n",
              "             'working': 319,\n",
              "             'date': 320,\n",
              "             'india': 321,\n",
              "             'john': 322,\n",
              "             'must': 323,\n",
              "             'part': 324,\n",
              "             'area': 325,\n",
              "             'government': 326,\n",
              "             'believe': 327,\n",
              "             'enough': 328,\n",
              "             'least': 329,\n",
              "             'ok': 330,\n",
              "             'review': 331,\n",
              "             'hard': 332,\n",
              "             'probably': 333,\n",
              "             'qaeda': 334,\n",
              "             'read': 335,\n",
              "             'says': 336,\n",
              "             'everything': 337,\n",
              "             'horse': 338,\n",
              "             'later': 339,\n",
              "             'list': 340,\n",
              "             'oil': 341,\n",
              "             'percentage': 342,\n",
              "             'send': 343,\n",
              "             'tell': 344,\n",
              "             'wanted': 345,\n",
              "             '6': 346,\n",
              "             'months': 347,\n",
              "             'school': 348,\n",
              "             'different': 349,\n",
              "             'due': 350,\n",
              "             'forward': 351,\n",
              "             'international': 352,\n",
              "             'leave': 353,\n",
              "             'national': 354,\n",
              "             'start': 355,\n",
              "             '!!!': 356,\n",
              "             '....': 357,\n",
              "             '20': 358,\n",
              "             'below': 359,\n",
              "             'free': 360,\n",
              "             'yet': 361,\n",
              "             'case': 362,\n",
              "             'line': 363,\n",
              "             'possible': 364,\n",
              "             'received': 365,\n",
              "             'stay': 366,\n",
              "             'bit': 367,\n",
              "             'george': 368,\n",
              "             'letter': 369,\n",
              "             'real': 370,\n",
              "             'thought': 371,\n",
              "             'friday': 372,\n",
              "             'friendly': 373,\n",
              "             'month': 374,\n",
              "             'needs': 375,\n",
              "             'pretty': 376,\n",
              "             'report': 377,\n",
              "             'second': 378,\n",
              "             'security': 379,\n",
              "             'set': 380,\n",
              "             'taking': 381,\n",
              "             'talk': 382,\n",
              "             \"'d\": 383,\n",
              "             'check': 384,\n",
              "             'customer': 385,\n",
              "             'dr.': 386,\n",
              "             'fact': 387,\n",
              "             'full': 388,\n",
              "             'pay': 389,\n",
              "             'system': 390,\n",
              "             'energy': 391,\n",
              "             'highly': 392,\n",
              "             'hours': 393,\n",
              "             'within': 394,\n",
              "             '2005': 395,\n",
              "             'couple': 396,\n",
              "             'everyone': 397,\n",
              "             'pakistan': 398,\n",
              "             'places': 399,\n",
              "             'understand': 400,\n",
              "             'using': 401,\n",
              "             'open': 402,\n",
              "             'september': 403,\n",
              "             'show': 404,\n",
              "             'soon': 405,\n",
              "             'space': 406,\n",
              "             '<<': 407,\n",
              "             'ask': 408,\n",
              "             'cost': 409,\n",
              "             'friend': 410,\n",
              "             'friends': 411,\n",
              "             'hotel': 412,\n",
              "             'less': 413,\n",
              "             'making': 414,\n",
              "             'mark': 415,\n",
              "             'question': 416,\n",
              "             'return': 417,\n",
              "             'run': 418,\n",
              "             'several': 419,\n",
              "             '2003': 420,\n",
              "             '>>': 421,\n",
              "             'close': 422,\n",
              "             'early': 423,\n",
              "             'following': 424,\n",
              "             'front': 425,\n",
              "             'mind': 426,\n",
              "             'process': 427,\n",
              "             'wait': 428,\n",
              "             'wo': 429,\n",
              "             'based': 430,\n",
              "             'children': 431,\n",
              "             'comments': 432,\n",
              "             'either': 433,\n",
              "             'guys': 434,\n",
              "             'issue': 435,\n",
              "             'means': 436,\n",
              "             'plan': 437,\n",
              "             'services': 438,\n",
              "             'visit': 439,\n",
              "             'whole': 440,\n",
              "             'almost': 441,\n",
              "             'along': 442,\n",
              "             'americans': 443,\n",
              "             'april': 444,\n",
              "             'buy': 445,\n",
              "             'fax': 446,\n",
              "             'gave': 447,\n",
              "             'global': 448,\n",
              "             'hand': 449,\n",
              "             'happy': 450,\n",
              "             'including': 451,\n",
              "             'kind': 452,\n",
              "             'message': 453,\n",
              "             'mr.': 454,\n",
              "             'prices': 455,\n",
              "             'problems': 456,\n",
              "             'quite': 457,\n",
              "             'store': 458,\n",
              "             '*': 459,\n",
              "             '2004': 460,\n",
              "             'become': 461,\n",
              "             'eat': 462,\n",
              "             'game': 463,\n",
              "             'helpful': 464,\n",
              "             'issues': 465,\n",
              "             'jeff': 466,\n",
              "             'monday': 467,\n",
              "             'outside': 468,\n",
              "             'restaurant': 469,\n",
              "             'together': 470,\n",
              "             'town': 471,\n",
              "             'whether': 472,\n",
              "             '2001': 473,\n",
              "             'above': 474,\n",
              "             'ago': 475,\n",
              "             'although': 476,\n",
              "             'cat': 477,\n",
              "             'clean': 478,\n",
              "             'important': 479,\n",
              "             'july': 480,\n",
              "             'legal': 481,\n",
              "             'minutes': 482,\n",
              "             'others': 483,\n",
              "             'remember': 484,\n",
              "             'top': 485,\n",
              "             'add': 486,\n",
              "             'file': 487,\n",
              "             'fine': 488,\n",
              "             'likely': 489,\n",
              "             'market': 490,\n",
              "             'maybe': 491,\n",
              "             'quality': 492,\n",
              "             'texas': 493,\n",
              "             'weeks': 494,\n",
              "             '>': 495,\n",
              "             'available': 496,\n",
              "             'credit': 497,\n",
              "             'given': 498,\n",
              "             'large': 499,\n",
              "             'members': 500,\n",
              "             'morning': 501,\n",
              "             'program': 502,\n",
              "             'provide': 503,\n",
              "             '<': 504,\n",
              "             'air': 505,\n",
              "             'dog': 506,\n",
              "             'draft': 507,\n",
              "             'earlier': 508,\n",
              "             'hair': 509,\n",
              "             'houston': 510,\n",
              "             'indian': 511,\n",
              "             'late': 512,\n",
              "             'sara': 513,\n",
              "             'test': 514,\n",
              "             'tried': 515,\n",
              "             'zawahiri': 516,\n",
              "             'answer': 517,\n",
              "             'b': 518,\n",
              "             'death': 519,\n",
              "             'discuss': 520,\n",
              "             'especially': 521,\n",
              "             'form': 522,\n",
              "             'fun': 523,\n",
              "             'meet': 524,\n",
              "             'nasa': 525,\n",
              "             'north': 526,\n",
              "             'note': 527,\n",
              "             'peace': 528,\n",
              "             'reason': 529,\n",
              "             'regards': 530,\n",
              "             'south': 531,\n",
              "             'worth': 532,\n",
              "             '’s': 533,\n",
              "             '2000': 534,\n",
              "             'afghanistan': 535,\n",
              "             'anthrax': 536,\n",
              "             'charge': 537,\n",
              "             'coming': 538,\n",
              "             'doctor': 539,\n",
              "             'etc': 540,\n",
              "             'further': 541,\n",
              "             'general': 542,\n",
              "             'iraqi': 543,\n",
              "             'kids': 544,\n",
              "             'law': 545,\n",
              "             'rate': 546,\n",
              "             'support': 547,\n",
              "             'word': 548,\n",
              "             'according': 549,\n",
              "             'canada': 550,\n",
              "             'easy': 551,\n",
              "             'former': 552,\n",
              "             'guy': 553,\n",
              "             'israel': 554,\n",
              "             'mean': 555,\n",
              "             'regarding': 556,\n",
              "             'request': 557,\n",
              "             'stop': 558,\n",
              "             'tank': 559,\n",
              "             'young': 560,\n",
              "             '#': 561,\n",
              "             '11': 562,\n",
              "             'bill': 563,\n",
              "             'cage': 564,\n",
              "             'city': 565,\n",
              "             'continue': 566,\n",
              "             'local': 567,\n",
              "             'move': 568,\n",
              "             'needed': 569,\n",
              "             'news': 570,\n",
              "             'points': 571,\n",
              "             'professional': 572,\n",
              "             'started': 573,\n",
              "             ':)': 574,\n",
              "             'changes': 575,\n",
              "             'door': 576,\n",
              "             'finally': 577,\n",
              "             'four': 578,\n",
              "             'level': 579,\n",
              "             'man': 580,\n",
              "             'mike': 581,\n",
              "             'ones': 582,\n",
              "             'options': 583,\n",
              "             'past': 584,\n",
              "             'recently': 585,\n",
              "             'upon': 586,\n",
              "             'wonderful': 587,\n",
              "             'address': 588,\n",
              "             'contact': 589,\n",
              "             'countries': 590,\n",
              "             'course': 591,\n",
              "             'customers': 592,\n",
              "             'feed': 593,\n",
              "             'head': 594,\n",
              "             'instead': 595,\n",
              "             'known': 596,\n",
              "             'language': 597,\n",
              "             'leaders': 598,\n",
              "             'living': 599,\n",
              "             'period': 600,\n",
              "             'position': 601,\n",
              "             'seems': 602,\n",
              "             'seen': 603,\n",
              "             'team': 604,\n",
              "             'walk': 605,\n",
              "             'weapons': 606,\n",
              "             'wrong': 607,\n",
              "             '%': 608,\n",
              "             '12': 609,\n",
              "             '[': 610,\n",
              "             ']': 611,\n",
              "             'campaign': 612,\n",
              "             'comes': 613,\n",
              "             'control': 614,\n",
              "             'definitely': 615,\n",
              "             'entire': 616,\n",
              "             'extremely': 617,\n",
              "             'half': 618,\n",
              "             'lots': 619,\n",
              "             'makes': 620,\n",
              "             'political': 621,\n",
              "             'poor': 622,\n",
              "             'response': 623,\n",
              "             'sorry': 624,\n",
              "             'usually': 625,\n",
              "             'vet': 626,\n",
              "             'works': 627,\n",
              "             '100': 628,\n",
              "             '15': 629,\n",
              "             '30': 630,\n",
              "             'administration': 631,\n",
              "             'board': 632,\n",
              "             'book': 633,\n",
              "             'bring': 634,\n",
              "             'conference': 635,\n",
              "             'hear': 636,\n",
              "             'june': 637,\n",
              "             'light': 638,\n",
              "             'manager': 639,\n",
              "             'march': 640,\n",
              "             'million': 641,\n",
              "             'natural': 642,\n",
              "             'often': 643,\n",
              "             'pet': 644,\n",
              "             'project': 645,\n",
              "             'sea': 646,\n",
              "             'short': 647,\n",
              "             'subject': 648,\n",
              "             'taken': 649,\n",
              "             'website': 650,\n",
              "             'amazing': 651,\n",
              "             'attack': 652,\n",
              "             'baby': 653,\n",
              "             'currently': 654,\n",
              "             'e-mail': 655,\n",
              "             'elections': 656,\n",
              "             'expect': 657,\n",
              "             'forces': 658,\n",
              "             'girl': 659,\n",
              "             'guess': 660,\n",
              "             'heard': 661,\n",
              "             'human': 662,\n",
              "             'idea': 663,\n",
              "             'interested': 664,\n",
              "             'major': 665,\n",
              "             'owner': 666,\n",
              "             'party': 667,\n",
              "             'paul': 668,\n",
              "             'public': 669,\n",
              "             'side': 670,\n",
              "             'special': 671,\n",
              "             'street': 672,\n",
              "             'taliban': 673,\n",
              "             'trip': 674,\n",
              "             'type': 675,\n",
              "             'yourself': 676,\n",
              "             'access': 677,\n",
              "             'chris': 678,\n",
              "             'eggs': 679,\n",
              "             'federal': 680,\n",
              "             'force': 681,\n",
              "             'god': 682,\n",
              "             'health': 683,\n",
              "             'include': 684,\n",
              "             'mentioned': 685,\n",
              "             'metal': 686,\n",
              "             'notice': 687,\n",
              "             'offer': 688,\n",
              "             'risk': 689,\n",
              "             'saying': 690,\n",
              "             'situation': 691,\n",
              "             'st.': 692,\n",
              "             'strong': 693,\n",
              "             'thursday': 694,\n",
              "             'unless': 695,\n",
              "             'worked': 696,\n",
              "             '50': 697,\n",
              "             '7': 698,\n",
              "             '@': 699,\n",
              "             'america': 700,\n",
              "             'among': 701,\n",
              "             'animals': 702,\n",
              "             'cats': 703,\n",
              "             'certain': 704,\n",
              "             'contract': 705,\n",
              "             'david': 706,\n",
              "             'intelligence': 707,\n",
              "             'kept': 708,\n",
              "             'key': 709,\n",
              "             'main': 710,\n",
              "             'officials': 711,\n",
              "             'parents': 712,\n",
              "             'press': 713,\n",
              "             'region': 714,\n",
              "             'rights': 715,\n",
              "             'site': 716,\n",
              "             'tax': 717,\n",
              "             'white': 718,\n",
              "             'wife': 719,\n",
              "             '“': 720,\n",
              "             '”': 721,\n",
              "             '.?': 722,\n",
              "             'across': 723,\n",
              "             'anyway': 724,\n",
              "             'army': 725,\n",
              "             'clear': 726,\n",
              "             'dollars': 727,\n",
              "             'felt': 728,\n",
              "             'final': 729,\n",
              "             'huge': 730,\n",
              "             'inside': 731,\n",
              "             'kay': 732,\n",
              "             'looks': 733,\n",
              "             'management': 734,\n",
              "             'media': 735,\n",
              "             'perfect': 736,\n",
              "             'policy': 737,\n",
              "             'reports': 738,\n",
              "             'shall': 739,\n",
              "             'simply': 740,\n",
              "             'tomorrow': 741,\n",
              "             'trust': 742,\n",
              "             'weekend': 743,\n",
              "             '2002': 744,\n",
              "             'application': 745,\n",
              "             'behind': 746,\n",
              "             'black': 747,\n",
              "             'copy': 748,\n",
              "             'counterparty': 749,\n",
              "             'dead': 750,\n",
              "             'dinner': 751,\n",
              "             'face': 752,\n",
              "             'future': 753,\n",
              "             'groups': 754,\n",
              "             'london': 755,\n",
              "             'lunch': 756,\n",
              "             'moving': 757,\n",
              "             'musharraf': 758,\n",
              "             'near': 759,\n",
              "             'piece': 760,\n",
              "             'planning': 761,\n",
              "             'quickly': 762,\n",
              "             'recent': 763,\n",
              "             'religious': 764,\n",
              "             'reviews': 765,\n",
              "             'spent': 766,\n",
              "             'suicide': 767,\n",
              "             'talking': 768,\n",
              "             'u.s.': 769,\n",
              "             'value': 770,\n",
              "             'west': 771,\n",
              "             '25': 772,\n",
              "             'act': 773,\n",
              "             'activities': 774,\n",
              "             'cia': 775,\n",
              "             'cruise': 776,\n",
              "             'fish': 777,\n",
              "             'gone': 778,\n",
              "             'hi': 779,\n",
              "             'perhaps': 780,\n",
              "             'play': 781,\n",
              "             'product': 782,\n",
              "             'provided': 783,\n",
              "             'reasonable': 784,\n",
              "             'rude': 785,\n",
              "             'six': 786,\n",
              "             'worst': 787,\n",
              "             '8': 788,\n",
              "             'brought': 789,\n",
              "             'chance': 790,\n",
              "             'cut': 791,\n",
              "             'daily': 792,\n",
              "             'department': 793,\n",
              "             'egyptian': 794,\n",
              "             'example': 795,\n",
              "             'gulf': 796,\n",
              "             'history': 797,\n",
              "             'member': 798,\n",
              "             'option': 799,\n",
              "             'paid': 800,\n",
              "             'post': 801,\n",
              "             'ready': 802,\n",
              "             'red': 803,\n",
              "             'rest': 804,\n",
              "             'running': 805,\n",
              "             'save': 806,\n",
              "             'saw': 807,\n",
              "             'settlement': 808,\n",
              "             'sometimes': 809,\n",
              "             'total': 810,\n",
              "             'turn': 811,\n",
              "             'vince': 812,\n",
              "             'winter': 813,\n",
              "             'additional': 814,\n",
              "             'afternoon': 815,\n",
              "             'areas': 816,\n",
              "             'attend': 817,\n",
              "             'basis': 818,\n",
              "             'body': 819,\n",
              "             'bus': 820,\n",
              "             'committee': 821,\n",
              "             'drive': 822,\n",
              "             'enjoy': 823,\n",
              "             'europe': 824,\n",
              "             'expensive': 825,\n",
              "             'fall': 826,\n",
              "             'fixed': 827,\n",
              "             'follow': 828,\n",
              "             'hot': 829,\n",
              "             'island': 830,\n",
              "             'islands': 831,\n",
              "             'itself': 832,\n",
              "             'lack': 833,\n",
              "             'learn': 834,\n",
              "             'longer': 835,\n",
              "             'personal': 836,\n",
              "             'sign': 837,\n",
              "             'signed': 838,\n",
              "             'snake': 839,\n",
              "             'summer': 840,\n",
              "             'sunday': 841,\n",
              "             'themselves': 842,\n",
              "             'threat': 843,\n",
              "             'true': 844,\n",
              "             'western': 845,\n",
              "             'willing': 846,\n",
              "             'yesterday': 847,\n",
              "             'action': 848,\n",
              "             'attention': 849,\n",
              "             'circle': 850,\n",
              "             'comfortable': 851,\n",
              "             'current': 852,\n",
              "             'delivery': 853,\n",
              "             'east': 854,\n",
              "             'except': 855,\n",
              "             'executive': 856,\n",
              "             'goes': 857,\n",
              "             'held': 858,\n",
              "             'hold': 859,\n",
              "             'horrible': 860,\n",
              "             'initial': 861,\n",
              "             'islamic': 862,\n",
              "             'january': 863,\n",
              "             'location': 864,\n",
              "             'marriage': 865,\n",
              "             'middle': 866,\n",
              "             'moved': 867,\n",
              "             'myself': 868,\n",
              "             'rather': 869,\n",
              "             'richard': 870,\n",
              "             'seeing': 871,\n",
              "             'seem': 872,\n",
              "             'shop': 873,\n",
              "             'shows': 874,\n",
              "             'takes': 875,\n",
              "             'thinking': 876,\n",
              "             'treated': 877,\n",
              "             'tv': 878,\n",
              "             'university': 879,\n",
              "             'waste': 880,\n",
              "             'york': 881,\n",
              "             'absolutely': 882,\n",
              "             'alone': 883,\n",
              "             'approved': 884,\n",
              "             'arab': 885,\n",
              "             'arms': 886,\n",
              "             'august': 887,\n",
              "             'born': 888,\n",
              "             'carol': 889,\n",
              "             'center': 890,\n",
              "             'cover': 891,\n",
              "             'decided': 892,\n",
              "             'desk': 893,\n",
              "             'financial': 894,\n",
              "             'guard': 895,\n",
              "             'hospital': 896,\n",
              "             'info': 897,\n",
              "             'interest': 898,\n",
              "             'involved': 899,\n",
              "             'knew': 900,\n",
              "             'leader': 901,\n",
              "             'lost': 902,\n",
              "             'nearly': 903,\n",
              "             'necessary': 904,\n",
              "             'overall': 905,\n",
              "             'pick': 906,\n",
              "             'population': 907,\n",
              "             'quick': 908,\n",
              "             'related': 909,\n",
              "             'result': 910,\n",
              "             'san': 911,\n",
              "             'search': 912,\n",
              "             'single': 913,\n",
              "             'social': 914,\n",
              "             'species': 915,\n",
              "             'term': 916,\n",
              "             'version': 917,\n",
              "             'vietnam': 918,\n",
              "             'wants': 919,\n",
              "             'washington': 920,\n",
              "             'advice': 921,\n",
              "             'attacks': 922,\n",
              "             'awesome': 923,\n",
              "             'box': 924,\n",
              "             'budget': 925,\n",
              "             'cause': 926,\n",
              "             'certainly': 927,\n",
              "             'commission': 928,\n",
              "             'computer': 929,\n",
              "             'connection': 930,\n",
              "             'delhi': 931,\n",
              "             'eb': 932,\n",
              "             'education': 933,\n",
              "             'employees': 934,\n",
              "             'fresh': 935,\n",
              "             'giving': 936,\n",
              "             'hands': 937,\n",
              "             'k': 938,\n",
              "             'link': 939,\n",
              "             'lopez': 940,\n",
              "             'ltte': 941,\n",
              "             'matter': 942,\n",
              "             'menu': 943,\n",
              "             'numbers': 944,\n",
              "             'ordered': 945,\n",
              "             'parts': 946,\n",
              "             'pizza': 947,\n",
              "             'playing': 948,\n",
              "             'putting': 949,\n",
              "             'range': 950,\n",
              "             'schedule': 951,\n",
              "             'stuff': 952,\n",
              "             'sun': 953,\n",
              "             'trading': 954,\n",
              "             'treatment': 955,\n",
              "             'visa': 956,\n",
              "             'warm': 957,\n",
              "             '!!!!': 958,\n",
              "             '14': 959,\n",
              "             'andaman': 960,\n",
              "             'apply': 961,\n",
              "             'beautiful': 962,\n",
              "             'birth': 963,\n",
              "             'california': 964,\n",
              "             'cash': 965,\n",
              "             'changed': 966,\n",
              "             'child': 967,\n",
              "             'chinese': 968,\n",
              "             'climate': 969,\n",
              "             'considered': 970,\n",
              "             'correct': 971,\n",
              "             'court': 972,\n",
              "             'dangerous': 973,\n",
              "             'development': 974,\n",
              "             'dogs': 975,\n",
              "             'eating': 976,\n",
              "             'families': 977,\n",
              "             'five': 978,\n",
              "             'handle': 979,\n",
              "             'heat': 980,\n",
              "             'hour': 981,\n",
              "             'increase': 982,\n",
              "             'jihad': 983,\n",
              "             'lead': 984,\n",
              "             'mom': 985,\n",
              "             'online': 986,\n",
              "             'pakistani': 987,\n",
              "             'paper': 988,\n",
              "             'paris': 989,\n",
              "             'payment': 990,\n",
              "             're': 991,\n",
              "             'record': 992,\n",
              "             'research': 993,\n",
              "             'road': 994,\n",
              "             'safe': 995,\n",
              "             'sales': 996,\n",
              "             'simple': 997,\n",
              "             'station': 998,\n",
              "             'step': 999,\n",
              "             ...})"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czlCsTpApwmx"
      },
      "source": [
        "def tag_sentence(model, device, sentence, text_field, tag_field):\n",
        "\n",
        "    model.eval() # переведите модель в режим применения\n",
        "\n",
        "    if isinstance(sentence, str):\n",
        "        nlp = spacy.load('en_core_web_sm')\n",
        "        tokens = [token.text for token in nlp(sentence)]\n",
        "    else:\n",
        "        tokens = [token for token in sentence]\n",
        "\n",
        "    if text_field.lower:\n",
        "        tokens = [t.lower() for t in tokens]\n",
        "\n",
        "    numericalized_tokens = [text_field.vocab.stoi[t] for t in tokens]# ваш код здесь - создайте список, состоящий из переведенных в индексы токенов из словаря text_field.vocab (используйте stoi)\n",
        "\n",
        "    unk_idx = text_field.vocab.stoi[text_field.unk_token]\n",
        "\n",
        "    unks = [t for t, n in zip(tokens, numericalized_tokens) if n == unk_idx]\n",
        "\n",
        "    token_tensor = torch.LongTensor(numericalized_tokens) # приведите numericalized_tokens к типу torch.LongTensor\n",
        "\n",
        "    token_tensor = token_tensor.unsqueeze(-1).to(device)\n",
        "\n",
        "    predictions = model(token_tensor)# ваш код здесь - примените модель к token_tenzor\n",
        "\n",
        "    top_predictions = predictions.argmax(-1)\n",
        "\n",
        "    predicted_tags = [tag_field.vocab.itos[t.item()] for t in top_predictions]\n",
        "\n",
        "    return tokens, predicted_tags, unks"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VG1mjbEpwmy"
      },
      "source": [
        "## Задание\n",
        "\n",
        "Запустите две следующие ячейки. Проверим, что написанная функция работает корректно.\n",
        "\n",
        "В ответе выберите те токены, которые были нераспознаны (их не было в обучающих данных)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX9dYDOhpwmy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dba4ae6b-7ff8-4467-bcce-30caffa7ad8f"
      },
      "source": [
        "example_index = 1\n",
        "\n",
        "sentence = vars(train_data.examples[example_index])['text']\n",
        "actual_tags = vars(train_data.examples[example_index])['udtags']\n",
        "\n",
        "print(sentence)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[', 'this', 'killing', 'of', 'a', 'respected', 'cleric', 'will', 'be', 'causing', 'us', 'trouble', 'for', 'years', 'to', 'come', '.', ']']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJmb8kVTpwmy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc3e471f-6970-4992-d272-08bc9f690156"
      },
      "source": [
        "tokens, pred_tags, unks = tag_sentence(model,\n",
        "                                       device,\n",
        "                                       sentence,\n",
        "                                       TEXT,\n",
        "                                       UD_TAGS)\n",
        "\n",
        "print(unks)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['respected', 'cleric']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zAXE1KTpwmy"
      },
      "source": [
        "## Задание\n",
        "\n",
        "Проверим качество модели. Запустите ячейку ниже. В ответе укажите число неверно классифицированных токенов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hixzgb6fpwmz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dca597b6-7dce-469b-967c-b77f959c6342"
      },
      "source": [
        "print(\"Pred. Tag\\tActual Tag\\tCorrect?\\tToken\\n\")\n",
        "\n",
        "for token, pred_tag, actual_tag in zip(tokens, pred_tags, actual_tags):\n",
        "    correct = '✔' if pred_tag == actual_tag else '✘'\n",
        "    print(f\"{pred_tag}\\t\\t{actual_tag}\\t\\t{correct}\\t\\t{token}\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pred. Tag\tActual Tag\tCorrect?\tToken\n",
            "\n",
            "PUNCT\t\tPUNCT\t\t✔\t\t[\n",
            "DET\t\tDET\t\t✔\t\tthis\n",
            "NOUN\t\tNOUN\t\t✔\t\tkilling\n",
            "ADP\t\tADP\t\t✔\t\tof\n",
            "DET\t\tDET\t\t✔\t\ta\n",
            "ADJ\t\tADJ\t\t✔\t\trespected\n",
            "NOUN\t\tNOUN\t\t✔\t\tcleric\n",
            "AUX\t\tAUX\t\t✔\t\twill\n",
            "AUX\t\tAUX\t\t✔\t\tbe\n",
            "VERB\t\tVERB\t\t✔\t\tcausing\n",
            "PRON\t\tPRON\t\t✔\t\tus\n",
            "NOUN\t\tNOUN\t\t✔\t\ttrouble\n",
            "ADP\t\tADP\t\t✔\t\tfor\n",
            "NOUN\t\tNOUN\t\t✔\t\tyears\n",
            "PART\t\tPART\t\t✔\t\tto\n",
            "VERB\t\tVERB\t\t✔\t\tcome\n",
            "PUNCT\t\tPUNCT\t\t✔\t\t.\n",
            "PUNCT\t\tPUNCT\t\t✔\t\t]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2o_BtQspwm2"
      },
      "source": [
        "## Задание\n",
        "\n",
        "Примените модель к любому предложению (на английском языке). Какая доля токенов размечена верно? Ответ напишите в комментариях."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEO3Wy48pwm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdc0139e-7ebe-41f4-e4dc-ff8950c83c1a"
      },
      "source": [
        "sentence = 'I love this game'\n",
        "\n",
        "tokens, pred_tags, unks = tag_sentence(model,\n",
        "                                       device,\n",
        "                                       sentence,\n",
        "                                       TEXT,\n",
        "                                       UD_TAGS)\n",
        "\n",
        "print(unks)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    }
  ]
}