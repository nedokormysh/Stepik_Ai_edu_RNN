{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nedokormysh/Stepik_Ai_edu_RNN/blob/week_5_char_RNN/AiEdu_CharRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W8R8WgZceEk"
      },
      "source": [
        "# Character-Level LSTM\n",
        "На этом занятии поговорим про рекуррентные нейронные сети (Recurrent Neural Networ, RNN). Мы обучим модель на тексте книги \"Анна Каренина\", после чего попробуем генерировать новый текст.\n",
        "\n",
        "**Модель сможет генерировать новый текст на основе текста \"Анны Карениной\"!**\n",
        "\n",
        "Можно посмотреть полезную [статью про RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) и [реализацию в Torch](https://github.com/karpathy/char-rnn).\n",
        "\n",
        "Ообщая архитектура RNN:\n",
        "\n",
        "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/charseq.jpeg?raw=1\" width=\"500\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqUOE2flceEl"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wHfCDyzceEl"
      },
      "source": [
        "## Загрузим данные\n",
        "\n",
        "Загрузим текстовый файл \"Анны Карениной\"."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/aiedu-courses/rnn_bootcamp/main/anna.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXAxa--jPUSd",
        "outputId": "484f3568-79af-4bcf-aaf4-5f710df81b2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-12 15:18:41--  https://raw.githubusercontent.com/aiedu-courses/rnn_bootcamp/main/anna.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1985223 (1.9M) [text/plain]\n",
            "Saving to: ‘anna.txt’\n",
            "\n",
            "anna.txt            100%[===================>]   1.89M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-01-12 15:18:42 (85.7 MB/s) - ‘anna.txt’ saved [1985223/1985223]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b34kfqIOceEl"
      },
      "source": [
        "# open text file and read in data as `text`\n",
        "with open(\"anna.txt\", \"r\") as f:\n",
        "    text = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp1Ljc4mceEl"
      },
      "source": [
        "Посмотрим первые 100 символов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VctmLQfceEl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "74aa74bf-d0c1-40a8-83d3-9b2d60fed2f8"
      },
      "source": [
        "text[:100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iC21bopceEl"
      },
      "source": [
        "### Токенизация\n",
        "\n",
        "В ячейках ниже создадим два **словаря** для преобразования символов в целые числа и обратно. Кодирование символов как целых чисел упрощает их использование в качестве входных данных в сети."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYVlmnxLceEl"
      },
      "source": [
        "# encode the text and map each character to an integer and vice versa\n",
        "\n",
        "# we create two dictionaries:\n",
        "# 1. int2char, which maps integers to characters\n",
        "# 2. char2int, which maps characters to unique integers\n",
        "chars = tuple(set(text))\n",
        "\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJIzwzSwceEl"
      },
      "source": [
        "Посмотрим как символы закодировались целыми числами"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK1MYr_9ceEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94088dae-2eb8-4ca2-96a0-96f0a38d5648"
      },
      "source": [
        "encoded[:100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([52, 12, 13, 39,  0, 56, 57,  2, 75, 55, 55, 55,  4, 13, 39, 39, 27,\n",
              "        2, 28, 13, 51, 37, 15, 37, 56,  5,  2, 13, 57, 56,  2, 13, 15, 15,\n",
              "        2, 13, 15, 37, 29, 56, 49,  2, 56, 67, 56, 57, 27,  2, 80, 76, 12,\n",
              "       13, 39, 39, 27,  2, 28, 13, 51, 37, 15, 27,  2, 37,  5,  2, 80, 76,\n",
              "       12, 13, 39, 39, 27,  2, 37, 76,  2, 37,  0,  5,  2, 18, 78, 76, 55,\n",
              "       78, 13, 27, 65, 55, 55, 25, 67, 56, 57, 27,  0, 12, 37, 76])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azltQy-gceEl"
      },
      "source": [
        "## Предобработка данных\n",
        "\n",
        "Как можно видеть на изображении char-RNN выше, сеть ожидает **one-hot encoded** входа, что означает, что каждый символ преобразуется в целое число (через созданный словарь), а затем преобразуется в вектор-столбец, где только соответствующий ему целочисленный индекс будет иметь значение 1, а остальная часть вектора будет заполнена нулями. Давайте создадим для этого функцию."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnahALhiceEl"
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "\n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "\n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.0\n",
        "\n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "\n",
        "    return one_hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3lTdLKfceEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e9d2d15-4fac-412c-b6ef-5ff42703f79a"
      },
      "source": [
        "# check that the function works as expected\n",
        "test_seq = np.array([[3, 5, 1]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "\n",
        "print(one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YyL91CuceEl"
      },
      "source": [
        "## Создаем мини-батчи (mini-batchs)\n",
        "\n",
        "\n",
        "Создатдим мини-батчи для обучения. На простом примере они будут выглядеть так:\n",
        "\n",
        "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/sequence_batching@1x.png?raw=1\" width=500px>\n",
        "<br>\n",
        "\n",
        "Возьмем закодированные символы (переданные как параметр `arr`) и разделим их на несколько последовательностей, заданных параметром `batch_size`. Каждая из наших последовательностей будет иметь длину `seq_length`.\n",
        "\n",
        "### Создани батчей\n",
        "\n",
        "**1. Первое, что нам нужно сделать, это отбросить часть текста, чтобы у нас были только полные мини-батчи**\n",
        "\n",
        "Каждый батч содержит $ N\\times M $ символов, где $ N $ - это размер батча (количество последовательностей в батче), а $ M $ - длина `seq_length` или количество шагов в последовательности. Затем, чтобы получить общее количество батчей $ K $, которое мы можем сделать из массива `arr`, нужно разделить длину `arr` на количество символов в батче. Когда мы узнаем количество батчей, можно получить общее количество символов, которые нужно сохранить, из `arr`: $ N * M * K $.\n",
        "\n",
        "**2. После этого нам нужно разделить `arr` на $N$ батчей**\n",
        "\n",
        "Это можно сделать с помощью `arr.reshape(size)`, где `size` - это кортеж, содержащий размеры измененного массива. Мы знаем, что нам нужно $ N $ последовательностей в батче, поэтому сделаем его размером первого измерения. Для второго измерения можем использовать «-1» в качестве заполнителя, он заполнит массив соответствующими данными. После этого должен остаться массив $N\\times(M * K)$.\n",
        "\n",
        "**3. Теперь, когда у нас есть этот массив, мы можем перебирать его, чтобы получить наши мини-батчи**\n",
        "\n",
        "Идея состоит в том, что каждая партия представляет собой окно $ N\\times M $ в массиве $ N\\times (M * K) $. Для каждого последующего батча окно перемещается на `seq_length`. Мы также хотим создать как входной, так и выходной массивы. Это окно можно сделать с помощью `range`, чтобы делать шаги размером `n_steps` от $ 0 $ до `arr.shape [1]`, общее количество токенов в каждой последовательности. Таким образом, целые числа, которые получены из диапазона, всегда указывают на начало батча, и каждое окно имеет ширину `seq_length`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(int_words, batch_size, seq_length):\n",
        "    \"\"\"\n",
        "    Generates batches from encoded sequence.\n",
        "    :param int_words: tensor of ints, which represents the text; shape: [batch_size, -1]\n",
        "    :param batch_size: number of sequences per batch\n",
        "    :param seq_length: number of encoded chars in a sequence\n",
        "    :return: generator of pairs (x, y); x_shape, y_shape: [batch_size, seq_length - 1]\n",
        "    \"\"\"\n",
        "    # 1. Truncate text, so there are only full batches\n",
        "    window_size = seq_length + 1\n",
        "    batch_size_total = batch_size * window_size\n",
        "    n_batches = len(int_words) // batch_size_total\n",
        "    int_words = int_words[: n_batches * batch_size_total]\n",
        "\n",
        "    # 2. Reshape into batch_size rows\n",
        "    int_words = int_words.reshape((batch_size, -1))\n",
        "\n",
        "    # 3. Iterate through the text matrix\n",
        "    for position in range(0, int_words.shape[1], window_size):\n",
        "        x = int_words[:, position : position + window_size - 1]\n",
        "        y = int_words[:, position + 1 : position + window_size]\n",
        "        yield x, y"
      ],
      "metadata": {
        "id": "FLBOH8ZQUl7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9uKOvbqceEl"
      },
      "source": [
        "### Протестируем\n",
        "\n",
        "Теперь создадим несколько наборов данных, и проверим, что происходит, когда мы создаем батчи."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtKlLXi1ceEl"
      },
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rg5MUTqqceEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "431b3997-ed77-495e-c0a7-b6d119c01422"
      },
      "source": [
        "# printing out the first 10 items in a sequence\n",
        "print(\"x\\n\", x[:10, :10])\n",
        "print(\"\\ny\\n\", y[:10, :10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x\n",
            " [[52 12 13 39  0 56 57  2 75 55]\n",
            " [39 39 56 16  2 13 76 16  2  5]\n",
            " [ 0 12 13  0  2 37 76  5 18 15]\n",
            " [56 78  2  0 12 18 80  3 12  0]\n",
            " [56 57 26  2 12 56  2 12 13 16]\n",
            " [57 26  2 78 12 18  2 78 13  5]\n",
            " [ 5  0  2 35 56  2 72 18 67 56]\n",
            " [26  2 17 15 56 77 56 27  2 17]]\n",
            "\n",
            "y\n",
            " [[12 13 39  0 56 57  2 75 55 55]\n",
            " [39 56 16  2 13 76 16  2  5 12]\n",
            " [12 13  0  2 37 76  5 18 15 80]\n",
            " [78  2  0 12 18 80  3 12  0 28]\n",
            " [57 26  2 12 56  2 12 13 16  2]\n",
            " [26  2 78 12 18  2 78 13  5  2]\n",
            " [ 0  2 35 56  2 72 18 67 56 57]\n",
            " [ 2 17 15 56 77 56 27  2 17 15]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_qHIAEIceEl"
      },
      "source": [
        "Если вы правильно реализовали get_batches, результат должен выглядеть примерно так:\n",
        "```\n",
        "x\n",
        " [[25  8 60 11 45 27 28 73  1  2]\n",
        " [17  7 20 73 45  8 60 45 73 60]\n",
        " [27 20 80 73  7 28 73 60 73 65]\n",
        " [17 73 45  8 27 73 66  8 46 27]\n",
        " [73 17 60 12 73  8 27 28 73 45]\n",
        " [66 64 17 17 46  7 20 73 60 20]\n",
        " [73 76 20 20 60 73  8 60 80 73]\n",
        " [47 35 43  7 20 17 24 50 37 73]]\n",
        "\n",
        "y\n",
        " [[ 8 60 11 45 27 28 73  1  2  2]\n",
        " [ 7 20 73 45  8 60 45 73 60 45]\n",
        " [20 80 73  7 28 73 60 73 65  7]\n",
        " [73 45  8 27 73 66  8 46 27 65]\n",
        " [17 60 12 73  8 27 28 73 45 27]\n",
        " [64 17 17 46  7 20 73 60 20 80]\n",
        " [76 20 20 60 73  8 60 80 73 17]\n",
        " [35 43  7 20 17 24 50 37 73 36]]\n",
        " ```\n",
        " хотя точные цифры могут отличаться. Убедитесь, что данные сдвинуты на один шаг для `y`!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jouxv0L2ceEl"
      },
      "source": [
        "---\n",
        "## Зададим архитектуру\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/charRNN.png?raw=1\" width=500px>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7s5eRaoceEl"
      },
      "source": [
        "### Структура модели\n",
        "\n",
        "В `__init__` предлагаемая структура выглядит следующим образом:\n",
        "* Создаваём и храним необходимые словари (уже релизовано)\n",
        "* Определяем слой LSTM, который принимает в качестве параметров: размер ввода (количество символов), размер скрытого слоя `n_hidden`, количество слоев` n_layers`, вероятность drop-out'а `drop_prob` и логическое значение batch_first (True)\n",
        "* Определяем слой drop-out с помощью drop_prob\n",
        "* Определяем полносвязанный слой с параметрами: размер ввода `n_hidden` и размер выхода - количество символов\n",
        "* Наконец, инициализируем веса\n",
        "\n",
        "Обратите внимание, что некоторые параметры были названы и указаны в функции `__init__`, их нужно сохранить и использовать, выполняя что-то вроде` self.drop_prob = drop_prob`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Plm1atCuceEl"
      },
      "source": [
        "---\n",
        "### Входы-выходы LSTM\n",
        "\n",
        "Вы можете создать [LSTM layer](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM) следующим образом\n",
        "\n",
        "```python\n",
        "self.lstm = nn.LSTM(input_size, n_hidden, n_layers,\n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "```\n",
        "\n",
        "где `input_siz`e - это количество символов, которые эта ячейка ожидает видеть в качестве последовательного ввода, а `n_hidden` - это количество элементов в скрытых слоях ячейки. Можно добавить drop-out, добавив параметр `dropout` с заданной вероятностью. Наконец, в функции `forward` мы можем складывать ячейки LSTM в слои, используя `.view`.\n",
        "\n",
        "Также требуется создать начальное скрытое состояние всех нулей:\n",
        "\n",
        "```python\n",
        "self.init_hidden()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlTnDntHceEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ae1d4dc-0570-4d35-fbd6-4d2baf2ee5fe"
      },
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if train_on_gpu:\n",
        "    print(\"Training on GPU!\")\n",
        "else:\n",
        "    print(\"No GPU available, training on CPU; consider making n_epochs very small.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on GPU!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPq1EA38rBqn"
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "\n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "\n",
        "        ## TODO: define the LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True\n",
        "        )\n",
        "\n",
        "        ## TODO: define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        ## TODO: define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"Forward pass through the network.\n",
        "        These inputs are x, and the hidden/cell state `hidden`.\"\"\"\n",
        "\n",
        "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "\n",
        "        ## TODO: pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "\n",
        "        # Stack up LSTM outputs using view\n",
        "        # you may need to use contiguous to reshape the output\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "\n",
        "        ## TODO: put x through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"Initializes hidden state\"\"\"\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        if train_on_gpu:\n",
        "            hidden = (\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "            )\n",
        "        else:\n",
        "            hidden = (\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "            )\n",
        "\n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IrBRlEPceEl"
      },
      "source": [
        "## Обучим модель\n",
        "\n",
        "Во время обучения нужно установить количество эпох, скорость обучения и другие параметры.\n",
        "\n",
        "Используем оптимизатор Adam и кросс-энтропию, считаем loss и, как обычно, выполняем back propagation.\n",
        "\n",
        "Пара подробностей об обучении:\n",
        "> * Во время цикла мы отделяем скрытое состояние от его истории; на этот раз установив его равным новой переменной * tuple *, потому что скрытое состояние LSTM, является кортежем скрытых состояний.\n",
        "* Мы используем [`clip_grad_norm_`](https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html) чтобы избавиться от взрывающихся градиентов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lv8VkRI0ceEl"
      },
      "source": [
        "def train(\n",
        "    net,\n",
        "    data,\n",
        "    epochs=10,\n",
        "    batch_size=10,\n",
        "    seq_length=50,\n",
        "    lr=0.001,\n",
        "    clip=5,\n",
        "    val_frac=0.1,\n",
        "    print_every=10,\n",
        "):\n",
        "    \"\"\"Training a network\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "\n",
        "    net: CharRNN network\n",
        "    data: text data to train the network\n",
        "    epochs: Number of epochs to train\n",
        "    batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "    seq_length: Number of character steps per mini-batch\n",
        "    lr: learning rate\n",
        "    clip: gradient clipping\n",
        "    val_frac: Fraction of data to hold out for validation\n",
        "    print_every: Number of steps for printing training and validation loss\n",
        "\n",
        "    \"\"\"\n",
        "    net.train()\n",
        "\n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data) * (1 - val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "\n",
        "    if train_on_gpu:\n",
        "        net.cuda()\n",
        "\n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "\n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "\n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "            if train_on_gpu:\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "\n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "\n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "\n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                    inputs, targets = x, y\n",
        "                    if train_on_gpu:\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(\n",
        "                        output, targets.view(batch_size * seq_length).long()\n",
        "                    )\n",
        "\n",
        "                    val_losses.append(val_loss.item())\n",
        "\n",
        "                net.train()  # reset to train mode after iterationg through validation data\n",
        "\n",
        "                print(\n",
        "                    \"Epoch: {}/{}...\".format(e + 1, epochs),\n",
        "                    \"Step: {}...\".format(counter),\n",
        "                    \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                    \"Val Loss: {:.4f}\".format(np.mean(val_losses)),\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt0q4KGEceEm"
      },
      "source": [
        "## Определим модель\n",
        "\n",
        "Теперь мы можем создать модель с заданными гиперпараметрами. Определим размеры мини-батчей."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykMcIloEr3G7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8034232f-104f-4ec9-93d2-4deeaa09fadc"
      },
      "source": [
        "# define and print the net\n",
        "n_hidden = 512\n",
        "n_layers = 2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHy6mECuceEm"
      },
      "source": [
        "### Установим гиперпараметры"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hTkNrWEsjgI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2a5571e-7912-47d5-a3f5-b1ae14c691e4"
      },
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 20  # start smaller if you are just testing initial behavior\n",
        "\n",
        "# train the model\n",
        "train(\n",
        "    net,\n",
        "    encoded,\n",
        "    epochs=n_epochs,\n",
        "    batch_size=batch_size,\n",
        "    seq_length=seq_length,\n",
        "    lr=0.001,\n",
        "    print_every=10,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/20... Step: 10... Loss: 3.2480... Val Loss: 3.2043\n",
            "Epoch: 1/20... Step: 20... Loss: 3.1523... Val Loss: 3.1327\n",
            "Epoch: 1/20... Step: 30... Loss: 3.1330... Val Loss: 3.1250\n",
            "Epoch: 1/20... Step: 40... Loss: 3.1193... Val Loss: 3.1211\n",
            "Epoch: 1/20... Step: 50... Loss: 3.1267... Val Loss: 3.1194\n",
            "Epoch: 1/20... Step: 60... Loss: 3.1203... Val Loss: 3.1186\n",
            "Epoch: 1/20... Step: 70... Loss: 3.1414... Val Loss: 3.1174\n",
            "Epoch: 1/20... Step: 80... Loss: 3.1209... Val Loss: 3.1138\n",
            "Epoch: 1/20... Step: 90... Loss: 3.1208... Val Loss: 3.1060\n",
            "Epoch: 1/20... Step: 100... Loss: 3.0807... Val Loss: 3.0885\n",
            "Epoch: 1/20... Step: 110... Loss: 3.0502... Val Loss: 3.0396\n",
            "Epoch: 1/20... Step: 120... Loss: 3.0024... Val Loss: 2.9729\n",
            "Epoch: 1/20... Step: 130... Loss: 2.8825... Val Loss: 2.8634\n",
            "Epoch: 2/20... Step: 140... Loss: 2.7623... Val Loss: 2.7649\n",
            "Epoch: 2/20... Step: 150... Loss: 2.6671... Val Loss: 2.6504\n",
            "Epoch: 2/20... Step: 160... Loss: 2.5735... Val Loss: 2.5578\n",
            "Epoch: 2/20... Step: 170... Loss: 2.5173... Val Loss: 2.4960\n",
            "Epoch: 2/20... Step: 180... Loss: 2.4568... Val Loss: 2.4523\n",
            "Epoch: 2/20... Step: 190... Loss: 2.4316... Val Loss: 2.4228\n",
            "Epoch: 2/20... Step: 200... Loss: 2.4173... Val Loss: 2.3918\n",
            "Epoch: 2/20... Step: 210... Loss: 2.3826... Val Loss: 2.3650\n",
            "Epoch: 2/20... Step: 220... Loss: 2.3596... Val Loss: 2.3471\n",
            "Epoch: 2/20... Step: 230... Loss: 2.3371... Val Loss: 2.3132\n",
            "Epoch: 2/20... Step: 240... Loss: 2.3094... Val Loss: 2.2840\n",
            "Epoch: 2/20... Step: 250... Loss: 2.2751... Val Loss: 2.2603\n",
            "Epoch: 2/20... Step: 260... Loss: 2.2584... Val Loss: 2.2437\n",
            "Epoch: 2/20... Step: 270... Loss: 2.2405... Val Loss: 2.2151\n",
            "Epoch: 3/20... Step: 280... Loss: 2.1947... Val Loss: 2.1953\n",
            "Epoch: 3/20... Step: 290... Loss: 2.1902... Val Loss: 2.1679\n",
            "Epoch: 3/20... Step: 300... Loss: 2.1422... Val Loss: 2.1419\n",
            "Epoch: 3/20... Step: 310... Loss: 2.1525... Val Loss: 2.1222\n",
            "Epoch: 3/20... Step: 320... Loss: 2.1402... Val Loss: 2.1045\n",
            "Epoch: 3/20... Step: 330... Loss: 2.1275... Val Loss: 2.0900\n",
            "Epoch: 3/20... Step: 340... Loss: 2.0774... Val Loss: 2.0757\n",
            "Epoch: 3/20... Step: 350... Loss: 2.0930... Val Loss: 2.0558\n",
            "Epoch: 3/20... Step: 360... Loss: 2.0572... Val Loss: 2.0383\n",
            "Epoch: 3/20... Step: 370... Loss: 2.0103... Val Loss: 2.0203\n",
            "Epoch: 3/20... Step: 380... Loss: 2.0235... Val Loss: 2.0129\n",
            "Epoch: 3/20... Step: 390... Loss: 2.0247... Val Loss: 1.9904\n",
            "Epoch: 3/20... Step: 400... Loss: 1.9772... Val Loss: 1.9770\n",
            "Epoch: 3/20... Step: 410... Loss: 1.9953... Val Loss: 1.9623\n",
            "Epoch: 4/20... Step: 420... Loss: 1.9837... Val Loss: 1.9508\n",
            "Epoch: 4/20... Step: 430... Loss: 1.9459... Val Loss: 1.9357\n",
            "Epoch: 4/20... Step: 440... Loss: 1.9349... Val Loss: 1.9186\n",
            "Epoch: 4/20... Step: 450... Loss: 1.9204... Val Loss: 1.9120\n",
            "Epoch: 4/20... Step: 460... Loss: 1.9276... Val Loss: 1.8985\n",
            "Epoch: 4/20... Step: 470... Loss: 1.8872... Val Loss: 1.8889\n",
            "Epoch: 4/20... Step: 480... Loss: 1.8793... Val Loss: 1.8732\n",
            "Epoch: 4/20... Step: 490... Loss: 1.8778... Val Loss: 1.8599\n",
            "Epoch: 4/20... Step: 500... Loss: 1.8751... Val Loss: 1.8557\n",
            "Epoch: 4/20... Step: 510... Loss: 1.8652... Val Loss: 1.8389\n",
            "Epoch: 4/20... Step: 520... Loss: 1.8895... Val Loss: 1.8308\n",
            "Epoch: 4/20... Step: 530... Loss: 1.8279... Val Loss: 1.8174\n",
            "Epoch: 4/20... Step: 540... Loss: 1.8258... Val Loss: 1.8082\n",
            "Epoch: 4/20... Step: 550... Loss: 1.8439... Val Loss: 1.7990\n",
            "Epoch: 5/20... Step: 560... Loss: 1.8074... Val Loss: 1.7884\n",
            "Epoch: 5/20... Step: 570... Loss: 1.7873... Val Loss: 1.7821\n",
            "Epoch: 5/20... Step: 580... Loss: 1.7955... Val Loss: 1.7726\n",
            "Epoch: 5/20... Step: 590... Loss: 1.7723... Val Loss: 1.7645\n",
            "Epoch: 5/20... Step: 600... Loss: 1.7834... Val Loss: 1.7532\n",
            "Epoch: 5/20... Step: 610... Loss: 1.7554... Val Loss: 1.7496\n",
            "Epoch: 5/20... Step: 620... Loss: 1.7559... Val Loss: 1.7397\n",
            "Epoch: 5/20... Step: 630... Loss: 1.7612... Val Loss: 1.7371\n",
            "Epoch: 5/20... Step: 640... Loss: 1.7704... Val Loss: 1.7324\n",
            "Epoch: 5/20... Step: 650... Loss: 1.7340... Val Loss: 1.7207\n",
            "Epoch: 5/20... Step: 660... Loss: 1.6989... Val Loss: 1.7109\n",
            "Epoch: 5/20... Step: 670... Loss: 1.7000... Val Loss: 1.7003\n",
            "Epoch: 5/20... Step: 680... Loss: 1.7359... Val Loss: 1.6953\n",
            "Epoch: 5/20... Step: 690... Loss: 1.7407... Val Loss: 1.6905\n",
            "Epoch: 6/20... Step: 700... Loss: 1.6994... Val Loss: 1.6873\n",
            "Epoch: 6/20... Step: 710... Loss: 1.6823... Val Loss: 1.6804\n",
            "Epoch: 6/20... Step: 720... Loss: 1.7056... Val Loss: 1.6780\n",
            "Epoch: 6/20... Step: 730... Loss: 1.6718... Val Loss: 1.6675\n",
            "Epoch: 6/20... Step: 740... Loss: 1.6842... Val Loss: 1.6668\n",
            "Epoch: 6/20... Step: 750... Loss: 1.6648... Val Loss: 1.6571\n",
            "Epoch: 6/20... Step: 760... Loss: 1.6941... Val Loss: 1.6519\n",
            "Epoch: 6/20... Step: 770... Loss: 1.6526... Val Loss: 1.6474\n",
            "Epoch: 6/20... Step: 780... Loss: 1.6277... Val Loss: 1.6473\n",
            "Epoch: 6/20... Step: 790... Loss: 1.6678... Val Loss: 1.6393\n",
            "Epoch: 6/20... Step: 800... Loss: 1.6511... Val Loss: 1.6308\n",
            "Epoch: 6/20... Step: 810... Loss: 1.6193... Val Loss: 1.6285\n",
            "Epoch: 6/20... Step: 820... Loss: 1.6491... Val Loss: 1.6232\n",
            "Epoch: 7/20... Step: 830... Loss: 1.6334... Val Loss: 1.6215\n",
            "Epoch: 7/20... Step: 840... Loss: 1.6391... Val Loss: 1.6149\n",
            "Epoch: 7/20... Step: 850... Loss: 1.6293... Val Loss: 1.6119\n",
            "Epoch: 7/20... Step: 860... Loss: 1.6006... Val Loss: 1.6071\n",
            "Epoch: 7/20... Step: 870... Loss: 1.5921... Val Loss: 1.6015\n",
            "Epoch: 7/20... Step: 880... Loss: 1.6104... Val Loss: 1.6010\n",
            "Epoch: 7/20... Step: 890... Loss: 1.5813... Val Loss: 1.5937\n",
            "Epoch: 7/20... Step: 900... Loss: 1.5688... Val Loss: 1.5925\n",
            "Epoch: 7/20... Step: 910... Loss: 1.5909... Val Loss: 1.5878\n",
            "Epoch: 7/20... Step: 920... Loss: 1.6010... Val Loss: 1.5836\n",
            "Epoch: 7/20... Step: 930... Loss: 1.6086... Val Loss: 1.5798\n",
            "Epoch: 7/20... Step: 940... Loss: 1.5622... Val Loss: 1.5731\n",
            "Epoch: 7/20... Step: 950... Loss: 1.5658... Val Loss: 1.5694\n",
            "Epoch: 7/20... Step: 960... Loss: 1.5909... Val Loss: 1.5701\n",
            "Epoch: 8/20... Step: 970... Loss: 1.5538... Val Loss: 1.5693\n",
            "Epoch: 8/20... Step: 980... Loss: 1.5716... Val Loss: 1.5620\n",
            "Epoch: 8/20... Step: 990... Loss: 1.5658... Val Loss: 1.5570\n",
            "Epoch: 8/20... Step: 1000... Loss: 1.5497... Val Loss: 1.5580\n",
            "Epoch: 8/20... Step: 1010... Loss: 1.5608... Val Loss: 1.5511\n",
            "Epoch: 8/20... Step: 1020... Loss: 1.5676... Val Loss: 1.5499\n",
            "Epoch: 8/20... Step: 1030... Loss: 1.5313... Val Loss: 1.5469\n",
            "Epoch: 8/20... Step: 1040... Loss: 1.5868... Val Loss: 1.5435\n",
            "Epoch: 8/20... Step: 1050... Loss: 1.5597... Val Loss: 1.5410\n",
            "Epoch: 8/20... Step: 1060... Loss: 1.5145... Val Loss: 1.5409\n",
            "Epoch: 8/20... Step: 1070... Loss: 1.5379... Val Loss: 1.5359\n",
            "Epoch: 8/20... Step: 1080... Loss: 1.5508... Val Loss: 1.5331\n",
            "Epoch: 8/20... Step: 1090... Loss: 1.5063... Val Loss: 1.5323\n",
            "Epoch: 8/20... Step: 1100... Loss: 1.5358... Val Loss: 1.5283\n",
            "Epoch: 9/20... Step: 1110... Loss: 1.5604... Val Loss: 1.5240\n",
            "Epoch: 9/20... Step: 1120... Loss: 1.5194... Val Loss: 1.5191\n",
            "Epoch: 9/20... Step: 1130... Loss: 1.5260... Val Loss: 1.5157\n",
            "Epoch: 9/20... Step: 1140... Loss: 1.5022... Val Loss: 1.5149\n",
            "Epoch: 9/20... Step: 1150... Loss: 1.5381... Val Loss: 1.5092\n",
            "Epoch: 9/20... Step: 1160... Loss: 1.4981... Val Loss: 1.5088\n",
            "Epoch: 9/20... Step: 1170... Loss: 1.4943... Val Loss: 1.5101\n",
            "Epoch: 9/20... Step: 1180... Loss: 1.5048... Val Loss: 1.5005\n",
            "Epoch: 9/20... Step: 1190... Loss: 1.5150... Val Loss: 1.4992\n",
            "Epoch: 9/20... Step: 1200... Loss: 1.5130... Val Loss: 1.5011\n",
            "Epoch: 9/20... Step: 1210... Loss: 1.5433... Val Loss: 1.4987\n",
            "Epoch: 9/20... Step: 1220... Loss: 1.4929... Val Loss: 1.4911\n",
            "Epoch: 9/20... Step: 1230... Loss: 1.4925... Val Loss: 1.4996\n",
            "Epoch: 9/20... Step: 1240... Loss: 1.5065... Val Loss: 1.4908\n",
            "Epoch: 10/20... Step: 1250... Loss: 1.4877... Val Loss: 1.4900\n",
            "Epoch: 10/20... Step: 1260... Loss: 1.4816... Val Loss: 1.4856\n",
            "Epoch: 10/20... Step: 1270... Loss: 1.4873... Val Loss: 1.4834\n",
            "Epoch: 10/20... Step: 1280... Loss: 1.4617... Val Loss: 1.4833\n",
            "Epoch: 10/20... Step: 1290... Loss: 1.4796... Val Loss: 1.4799\n",
            "Epoch: 10/20... Step: 1300... Loss: 1.4519... Val Loss: 1.4767\n",
            "Epoch: 10/20... Step: 1310... Loss: 1.4697... Val Loss: 1.4736\n",
            "Epoch: 10/20... Step: 1320... Loss: 1.4644... Val Loss: 1.4729\n",
            "Epoch: 10/20... Step: 1330... Loss: 1.4722... Val Loss: 1.4737\n",
            "Epoch: 10/20... Step: 1340... Loss: 1.4505... Val Loss: 1.4721\n",
            "Epoch: 10/20... Step: 1350... Loss: 1.4419... Val Loss: 1.4692\n",
            "Epoch: 10/20... Step: 1360... Loss: 1.4316... Val Loss: 1.4664\n",
            "Epoch: 10/20... Step: 1370... Loss: 1.4599... Val Loss: 1.4659\n",
            "Epoch: 10/20... Step: 1380... Loss: 1.4791... Val Loss: 1.4639\n",
            "Epoch: 11/20... Step: 1390... Loss: 1.4580... Val Loss: 1.4607\n",
            "Epoch: 11/20... Step: 1400... Loss: 1.4395... Val Loss: 1.4589\n",
            "Epoch: 11/20... Step: 1410... Loss: 1.4607... Val Loss: 1.4559\n",
            "Epoch: 11/20... Step: 1420... Loss: 1.4230... Val Loss: 1.4552\n",
            "Epoch: 11/20... Step: 1430... Loss: 1.4343... Val Loss: 1.4528\n",
            "Epoch: 11/20... Step: 1440... Loss: 1.4365... Val Loss: 1.4502\n",
            "Epoch: 11/20... Step: 1450... Loss: 1.4599... Val Loss: 1.4536\n",
            "Epoch: 11/20... Step: 1460... Loss: 1.4236... Val Loss: 1.4510\n",
            "Epoch: 11/20... Step: 1470... Loss: 1.3945... Val Loss: 1.4522\n",
            "Epoch: 11/20... Step: 1480... Loss: 1.4515... Val Loss: 1.4481\n",
            "Epoch: 11/20... Step: 1490... Loss: 1.4327... Val Loss: 1.4453\n",
            "Epoch: 11/20... Step: 1500... Loss: 1.4078... Val Loss: 1.4437\n",
            "Epoch: 11/20... Step: 1510... Loss: 1.4311... Val Loss: 1.4449\n",
            "Epoch: 12/20... Step: 1520... Loss: 1.4344... Val Loss: 1.4457\n",
            "Epoch: 12/20... Step: 1530... Loss: 1.4242... Val Loss: 1.4375\n",
            "Epoch: 12/20... Step: 1540... Loss: 1.4256... Val Loss: 1.4398\n",
            "Epoch: 12/20... Step: 1550... Loss: 1.3911... Val Loss: 1.4383\n",
            "Epoch: 12/20... Step: 1560... Loss: 1.4074... Val Loss: 1.4330\n",
            "Epoch: 12/20... Step: 1570... Loss: 1.4104... Val Loss: 1.4321\n",
            "Epoch: 12/20... Step: 1580... Loss: 1.3957... Val Loss: 1.4312\n",
            "Epoch: 12/20... Step: 1590... Loss: 1.3838... Val Loss: 1.4276\n",
            "Epoch: 12/20... Step: 1600... Loss: 1.4022... Val Loss: 1.4305\n",
            "Epoch: 12/20... Step: 1610... Loss: 1.4198... Val Loss: 1.4297\n",
            "Epoch: 12/20... Step: 1620... Loss: 1.4255... Val Loss: 1.4270\n",
            "Epoch: 12/20... Step: 1630... Loss: 1.3684... Val Loss: 1.4248\n",
            "Epoch: 12/20... Step: 1640... Loss: 1.3799... Val Loss: 1.4268\n",
            "Epoch: 12/20... Step: 1650... Loss: 1.4109... Val Loss: 1.4259\n",
            "Epoch: 13/20... Step: 1660... Loss: 1.3950... Val Loss: 1.4316\n",
            "Epoch: 13/20... Step: 1670... Loss: 1.3901... Val Loss: 1.4272\n",
            "Epoch: 13/20... Step: 1680... Loss: 1.4000... Val Loss: 1.4210\n",
            "Epoch: 13/20... Step: 1690... Loss: 1.3833... Val Loss: 1.4190\n",
            "Epoch: 13/20... Step: 1700... Loss: 1.3952... Val Loss: 1.4171\n",
            "Epoch: 13/20... Step: 1710... Loss: 1.4038... Val Loss: 1.4156\n",
            "Epoch: 13/20... Step: 1720... Loss: 1.3678... Val Loss: 1.4172\n",
            "Epoch: 13/20... Step: 1730... Loss: 1.4194... Val Loss: 1.4142\n",
            "Epoch: 13/20... Step: 1740... Loss: 1.3774... Val Loss: 1.4128\n",
            "Epoch: 13/20... Step: 1750... Loss: 1.3600... Val Loss: 1.4153\n",
            "Epoch: 13/20... Step: 1760... Loss: 1.3758... Val Loss: 1.4114\n",
            "Epoch: 13/20... Step: 1770... Loss: 1.3951... Val Loss: 1.4103\n",
            "Epoch: 13/20... Step: 1780... Loss: 1.3452... Val Loss: 1.4064\n",
            "Epoch: 13/20... Step: 1790... Loss: 1.3892... Val Loss: 1.4128\n",
            "Epoch: 14/20... Step: 1800... Loss: 1.4043... Val Loss: 1.4100\n",
            "Epoch: 14/20... Step: 1810... Loss: 1.3799... Val Loss: 1.4035\n",
            "Epoch: 14/20... Step: 1820... Loss: 1.3808... Val Loss: 1.4016\n",
            "Epoch: 14/20... Step: 1830... Loss: 1.3515... Val Loss: 1.4008\n",
            "Epoch: 14/20... Step: 1840... Loss: 1.3945... Val Loss: 1.3981\n",
            "Epoch: 14/20... Step: 1850... Loss: 1.3712... Val Loss: 1.4045\n",
            "Epoch: 14/20... Step: 1860... Loss: 1.3500... Val Loss: 1.3997\n",
            "Epoch: 14/20... Step: 1870... Loss: 1.3845... Val Loss: 1.3993\n",
            "Epoch: 14/20... Step: 1880... Loss: 1.3849... Val Loss: 1.4003\n",
            "Epoch: 14/20... Step: 1890... Loss: 1.3823... Val Loss: 1.4008\n",
            "Epoch: 14/20... Step: 1900... Loss: 1.3986... Val Loss: 1.3962\n",
            "Epoch: 14/20... Step: 1910... Loss: 1.3654... Val Loss: 1.3930\n",
            "Epoch: 14/20... Step: 1920... Loss: 1.3719... Val Loss: 1.3952\n",
            "Epoch: 14/20... Step: 1930... Loss: 1.3857... Val Loss: 1.3934\n",
            "Epoch: 15/20... Step: 1940... Loss: 1.3604... Val Loss: 1.3978\n",
            "Epoch: 15/20... Step: 1950... Loss: 1.3542... Val Loss: 1.3901\n",
            "Epoch: 15/20... Step: 1960... Loss: 1.3558... Val Loss: 1.3878\n",
            "Epoch: 15/20... Step: 1970... Loss: 1.3360... Val Loss: 1.3890\n",
            "Epoch: 15/20... Step: 1980... Loss: 1.3646... Val Loss: 1.3890\n",
            "Epoch: 15/20... Step: 1990... Loss: 1.3373... Val Loss: 1.3896\n",
            "Epoch: 15/20... Step: 2000... Loss: 1.3509... Val Loss: 1.3871\n",
            "Epoch: 15/20... Step: 2010... Loss: 1.3474... Val Loss: 1.3905\n",
            "Epoch: 15/20... Step: 2020... Loss: 1.3490... Val Loss: 1.3837\n",
            "Epoch: 15/20... Step: 2030... Loss: 1.3308... Val Loss: 1.3862\n",
            "Epoch: 15/20... Step: 2040... Loss: 1.3213... Val Loss: 1.3825\n",
            "Epoch: 15/20... Step: 2050... Loss: 1.3181... Val Loss: 1.3784\n",
            "Epoch: 15/20... Step: 2060... Loss: 1.3497... Val Loss: 1.3827\n",
            "Epoch: 15/20... Step: 2070... Loss: 1.3596... Val Loss: 1.3834\n",
            "Epoch: 16/20... Step: 2080... Loss: 1.3469... Val Loss: 1.3834\n",
            "Epoch: 16/20... Step: 2090... Loss: 1.3306... Val Loss: 1.3775\n",
            "Epoch: 16/20... Step: 2100... Loss: 1.3520... Val Loss: 1.3783\n",
            "Epoch: 16/20... Step: 2110... Loss: 1.3136... Val Loss: 1.3775\n",
            "Epoch: 16/20... Step: 2120... Loss: 1.3245... Val Loss: 1.3762\n",
            "Epoch: 16/20... Step: 2130... Loss: 1.3264... Val Loss: 1.3762\n",
            "Epoch: 16/20... Step: 2140... Loss: 1.3654... Val Loss: 1.3759\n",
            "Epoch: 16/20... Step: 2150... Loss: 1.3150... Val Loss: 1.3781\n",
            "Epoch: 16/20... Step: 2160... Loss: 1.2887... Val Loss: 1.3759\n",
            "Epoch: 16/20... Step: 2170... Loss: 1.3600... Val Loss: 1.3738\n",
            "Epoch: 16/20... Step: 2180... Loss: 1.3299... Val Loss: 1.3740\n",
            "Epoch: 16/20... Step: 2190... Loss: 1.3051... Val Loss: 1.3716\n",
            "Epoch: 16/20... Step: 2200... Loss: 1.3254... Val Loss: 1.3731\n",
            "Epoch: 17/20... Step: 2210... Loss: 1.3280... Val Loss: 1.3771\n",
            "Epoch: 17/20... Step: 2220... Loss: 1.3353... Val Loss: 1.3705\n",
            "Epoch: 17/20... Step: 2230... Loss: 1.3249... Val Loss: 1.3693\n",
            "Epoch: 17/20... Step: 2240... Loss: 1.2909... Val Loss: 1.3699\n",
            "Epoch: 17/20... Step: 2250... Loss: 1.3102... Val Loss: 1.3672\n",
            "Epoch: 17/20... Step: 2260... Loss: 1.3160... Val Loss: 1.3649\n",
            "Epoch: 17/20... Step: 2270... Loss: 1.2994... Val Loss: 1.3631\n",
            "Epoch: 17/20... Step: 2280... Loss: 1.2967... Val Loss: 1.3641\n",
            "Epoch: 17/20... Step: 2290... Loss: 1.3137... Val Loss: 1.3626\n",
            "Epoch: 17/20... Step: 2300... Loss: 1.3328... Val Loss: 1.3632\n",
            "Epoch: 17/20... Step: 2310... Loss: 1.3323... Val Loss: 1.3655\n",
            "Epoch: 17/20... Step: 2320... Loss: 1.2849... Val Loss: 1.3627\n",
            "Epoch: 17/20... Step: 2330... Loss: 1.2893... Val Loss: 1.3666\n",
            "Epoch: 17/20... Step: 2340... Loss: 1.3062... Val Loss: 1.3648\n",
            "Epoch: 18/20... Step: 2350... Loss: 1.2965... Val Loss: 1.3640\n",
            "Epoch: 18/20... Step: 2360... Loss: 1.3008... Val Loss: 1.3557\n",
            "Epoch: 18/20... Step: 2370... Loss: 1.3195... Val Loss: 1.3605\n",
            "Epoch: 18/20... Step: 2380... Loss: 1.2977... Val Loss: 1.3590\n",
            "Epoch: 18/20... Step: 2390... Loss: 1.3135... Val Loss: 1.3565\n",
            "Epoch: 18/20... Step: 2400... Loss: 1.3164... Val Loss: 1.3580\n",
            "Epoch: 18/20... Step: 2410... Loss: 1.2962... Val Loss: 1.3594\n",
            "Epoch: 18/20... Step: 2420... Loss: 1.3371... Val Loss: 1.3552\n",
            "Epoch: 18/20... Step: 2430... Loss: 1.3036... Val Loss: 1.3529\n",
            "Epoch: 18/20... Step: 2440... Loss: 1.2773... Val Loss: 1.3513\n",
            "Epoch: 18/20... Step: 2450... Loss: 1.3111... Val Loss: 1.3555\n",
            "Epoch: 18/20... Step: 2460... Loss: 1.3216... Val Loss: 1.3557\n",
            "Epoch: 18/20... Step: 2470... Loss: 1.2732... Val Loss: 1.3574\n",
            "Epoch: 18/20... Step: 2480... Loss: 1.2988... Val Loss: 1.3574\n",
            "Epoch: 19/20... Step: 2490... Loss: 1.3212... Val Loss: 1.3535\n",
            "Epoch: 19/20... Step: 2500... Loss: 1.3015... Val Loss: 1.3507\n",
            "Epoch: 19/20... Step: 2510... Loss: 1.3016... Val Loss: 1.3507\n",
            "Epoch: 19/20... Step: 2520... Loss: 1.2872... Val Loss: 1.3499\n",
            "Epoch: 19/20... Step: 2530... Loss: 1.3229... Val Loss: 1.3510\n",
            "Epoch: 19/20... Step: 2540... Loss: 1.2928... Val Loss: 1.3534\n",
            "Epoch: 19/20... Step: 2550... Loss: 1.2796... Val Loss: 1.3520\n",
            "Epoch: 19/20... Step: 2560... Loss: 1.2911... Val Loss: 1.3462\n",
            "Epoch: 19/20... Step: 2570... Loss: 1.3049... Val Loss: 1.3456\n",
            "Epoch: 19/20... Step: 2580... Loss: 1.3028... Val Loss: 1.3455\n",
            "Epoch: 19/20... Step: 2590... Loss: 1.3321... Val Loss: 1.3465\n",
            "Epoch: 19/20... Step: 2600... Loss: 1.2918... Val Loss: 1.3497\n",
            "Epoch: 19/20... Step: 2610... Loss: 1.2946... Val Loss: 1.3441\n",
            "Epoch: 19/20... Step: 2620... Loss: 1.3142... Val Loss: 1.3493\n",
            "Epoch: 20/20... Step: 2630... Loss: 1.2803... Val Loss: 1.3463\n",
            "Epoch: 20/20... Step: 2640... Loss: 1.2850... Val Loss: 1.3400\n",
            "Epoch: 20/20... Step: 2650... Loss: 1.2850... Val Loss: 1.3424\n",
            "Epoch: 20/20... Step: 2660... Loss: 1.2662... Val Loss: 1.3459\n",
            "Epoch: 20/20... Step: 2670... Loss: 1.2895... Val Loss: 1.3419\n",
            "Epoch: 20/20... Step: 2680... Loss: 1.2660... Val Loss: 1.3448\n",
            "Epoch: 20/20... Step: 2690... Loss: 1.2750... Val Loss: 1.3444\n",
            "Epoch: 20/20... Step: 2700... Loss: 1.2763... Val Loss: 1.3453\n",
            "Epoch: 20/20... Step: 2710... Loss: 1.2802... Val Loss: 1.3413\n",
            "Epoch: 20/20... Step: 2720... Loss: 1.2506... Val Loss: 1.3431\n",
            "Epoch: 20/20... Step: 2730... Loss: 1.2528... Val Loss: 1.3406\n",
            "Epoch: 20/20... Step: 2740... Loss: 1.2556... Val Loss: 1.3416\n",
            "Epoch: 20/20... Step: 2750... Loss: 1.2820... Val Loss: 1.3413\n",
            "Epoch: 20/20... Step: 2760... Loss: 1.2864... Val Loss: 1.3394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfZxvNoDceEm"
      },
      "source": [
        "## Checkpoint\n",
        "\n",
        "После обучения сохраним модель, чтобы можно было загрузить ее позже. Здесь сохраняются параметры, необходимые для создания той же архитектуры, гиперпараметры скрытого слоя и токены."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6RXl5VAceEm"
      },
      "source": [
        "# change the name, for saving multiple files\n",
        "model_name = \"rnn_x_epoch.net\"\n",
        "\n",
        "checkpoint = {\n",
        "    \"n_hidden\": net.n_hidden,\n",
        "    \"n_layers\": net.n_layers,\n",
        "    \"state_dict\": net.state_dict(),\n",
        "    \"tokens\": net.chars,\n",
        "}\n",
        "\n",
        "with open(model_name, \"wb\") as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2sJhx5iceEm"
      },
      "source": [
        "---\n",
        "## Делаем предсказания\n",
        "\n",
        "Теперь, когда мы обучили модель, сделаем предсказание следующих символов! Для предсказания мы передаем последний символ, и сеть предсказывает следующий символ, который мы потом передаем снова на вхол и получаем еще один предсказанный символ и так далее...\n",
        "\n",
        "Наши прогнозы основаны на категориальном распределении вероятностей по всем возможным символам. Мы можем ограничить число символов, чтобы сделать получаемый предсказанный текст более разумным, рассматривая только некоторые наиболее вероятные символы $K$. Это не позволит сети выдавать нам совершенно абсурдные прогнозы, а также позволит внести некоторый шум и случайность в выбранный текст. Узнать больше [можно здесь](https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEIRW_B2ceEm"
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "    \"\"\"Given a character, predict the next character.\n",
        "    Returns the predicted character and the hidden state.\n",
        "    \"\"\"\n",
        "\n",
        "    # tensor inputs\n",
        "    x = np.array([[net.char2int[char]]])\n",
        "    x = one_hot_encode(x, len(net.chars))\n",
        "    inputs = torch.from_numpy(x)\n",
        "\n",
        "    if train_on_gpu:\n",
        "        inputs = inputs.cuda()\n",
        "\n",
        "    # detach hidden state from history\n",
        "    h = tuple([each.data for each in h])\n",
        "    # get the output of the model\n",
        "    out, h = net(inputs, h)\n",
        "\n",
        "    # get the character probabilities\n",
        "    p = F.softmax(out, dim=1).data\n",
        "    if train_on_gpu:\n",
        "        p = p.cpu()  # move to cpu\n",
        "\n",
        "    # get top characters\n",
        "    if top_k is None:\n",
        "        top_ch = np.arange(len(net.chars))\n",
        "    else:\n",
        "        p, top_ch = p.topk(top_k)\n",
        "        top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "    # select the likely next character with some element of randomness\n",
        "    p = p.numpy().squeeze()\n",
        "    char = np.random.choice(top_ch, p=p / p.sum())\n",
        "\n",
        "    # return the encoded value of the predicted char and the hidden state\n",
        "    return net.int2char[char], h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OG38j3gQceEm"
      },
      "source": [
        "### Priming и генерирование текста\n",
        "\n",
        "Нужно задать скрытое состояние, чтобы сеть не генерировала произвольные символы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9vpB5gRceEm"
      },
      "source": [
        "def sample(net, size, prime=\"The\", top_k=None):\n",
        "\n",
        "    if train_on_gpu:\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "\n",
        "    net.eval()  # eval mode\n",
        "\n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "\n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return \"\".join(chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqmFA9eEceEm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c36e5bd-5f9a-4360-d91a-44877e37e471"
      },
      "source": [
        "print(sample(net, 1000, prime=\"Anna\", top_k=5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anna.\n",
            "\n",
            "\"Why say,\" thought Stepan Arkadyevitch.\n",
            "\n",
            "\"Why, taken it! Would not let them about her.\"\n",
            "\n",
            "\"Where is the tenth is awful!\" said Vronsky, was still more straight-but her life. \"That's it,\n",
            "and there's no meant of te part,\" said the legity, stream of\n",
            "singer. \"To take the sound of more sense when the steps, and I hear you with a mere all this\n",
            "treat of\n",
            "corcurory too. That's it to make them\n",
            "impossible, and what does not change it, I'm so many more to them.......\"\n",
            "\n",
            "\"Why are you than you. He's not beginning to this stand.\" He was neven a sight of the district, as\n",
            "though all a letter would be seen her, she was said, while his window and her former strength\n",
            "they had a man, said the supper. Shim in the supper aroused, were, the preterr of the conversation of this man who was to take\n",
            "a member, whom she saw that there had been time to bad anyone they would be talking in answer. Anna still there her son, of his wife, sand in the rubble operation to spend he would have taken him,\n",
            "and that the sended o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "942mjdQHceEm"
      },
      "source": [
        "## Loading a checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt9ldUuSceEm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5f42e36-9bcd-41b6-d937-05d087d32e82"
      },
      "source": [
        "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
        "with open(\"rnn_x_epoch.net\", \"rb\") as f:\n",
        "    checkpoint = torch.load(f)\n",
        "\n",
        "loaded = CharRNN(\n",
        "    checkpoint[\"tokens\"],\n",
        "    n_hidden=checkpoint[\"n_hidden\"],\n",
        "    n_layers=checkpoint[\"n_layers\"],\n",
        ")\n",
        "loaded.load_state_dict(checkpoint[\"state_dict\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut6R3zDcceEm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7251338-373c-4cb1-84f5-bbc5ecf8eff0"
      },
      "source": [
        "# Sample using a loaded model\n",
        "print(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "And Levin said to Vronsky in his heart.\n",
            "\n",
            "\"Yes, I want to see him. Have you, there will be something of the\n",
            "position,\" he thought, began to talk to him with the\n",
            "sungring and an insidely official chores, which he was dead, as he strange\n",
            "her eyes.\n",
            "\n",
            "There was a proprising at\n",
            "all timidle. And\n",
            "at his horses she felt something but he\n",
            "could not take them. At a learned, were taking a conversation with his winder, and was a little. The mother were contemptuously as the dream in his shoulders, he wanted.\n",
            "\n",
            "Stepan Arkadyevitch talked about that the wintow and saw the\n",
            "staid, and that she was delightful to see her,\n",
            "which she could\n",
            "not\n",
            "see him and so as to say to said. The porter, this stream of the sungen them over with hin sposts and he had no compresent, they was the secrets of\n",
            "passing and decided by talk of the same,\n",
            "and so as to do it was not been from the possession as to be thinking of the carriage.\n",
            "\n",
            "\"And what am I show,\" said Levin to himself. The convistion of the same wife and asked his brother would sake the\n",
            "children, all the trous as\n",
            "interested that the crue and as this peasants only a such an offering a class as the same his face, her sons, the mandel of his sour. The such\n",
            "cares, they\n",
            "were nice talking times. But in a third way--husband,\n",
            "the same work, at it with his weak one there too sent to him, had attached any sour, was not in the matter, and she was not beginning. But the part in his clitstinic compenions was the princess\n",
            "so is till that who cauled that as though the\n",
            "presented hairs were thinking of the doctor shout, but at that house was being brother of\n",
            "him on\n",
            "what he had the strength\n",
            "with his bottle of the crask and at the\n",
            "carriage, and still shothed to be\n",
            "suddenly a great shuller, that then all over it was not telling herself. She sat down\n",
            "a stand and saw something to think of the secret, which was not\n",
            "beginning, and with this\n",
            "first moment in a single point in the parert and setting off it.\n",
            "\n",
            "He had not been fond of his sisser. And the\n",
            "service on his wife, and they were said\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rhp9qC8neCk"
      },
      "source": [
        "### Полезные ссылки:\n",
        "\n",
        "\n",
        "*   [Блог-пост Christopher'а Olah'а по LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
      ]
    }
  ]
}